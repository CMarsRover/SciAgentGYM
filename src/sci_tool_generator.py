from openai import OpenAI
from dataclasses import dataclass
from typing import List, Optional, Dict, Any
import json
import base64
import mimetypes
import os
import re
import argparse
from py_coding_extract import extract_number_from_path, extract_code_with_filename, save_extracted_code
from pydantic import BaseModel, Field 

# å¯¼å…¥è‡ªå®šä¹‰çš„æœç´¢å·¥å…·
from llm_integration import LLMWithSearch
SEARCH_AVAILABLE = True


class FirstRoundAnalysis(BaseModel):
    """ç¬¬ä¸€è½®åˆ†æç»“æœçš„æ•°æ®ç»“æ„"""
    # å›¾ç‰‡ç›¸å…³å­—æ®µ
    image_description: Optional[str] = Field(None, description="å›¾ç‰‡å†…å®¹ã€ç»†èŠ‚å’Œå…³é”®ä¿¡æ¯çš„è¯¦ç»†æè¿°")
    identified_concepts: Optional[List[str]] = Field(None, description="ä»å›¾ç‰‡ä¸­è¯†åˆ«å‡ºçš„æ ¸å¿ƒæ¦‚å¿µåˆ—è¡¨")
    
    # é—®é¢˜åˆ†æå­—æ®µï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰
    image_analysis: Optional[str] = Field(None, description="å›¾ç‰‡å†…å®¹ã€ç»†èŠ‚å’Œå…³é”®ä¿¡æ¯çš„è¯¦ç»†æè¿°ï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰")
    problem_analysis: Optional[str] = Field(None, description="é—®é¢˜åˆ†æå†…å®¹")
    
    # æœç´¢ç›¸å…³å­—æ®µ
    need_search: bool = Field(..., description="æ˜¯å¦éœ€è¦ç½‘ç»œæœç´¢è·å–é¢å¤–ä¿¡æ¯")
    search_query: Optional[str] = Field(None, description="å¦‚æœéœ€è¦æœç´¢ï¼Œæä¾›å…·ä½“çš„æœç´¢å…³é”®è¯å’Œç›®çš„ï¼Œç»„ç»‡æˆä¸€å¥è¯")
    search_reason: Optional[str] = Field(None, description="éœ€è¦æœç´¢çš„åŸå› ï¼Œæ¯”å¦‚ä¸“ä¸šæ•°æ®åº“ä¿¡æ¯ã€å­¦ç§‘ä¸“å±pythonåŒ…æˆ–ä¸“å®¶ç»éªŒ")
    
    def get_analysis_content(self) -> str:
        """è·å–åˆ†æå†…å®¹ï¼Œä¼˜å…ˆè¿”å›image_descriptionï¼Œç„¶åæ˜¯image_analysisï¼Œæœ€åæ˜¯problem_analysis"""
        return (self.image_description or 
                self.image_analysis or 
                self.problem_analysis or 
                "æ— åˆ†æå†…å®¹")
    
    def get_concepts(self) -> List[str]:
        """è·å–è¯†åˆ«çš„æ¦‚å¿µåˆ—è¡¨"""
        return self.identified_concepts or []

@dataclass
class ConversationTurn:
    """å¯¹è¯è½®æ¬¡æ•°æ®ç»“æ„"""
    role: str  # "user", "assistant", æˆ– "system"
    content: str
    images: List[str] = None  # å›¾ç‰‡è·¯å¾„åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºç©ºåˆ—è¡¨
    metadata: Optional[Dict[str, Any]] = None
    
    def __post_init__(self):
        """åˆå§‹åŒ–åå¤„ç†ï¼Œç¡®ä¿imagesæ˜¯åˆ—è¡¨"""
        if self.images is None:
            self.images = []

def create_sci_tool_conversation(query: str, image_paths: List[str] = None, search_results: str = None, first_round_result: str = None) -> List[ConversationTurn]:
    """åˆ›å»ºç§‘å­¦å·¥å…·ç”Ÿæˆçš„ä¸¤è½®å¯¹è¯"""
    
    # ç¡®ä¿image_pathsæ˜¯åˆ—è¡¨æ ¼å¼
    if image_paths is None:
        image_paths = []
    elif isinstance(image_paths, str):
        image_paths = [image_paths]
    
    # ç¬¬ä¸€è½®ï¼šå›¾ç‰‡è¯†åˆ«å’Œæè¿°
    first_round_system = "ä½ æ˜¯ä¸€ä¸ªå­¦ç§‘ä¸“å®¶èƒ½è¯†åˆ«å„ç§å­¦ç§‘ä¸“ä¸šçš„å›¾ç‰‡ï¼Œè¯·å¯¹å›¾ç‰‡çš„å†…å®¹ã€ç»†èŠ‚ä¸å…³é”®ä¿¡æ¯ä½œå‡ºæè¿°ã€‚"
    first_round_user = f"è¯·åˆ†æä»¥ä¸‹å›¾ç‰‡å¹¶æè¿°å…¶å†…å®¹ã€ç»†èŠ‚å’Œå…³é”®ä¿¡æ¯ï¼š\né—®é¢˜ï¼š{query}"
    
    # è¯»å–ç¬¬äºŒè½®çš„system prompt
    with open("prompts/SystemPrompt_Science_Toolkit.md", 'r', encoding='utf-8') as f:
        second_round_system = f.read()
    
    # æ„å»ºç¬¬äºŒè½®çš„ç”¨æˆ·æ¶ˆæ¯
    second_round_user_parts = [
        f"ç§‘å­¦é—®é¢˜ï¼š{query}",
        f"å›¾ç‰‡åˆ†æç»“æœï¼š{first_round_result if first_round_result else '[ç­‰å¾…ç¬¬ä¸€è½®åˆ†æç»“æœ]'}",
        f"æœç´¢å·¥å…·ç»“æœï¼š{search_results if search_results else '[ç­‰å¾…æœç´¢å·¥å…·ç»“æœ]'}"
    ]
    second_round_user = "\n\n".join(second_round_user_parts)
    
    return [
        # ç¬¬ä¸€è½®å¯¹è¯
        ConversationTurn("system", first_round_system),
        ConversationTurn("user", first_round_user, images=image_paths),
        
        # ç¬¬äºŒè½®å¯¹è¯
        ConversationTurn("system", second_round_system),
        ConversationTurn("user", second_round_user, images=image_paths)
    ]

def convert_turns_to_api_messages(conversation_history: List[ConversationTurn]) -> List[Dict[str, Any]]:
    """å°†ConversationTurnåˆ—è¡¨è½¬æ¢ä¸ºAPIæ‰€éœ€çš„messagesæ ¼å¼ï¼Œæ”¯æŒå›¾ç‰‡"""
    api_messages = []
    for turn in conversation_history:
        message_content = []
        
        # æ·»åŠ æ–‡æœ¬å†…å®¹
        if turn.content:
            message_content.append({
                "type": "text",
                "text": turn.content
            })
        
        # æ·»åŠ å›¾ç‰‡å†…å®¹
        if turn.images:
            # ç¡®ä¿turn.imagesæ˜¯åˆ—è¡¨æ ¼å¼
            images_list = turn.images
            if isinstance(images_list, str):
                images_list = [images_list]
            elif images_list is None:
                images_list = []
            
            for image_path in images_list:
                if image_path:  # ç¡®ä¿å›¾ç‰‡è·¯å¾„ä¸ä¸ºç©º
                    try:
                        # ç¼–ç å›¾ç‰‡ä¸ºbase64
                        import base64
                        with open(image_path, "rb") as image_file:
                            base64_image = base64.b64encode(image_file.read()).decode('utf-8')
                        
                        # è·å–å›¾ç‰‡MIMEç±»å‹
                        import mimetypes
                        mime_type, _ = mimetypes.guess_type(image_path)
                        if not mime_type:
                            mime_type = "image/jpeg"  # é»˜è®¤ç±»å‹
                        
                        message_content.append({
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:{mime_type};base64,{base64_image}"
                            }
                        })
                    except Exception as e:
                        print(f"å›¾ç‰‡å¤„ç†å¤±è´¥ {image_path}: {e}")
        
        # æ„å»ºæ¶ˆæ¯
        if message_content:
            api_messages.append({
                "role": turn.role,
                "content": message_content
            })
        elif turn.role == "system":
            # system æ¶ˆæ¯ä¸éœ€è¦å›¾ç‰‡ï¼Œç›´æ¥ä½¿ç”¨æ–‡æœ¬å†…å®¹
            api_messages.append({
                "role": "system",
                "content": turn.content
            })
 
    return api_messages

def multi_turn_chat(conversation_turns: List[ConversationTurn], stream: bool = False) -> List[ConversationTurn]:
    """å¤šè½®å¯¹è¯ - éå†ä¼ å…¥çš„ConversationTurnåˆ—è¡¨
    
    Args:
        conversation_turns: å¯¹è¯è½®æ¬¡åˆ—è¡¨
        stream: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡ºï¼ˆé»˜è®¤Falseï¼Œç”¨äºé•¿å“åº”é¿å…è¶…æ—¶ï¼‰
    """
    conversation_history = []  # å­˜å‚¨å®Œæ•´çš„å¯¹è¯å†å²
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = OpenAI(
            api_key="sk-bhcvvaKeyQyguQ0dMUxjXUHJ3LtZPvASsLx9YnXtLdNhwD0R",
            base_url="https://api.boyuerichdata.opensphereai.com/v1"
        )
    
    
    first_round_result = None  # å­˜å‚¨ç¬¬ä¸€è½®çš„ç»“æœ
    
    for turn in conversation_turns:
        # æ·»åŠ å½“å‰è½®æ¬¡åˆ°å¯¹è¯å†å²
        conversation_history.append(turn)
        
        # å¦‚æœæ˜¯ç”¨æˆ·æ¶ˆæ¯ï¼Œéœ€è¦è·å–åŠ©æ‰‹å›å¤
        if turn.role == "user":
            try:
                # å°†ConversationTurnå¯¹è±¡è½¬æ¢ä¸ºAPIæ‰€éœ€çš„æ ¼å¼
                api_messages = convert_turns_to_api_messages(conversation_history)
                
                if stream:
                    # æµå¼è¾“å‡ºæ¨¡å¼
                    print("ğŸ”„ ä½¿ç”¨æµå¼è¾“å‡ºæ¨¡å¼ï¼ˆé¿å…è¶…æ—¶ï¼‰...")
                    assistant_reply = _stream_chat_completion(client, api_messages)
                else:
                    # æ™®é€šæ¨¡å¼
                    response = client.chat.completions.create(
                        model="anthropic/claude-sonnet-4.5",
                        messages=api_messages,
                        temperature=0.2,
                        max_tokens=64000
                    ) 
                    assistant_reply = response.choices[0].message.content
                    print(f"Assistant: {assistant_reply[:200]}...")
                
                # åˆ›å»ºåŠ©æ‰‹å¯¹è¯è½®æ¬¡å¹¶æ·»åŠ åˆ°å†å²
                assistant_turn = ConversationTurn(
                    role="assistant",
                    content=assistant_reply,
                    images=[],  # åŠ©æ‰‹å›å¤æš‚æ—¶ä¸æ”¯æŒå›¾ç‰‡
                    metadata={"timestamp": None}
                )
                conversation_history.append(assistant_turn)
                
                # å¦‚æœæ˜¯ç¬¬ä¸€è½®ï¼Œä¿å­˜ç»“æœç”¨äºç¬¬äºŒè½®
                if first_round_result is None:
                    first_round_result = assistant_reply
                    print(f"ç¬¬ä¸€è½®ç»“æœå·²ä¿å­˜: {first_round_result[:100]}...")
                
            except Exception as e:
                print(f"è¯·æ±‚å¤±è´¥: {e}")
                # å¯ä»¥é€‰æ‹©ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªturnæˆ–è€…break
                break
    
    return conversation_history

def _stream_chat_completion(client, api_messages, model: str = "claude-sonnet-4-20250514", temperature: float = 0.2, max_tokens: int = 64000) -> str:
    """æµå¼è¾“å‡ºå¤„ç†å‡½æ•°
    
    Args:
        client: OpenAIå®¢æˆ·ç«¯
        api_messages: APIæ¶ˆæ¯åˆ—è¡¨
        model: æ¨¡å‹åç§°
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§tokenæ•°
        
    Returns:
        å®Œæ•´çš„åŠ©æ‰‹å›å¤å†…å®¹
    """
    print("\n" + "="*60)
    print("å¼€å§‹æµå¼æ¥æ”¶å“åº”...")
    print("="*60)
    
    full_content = ""
    chunk_count = 0
    
    try:
        stream = client.chat.completions.create(
            model=model,
            messages=api_messages,
            temperature=temperature,
            max_tokens=max_tokens,
            stream=True  # å¯ç”¨æµå¼è¾“å‡º
        )
        
        for chunk in stream:
            if chunk.choices and len(chunk.choices) > 0:
                delta = chunk.choices[0].delta
                if delta and delta.content:
                    content = delta.content
                    full_content += content
                    chunk_count += 1
                    
                    # å®æ—¶æ‰“å°å†…å®¹ï¼ˆæ¯10ä¸ªchunkæ‰“å°ä¸€æ¬¡è¿›åº¦ï¼Œé¿å…åˆ·å±ï¼‰
                    if chunk_count % 10 == 0:
                        print(f"ğŸ“¥ å·²æ¥æ”¶ {chunk_count} ä¸ªæ•°æ®å—ï¼Œå½“å‰é•¿åº¦: {len(full_content)} å­—ç¬¦", end='\r')
                    
                    # å®æ—¶è¾“å‡ºå†…å®¹ï¼ˆå¯é€‰ï¼šå¦‚æœå¸Œæœ›çœ‹åˆ°å®æ—¶å†…å®¹ï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Šï¼‰
                    # print(content, end='', flush=True)
        
        print(f"\nâœ… æµå¼æ¥æ”¶å®Œæˆï¼å…±æ¥æ”¶ {chunk_count} ä¸ªæ•°æ®å—ï¼Œæ€»é•¿åº¦: {len(full_content)} å­—ç¬¦")
        print("="*60 + "\n")
        
        return full_content
        
    except Exception as e:
        print(f"\nâŒ æµå¼è¾“å‡ºè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        # å¦‚æœæµå¼è¾“å‡ºå¤±è´¥ï¼Œè¿”å›å·²æ¥æ”¶çš„å†…å®¹
        if full_content:
            print(f"âš  è¿”å›å·²æ¥æ”¶çš„éƒ¨åˆ†å†…å®¹ï¼ˆ{len(full_content)} å­—ç¬¦ï¼‰")
            return full_content
        else:
            raise

def display_conversation_history(conversation_history: List[ConversationTurn]):
    """æ˜¾ç¤ºå¯¹è¯å†å²ï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
    print("\n=== å¯¹è¯å†å² ===")
    for i, turn in enumerate(conversation_history):
        print(f"{i+1}. {turn.role}: {turn.content[:100]}{'...' if len(turn.content) > 100 else ''}")
        if turn.images:
            print(f"   å›¾ç‰‡: {turn.images}")
        if turn.metadata:
            print(f"   å…ƒæ•°æ®: {turn.metadata}")
    print("================\n") 
 

def parse_first_round_result(first_round_result: str) -> FirstRoundAnalysis:
    """è§£æç¬¬ä¸€è½®ç»“æœï¼Œä½¿ç”¨PydanticéªŒè¯JSONæ ¼å¼"""
    if not first_round_result:
        return FirstRoundAnalysis(
            image_analysis="æ— åˆ†æç»“æœ",
            need_search=False,
            search_query=None,
            search_reason=None
        )
    
    try:
        # å°è¯•ä»ç»“æœä¸­æå–JSON
        import re
        
        # æŸ¥æ‰¾JSONéƒ¨åˆ† - æ”¯æŒå¤šç§æ ¼å¼
        json_match = re.search(r'\{[^{}]*"(?:image_description|image_analysis|problem_analysis)"[^{}]*\}', first_round_result, re.DOTALL)
        if not json_match:
            # å°è¯•æ›´å®½æ¾çš„åŒ¹é…
            json_match = re.search(r'\{.*\}', first_round_result, re.DOTALL)
        
        if json_match:
            json_str = json_match.group()
            print(f"æå–çš„JSON: {json_str[:200]}...")
            
            # è§£æJSON
            data = json.loads(json_str) 
            print("!debug!")
            print(data)
        
            
            # ä½¿ç”¨PydanticéªŒè¯
            analysis = FirstRoundAnalysis(**data)
            print(f"è§£ææˆåŠŸ: need_search={analysis.need_search}, search_query={analysis.search_query}")
           
            return analysis
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•è§£æ
            print("è­¦å‘Šï¼šæœªæ‰¾åˆ°JSONæ ¼å¼ï¼Œä½¿ç”¨ä¼ ç»Ÿè§£ææ–¹æ³•")
            return _parse_legacy_format(first_round_result)
            
    except json.JSONDecodeError as e:
        print(f"JSONè§£æé”™è¯¯: {e}")
        print("å°è¯•ä½¿ç”¨ä¼ ç»Ÿè§£ææ–¹æ³•")
        return _parse_legacy_format(first_round_result)
    except Exception as e:
        print(f"è§£æç¬¬ä¸€è½®ç»“æœæ—¶å‡ºé”™: {e}")
        return _parse_legacy_format(first_round_result)

def _parse_legacy_format(first_round_result: str) -> FirstRoundAnalysis:
    """ä¼ ç»Ÿæ ¼å¼è§£æï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
    # ç®€å•çš„å…³é”®è¯æ£€æµ‹
    search_indicators = ["éœ€è¦æœç´¢", "éœ€è¦ç½‘ç»œæœç´¢", "éœ€è¦æŸ¥è¯¢", "éœ€è¦è·å–", "æœç´¢å…³é”®è¯", "æœç´¢ç›®çš„"]
    need_search = any(indicator in first_round_result for indicator in search_indicators)
    
    # æå–æœç´¢å…³é”®è¯
    search_query = ""
    if need_search:
        lines = first_round_result.split('\n')
        for line in lines:
            if "æœç´¢å…³é”®è¯" in line or "æœç´¢ç›®çš„" in line:
                search_query = line.strip()
                break
    
    return FirstRoundAnalysis(
        image_analysis=first_round_result[:200] + "..." if len(first_round_result) > 200 else first_round_result,
        need_search=need_search,
        search_query=search_query if search_query else None,
        search_reason="ä¼ ç»Ÿè§£ææ–¹æ³•" if need_search else None
    )

def perform_search_if_needed(query: str, first_round_result: str) -> str:
    """å¦‚æœéœ€è¦æœç´¢ï¼Œæ‰§è¡Œæœç´¢"""
    # å¤„ç† first_round_result ä¸º None çš„æƒ…å†µ
    if not first_round_result:
        print("ç¬¬ä¸€è½®ç»“æœä¸ºç©ºï¼Œè·³è¿‡æœç´¢")
        return ""
    
    # ä½¿ç”¨æ–°çš„è§£ææ–¹æ³•
    analysis = parse_first_round_result(first_round_result)
    
    if not analysis.need_search or not SEARCH_AVAILABLE:
        print("ä¸éœ€è¦æœç´¢æˆ–æœç´¢å·¥å…·ä¸å¯ç”¨")
        return ""
    
    print(f"\n=== æ‰§è¡Œæœç´¢ ===")
    print(f"æœç´¢åŸå› : {analysis.search_reason}")
    print(f"æœç´¢æŸ¥è¯¢: {analysis.search_query}")
    
    try:
        # åˆ›å»ºæœç´¢åŠ©æ‰‹
        search_assistant = LLMWithSearch()
        
        # ä½¿ç”¨è§£æå‡ºçš„æœç´¢æŸ¥è¯¢
        search_query = analysis.search_query or f"{query} {analysis.search_reason}"
        
        # æ‰§è¡Œæœç´¢
        search_result = search_assistant.answer_with_search(search_query, num_results=5)
        
        print(f"æœç´¢å®Œæˆï¼Œè·å¾— {len(search_result.get('search_results', ''))} å­—ç¬¦çš„æœç´¢ç»“æœ")
        print(f"æœç´¢å®Œæˆï¼Œå†…å®¹æ˜¯{search_result.get('search_results', '')} ")
        return search_result
        
    except Exception as e:
        print(f"æœç´¢è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return ""

def generate_sci_tool(query: str, answer:str,image_paths: List[str] = None, search_results: str = None,subfield:str = None, use_stream: bool = False):
    """ç”Ÿæˆç§‘å­¦å·¥å…·çš„ä¸»å‡½æ•°
    
    Args:
        query: ç§‘å­¦é—®é¢˜
        answer: æ ‡å‡†ç­”æ¡ˆ
        image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
        search_results: é¢„å®šä¹‰çš„æœç´¢ç»“æœï¼ˆå¯é€‰ï¼‰
        subfield: å­¦ç§‘å­é¢†åŸŸ
        use_stream: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡ºï¼ˆé»˜è®¤Falseï¼Œä½¿ç”¨éæµå¼ã€‚è®¾ç½®ä¸ºTrueå¯é¿å…é•¿å“åº”è¶…æ—¶ï¼‰
    """
    print(f"å¼€å§‹ç”Ÿæˆç§‘å­¦å·¥å…·...")
    print(f"é—®é¢˜: {query}")
    print(f"è¾“å‡ºæ¨¡å¼: {'æµå¼' if use_stream else 'éæµå¼'}")
    # print(f"å›¾ç‰‡: {image_paths if image_paths else 'æ— å›¾ç‰‡'}")
    
    # ç¬¬ä¸€è½®ï¼šé—®é¢˜åˆ†æï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
    if image_paths:
        round_name = "å›¾ç‰‡è¯†åˆ«å’Œæè¿°"
    else:
        round_name = "é—®é¢˜åˆ†æå’Œæœç´¢åˆ¤æ–­"
    print(f"\n=== ç¬¬ä¸€è½®ï¼š{round_name} ===")
    first_round_result = None
    analysis = None
    max_retries = 3
    
    for attempt in range(max_retries):
        print(f"å°è¯•ç¬¬ {attempt + 1} æ¬¡...")
        first_round_conversations = create_first_round_conversation(query, image_paths,subfield)
        first_round_result = multi_turn_chat(first_round_conversations, stream=use_stream)
        
        # æå–ç¬¬ä¸€è½®çš„ç»“æœ
        first_round_analysis = None
        for turn in first_round_result:
            if turn.role == "assistant":
                first_round_analysis = turn.content
                break
        
        print(f"\nç¬¬ä¸€è½®åˆ†æç»“æœ: {first_round_analysis[:200] if first_round_analysis else 'None'}...")
        
        # è§£æç¬¬ä¸€è½®ç»“æœ
        analysis = parse_first_round_result(first_round_analysis)
        print(f"è§£æç»“æœ: éœ€è¦æœç´¢={analysis.need_search}, æœç´¢æŸ¥è¯¢={analysis.search_query}")
        
        # æ£€æŸ¥æ˜¯å¦æˆåŠŸè§£æä¸ºJSONæ ¼å¼
        if analysis.search_query is not None or not analysis.need_search:
            print("âœ“ JSONæ ¼å¼è§£ææˆåŠŸ")
            break
        else:
            print(f"âœ— JSONæ ¼å¼è§£æå¤±è´¥ï¼Œå°è¯•é‡è¯•...")
            if attempt < max_retries - 1:
                print("ç­‰å¾…2ç§’åé‡è¯•...")
                import time
                time.sleep(2)
    
    if analysis is None:
        print("è­¦å‘Šï¼šå¤šæ¬¡é‡è¯•åä»æ— æ³•è§£æJSONæ ¼å¼ï¼Œä½¿ç”¨é»˜è®¤å€¼")
        if image_paths:
            analysis = FirstRoundAnalysis(
                image_analysis=first_round_analysis or "æ— æ³•è§£æå›¾ç‰‡å†…å®¹",
                need_search=True,
                search_query="pubchemæ•°æ®åº“ä½¿ç”¨ä¸ä¸‹è½½æ–¹å¼",
                search_reason="éœ€è¦è·å–ä¸“ä¸šçš„åŒ–å­¦è®¡ç®—æ–¹æ³•å’Œæ•°æ®åº“ä¿¡æ¯"
            )
        else:
            analysis = FirstRoundAnalysis(
                image_analysis=first_round_analysis or "æ— æ³•è§£æé—®é¢˜å†…å®¹",
                need_search=True,
                search_query="åŒ–å­¦æº¶è§£å¹³è¡¡è®¡ç®—æ–¹æ³•å’ŒpHå€¼è®¡ç®—å·¥å…·",
                search_reason="éœ€è¦è·å–ä¸“ä¸šçš„åŒ–å­¦è®¡ç®—æ–¹æ³•å’Œæ•°æ®åº“ä¿¡æ¯æ¥å‡†ç¡®è®¡ç®—æº¶è§£å¹³è¡¡å’ŒpHå€¼"
            )
    
    # åˆ¤æ–­æ˜¯å¦éœ€è¦æœç´¢å¹¶æ‰§è¡Œæœç´¢
    actual_search_results = search_results  # ä½¿ç”¨ä¼ å…¥çš„æœç´¢ç»“æœ
    summary_search = None  # åˆå§‹åŒ–æœç´¢æ€»ç»“
    
    if not actual_search_results:  # å¦‚æœæ²¡æœ‰ä¼ å…¥æœç´¢ç»“æœï¼Œåˆ™æ ¹æ®ç¬¬ä¸€è½®ç»“æœå†³å®šæ˜¯å¦æœç´¢
        actual_search_results = perform_search_if_needed(query, first_round_analysis)
    
    # å¦‚æœæœ‰æœç´¢ç»“æœï¼Œè¿›è¡Œæ€»ç»“
    if actual_search_results and isinstance(actual_search_results, dict) and "prompt" in actual_search_results:
        try:
            client = OpenAI(
                api_key="sk-dkqEVEHBBbWtdmwLeyc0xyGxfcNTTHTESX5cmr4jxIh6S00M",
                base_url="https://zjuapi.com/v1"
            ) 

            api_messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæ€»ç»“åŠ©æ‰‹"},
                {"role": "user", "content": actual_search_results["prompt"]}
            ]
            
            if use_stream:
                # ä½¿ç”¨æµå¼è¾“å‡ºé¿å…è¶…æ—¶
                print("ğŸ”„ æœç´¢æ€»ç»“ä½¿ç”¨æµå¼è¾“å‡ºæ¨¡å¼...")
                summary_search = _stream_chat_completion(
                    client, 
                    api_messages, 
                    model="claude-sonnet-4-5-20250929", 
                    temperature=0.7
                )
            else:
                # ä½¿ç”¨éæµå¼è¾“å‡º
                print("ğŸ“ æœç´¢æ€»ç»“ä½¿ç”¨éæµå¼è¾“å‡ºæ¨¡å¼...")
                response = client.chat.completions.create(
                    model="anthropic/claude-sonnet-4.5",
                    messages=api_messages,
                    temperature=0.7
                )
                summary_search = response.choices[0].message.content
            
            print(f"\nâœ“ æœç´¢æ€»ç»“å®Œæˆï¼Œé•¿åº¦: {len(summary_search)} å­—ç¬¦")
        except Exception as e:
            print(f"âš  æœç´¢æ€»ç»“å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æœç´¢ç»“æœ")
            summary_search = actual_search_results.get("search_results", str(actual_search_results))
    elif actual_search_results:
        # å¦‚æœactual_search_resultsæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
        summary_search = str(actual_search_results)
    
    # ç¬¬äºŒè½®ï¼šç§‘å­¦å·¥å…·ç”Ÿæˆ
    stream_mode_text = "æµå¼è¾“å‡ºæ¨¡å¼" if use_stream else "éæµå¼è¾“å‡ºæ¨¡å¼"
    print(f"\n=== ç¬¬äºŒè½®ï¼šç§‘å­¦å·¥å…·ç”Ÿæˆï¼ˆ{stream_mode_text}ï¼‰===")
    second_round_conversations = create_second_round_conversation(
        query, answer, image_paths, 
        summary_search if summary_search else "[æ— æœç´¢ç»“æœ]", 
        analysis.get_analysis_content()
    )
    # æ ¹æ®use_streamå‚æ•°å†³å®šæ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º
    second_round_result = multi_turn_chat(second_round_conversations, stream=use_stream)
    
    # åˆå¹¶ä¸¤è½®ç»“æœ
    complete_result = first_round_result + second_round_result
    
    # æ˜¾ç¤ºå®Œæ•´å¯¹è¯å†å²
    display_conversation_history(complete_result)
    
    return complete_result

def create_first_round_conversation(query: str, image_paths: List[str] = None,subfield:str = None) -> List[ConversationTurn]:
    """åˆ›å»ºç¬¬ä¸€è½®å¯¹è¯ï¼šå›¾ç‰‡è¯†åˆ«å’Œæè¿°ï¼Œå¹¶åˆ¤æ–­æ˜¯å¦éœ€è¦æœç´¢"""
    if image_paths is None:
        image_paths = []
    elif isinstance(image_paths, str):
        image_paths = [image_paths]
    
    # æ ¹æ®æ˜¯å¦æœ‰å›¾ç‰‡è°ƒæ•´ç³»ç»Ÿæç¤º
    if image_paths:
        system_instruction = f"ä½ æ˜¯ä¸€ä¸ª{subfield}å­¦ç§‘ä¸“å®¶èƒ½è¯†åˆ«å„ç§å­¦ç§‘ä¸“ä¸šçš„å›¾ç‰‡ï¼Œè¯·å¯¹å›¾ç‰‡çš„å†…å®¹ã€ç»†èŠ‚ä¸å…³é”®ä¿¡æ¯ä½œå‡ºæè¿°ã€‚"
    else:
        system_instruction = "ä½ æ˜¯ä¸€ä¸ª{subfield}å­¦ç§‘ä¸“å®¶ï¼Œæ“…é•¿åˆ†æå„ç§å­¦ç§‘ä¸“ä¸šé—®é¢˜ï¼Œä¸“æ³¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦ç½‘ç»œæœç´¢æ¥è·å–æ›´å¤šä¿¡æ¯ã€‚"
    
    first_round_system = f"""{system_instruction}

ä½ éœ€è¦åˆ¤æ–­å½“å‰é—®é¢˜æ˜¯å¦éœ€è¦é¢å¤–çš„ç½‘ç»œæœç´¢æ¥è·å–æ›´å¤šä¿¡æ¯ã€‚å¦‚æœéœ€è¦æœç´¢ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºéœ€è¦æœç´¢çš„å…³é”®è¯å’Œæœç´¢ç›®çš„ã€‚

é‡è¦ï¼šä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§æŒ‡å®šçš„JSONæ ¼å¼è¿”å›ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—ã€‚"""
    
    # æ ¹æ®æ˜¯å¦æœ‰å›¾ç‰‡è°ƒæ•´ç”¨æˆ·æ¶ˆæ¯
    if image_paths:
        first_round_user = """
ä»»åŠ¡ï¼š
1. è¯†åˆ«å›¾ç‰‡ä¸­çš„å­¦ç§‘é¢†åŸŸã€å…³é”®å…ƒç´ å’Œä¸“ä¸šä¿¡æ¯
2. æè¿°å›¾ç‰‡çš„æ ¸å¿ƒå†…å®¹å’Œç»†èŠ‚ç‰¹å¾
3. åˆ¤æ–­æ˜¯å¦éœ€è¦æœç´¢å¤–éƒ¨èµ„æº
4. ç”Ÿæˆé’ˆå¯¹å›¾ç‰‡å†…å®¹çš„ä¼˜åŒ–æœç´¢æŸ¥è¯¢

å›¾ç‰‡åˆ†æè¦ç‚¹ï¼š
- è¯†åˆ«å­¦ç§‘ç±»å‹ï¼ˆæ•°å­¦å…¬å¼ã€åŒ–å­¦ç»“æ„ã€ç”Ÿç‰©å›¾è°±ã€å·¥ç¨‹å›¾çº¸ã€ææ–™ç‰¹å¾ç­‰ï¼‰
- æå–å…³é”®ç¬¦å·ã€æœ¯è¯­ã€æ•°æ®
- åˆ¤æ–­å›¾ç‰‡å±•ç¤ºçš„å…·ä½“é—®é¢˜æˆ–æ¦‚å¿µ

æœç´¢æŸ¥è¯¢è§„åˆ™ï¼š
- åŸºäºå†…å®¹ç”Ÿæˆ3-6ä¸ªå…³é”®è¯(å¯ä»¥é‡‡ç”¨å¤šè¯­è¨€çš„å…³é”®è¯è”åˆæœç´¢)
- åŒ…å«è¯†åˆ«åˆ°çš„ä¸“ä¸šæœ¯è¯­ã€ç¬¦å·ã€æ¦‚å¿µåç§°
- æ·»åŠ é™å®šè¯ï¼štutorial, example, solution, æ•™ç¨‹, è§£æ,è¯¦è§£,è§£è¯»ç­‰
- å¦‚æœå›¾ç‰‡åŒ…å«ç‰¹å®šç¬¦å·/å…¬å¼ï¼Œåœ¨æŸ¥è¯¢ä¸­ä½“ç° 
- æœ‰ç±»ä¼¼åƒçŸ¥ä¹/è´´å§/å­¦æœ¯è®ºå›/å…¬ä¼—å·è¿™æ ·çš„ç½‘å€å¯ä»¥é‡ç‚¹æŸ¥çœ‹

è¿”å›JSONæ ¼å¼ï¼š
{{
    "image_description": "è¯¦ç»†æè¿°å›¾ç‰‡å†…å®¹ã€å­¦ç§‘é¢†åŸŸã€å…³é”®å…ƒç´ å’Œç»†èŠ‚ä¿¡æ¯",
    "identified_concepts": ["æ¦‚å¿µ1", "æ¦‚å¿µ2", "æ¦‚å¿µ3"],
    "problem_analysis": "åˆ†æå›¾ç‰‡å±•ç¤ºçš„æ ¸å¿ƒé—®é¢˜å’Œå­¦ç§‘ä»»åŠ¡",
    "need_search": true/false,
    "search_query": "åŸºäºå›¾ç‰‡å†…å®¹çš„æœç´¢æŸ¥è¯¢ï¼ˆå…³é”®è¯å½¢å¼ï¼‰",
    "search_reason": "éœ€è¦æœç´¢ä»€ä¹ˆç±»å‹çš„èµ„æºæ¥è§£å†³å›¾ç‰‡ä¸­çš„é—®é¢˜"
}}

ç¤ºä¾‹1 - æ•°å­¦å…¬å¼å›¾ç‰‡ï¼š
{{
    "image_description": "å›¾ç‰‡å±•ç¤ºä¸€ä¸ªå¾®åˆ†æ–¹ç¨‹ï¼šdy/dx + p(x)y = q(x)ï¼Œå±äºä¸€é˜¶çº¿æ€§å¾®åˆ†æ–¹ç¨‹ï¼Œæ—è¾¹æœ‰åˆå§‹æ¡ä»¶y(0)=1",
    "identified_concepts": ["ä¸€é˜¶çº¿æ€§å¾®åˆ†æ–¹ç¨‹", "åˆå€¼é—®é¢˜", "å¸¸å¾®åˆ†æ–¹ç¨‹"],
    "problem_analysis": "æ ¸å¿ƒæ˜¯æ±‚è§£ä¸€é˜¶çº¿æ€§å¾®åˆ†æ–¹ç¨‹çš„åˆå€¼é—®é¢˜ï¼Œå±äºå¾®ç§¯åˆ†/å¸¸å¾®åˆ†æ–¹ç¨‹é¢†åŸŸ",
    "need_search": true,
    "search_query": "first order linear differential equation solution method æˆ– ä¸€é˜¶çº¿æ€§å¾®åˆ†æ–¹ç¨‹ æ±‚è§£æ­¥éª¤",
    "search_reason": "éœ€è¦è·å–ä¸€é˜¶çº¿æ€§å¾®åˆ†æ–¹ç¨‹çš„æ ‡å‡†æ±‚è§£æ–¹æ³•å’Œæ­¥éª¤æ•™ç¨‹"
}}

ç¤ºä¾‹2 - åŒ–å­¦ç»“æ„å¼ï¼š
{{
    "image_description": "å›¾ç‰‡æ˜¾ç¤ºä¸€ä¸ªæœ‰æœºåŒ–åˆç‰©ç»“æ„å¼ï¼ŒåŒ…å«è‹¯ç¯ã€ç¾ŸåŸº(-OH)å’Œç¾§åŸº(-COOH)ï¼Œç–‘ä¼¼æ°´æ¨é…¸ç»“æ„",
    "identified_concepts": ["æœ‰æœºåŒ–åˆç‰©", "è‹¯ç¯", "ç¾ŸåŸº", "ç¾§åŸº", "æ°´æ¨é…¸"],
    "problem_analysis": "æ ¸å¿ƒæ˜¯è¯†åˆ«å’Œå‘½åæœ‰æœºåŒ–åˆç‰©ç»“æ„ï¼Œå±äºæœ‰æœºåŒ–å­¦é¢†åŸŸçš„ç»“æ„è§£æä»»åŠ¡",
    "need_search": true,
    "search_query": "salicylic acid structure properties æˆ– æ°´æ¨é…¸ åŒ–å­¦æ€§è´¨ ååº”",
    "search_reason": "éœ€è¦è·å–è¯¥åŒ–åˆç‰©çš„æ ‡å‡†å‘½åã€æ€§è´¨å’Œç›¸å…³ååº”ä¿¡æ¯"
}}

ç¤ºä¾‹3 - ç”µè·¯å›¾ï¼š
{{
    "image_description": "å›¾ç‰‡å±•ç¤ºä¸€ä¸ªRCä¸²è”ç”µè·¯ï¼ŒåŒ…å«ç”µé˜»R=10kÎ©ã€ç”µå®¹C=100Î¼Fã€ç”µæºV=5Vï¼Œæ ‡æ³¨äº†ç”µå‹å’Œç”µæµæ–¹å‘",
    "identified_concepts": ["RCç”µè·¯", "ä¸²è”ç”µè·¯", "ç”µå®¹å……æ”¾ç”µ", "æ—¶é—´å¸¸æ•°"],
    "problem_analysis": "æ ¸å¿ƒæ˜¯åˆ†æRCç”µè·¯çš„å……æ”¾ç”µè¿‡ç¨‹ï¼Œå±äºç”µè·¯åˆ†æé¢†åŸŸçš„ç¬æ€å“åº”é—®é¢˜",
    "need_search": true,
    "search_query": "RC circuit charging discharging calculation æˆ– RCç”µè·¯ æ—¶é—´å¸¸æ•° è®¡ç®—å…¬å¼",
    "search_reason": "éœ€è¦è·å–RCç”µè·¯çš„å……æ”¾ç”µå…¬å¼ã€æ—¶é—´å¸¸æ•°è®¡ç®—æ–¹æ³•å’Œæ³¢å½¢åˆ†æ"
}}

ç¤ºä¾‹4 - ç”Ÿç‰©å›¾è°±ï¼š
{{
    "image_description": "å›¾ç‰‡æ˜¾ç¤ºç»†èƒæœ‰ä¸åˆ†è£‚çš„ä¸åŒé˜¶æ®µç¤ºæ„å›¾ï¼ŒåŒ…å«å‰æœŸã€ä¸­æœŸã€åæœŸã€æœ«æœŸçš„æŸ“è‰²ä½“å½¢æ€å˜åŒ–",
    "identified_concepts": ["æœ‰ä¸åˆ†è£‚", "æŸ“è‰²ä½“", "ç»†èƒåˆ†è£‚", "ç»†èƒå‘¨æœŸ"],
    "problem_analysis": "æ ¸å¿ƒæ˜¯ç†è§£ç»†èƒæœ‰ä¸åˆ†è£‚çš„å„ä¸ªé˜¶æ®µç‰¹å¾ï¼Œå±äºç»†èƒç”Ÿç‰©å­¦é¢†åŸŸ",
    "need_search": true,
    "search_query": "mitosis stages diagram explanation æˆ– æœ‰ä¸åˆ†è£‚ å„æ—¶æœŸç‰¹ç‚¹ å›¾è§£",
    "search_reason": "éœ€è¦è·å–æœ‰ä¸åˆ†è£‚å„é˜¶æ®µçš„è¯¦ç»†è§£é‡Šå’Œç‰¹å¾å¯¹æ¯”"
}}

ç¤ºä¾‹5 - å‡ ä½•å›¾å½¢ï¼š
{{
    "image_description": "å›¾ç‰‡å±•ç¤ºä¸€ä¸ªä¸‰è§’å½¢ABCï¼Œæ ‡æ³¨äº†è¾¹é•¿a=5, b=7, c=8ï¼Œæ±‚è§’Açš„åº¦æ•°",
    "identified_concepts": ["ä¸‰è§’å½¢", "ä½™å¼¦å®šç†", "è§£ä¸‰è§’å½¢"],
    "problem_analysis": "æ ¸å¿ƒæ˜¯åˆ©ç”¨ä¸‰è¾¹é•¿æ±‚è§’åº¦ï¼Œå±äºä¸‰è§’å‡½æ•°/è§£æå‡ ä½•é¢†åŸŸ",
    "need_search": true,
    "search_query": "law of cosines formula calculator æˆ– ä½™å¼¦å®šç† æ±‚è§’åº¦ å…¬å¼",
    "search_reason": "éœ€è¦è·å–ä½™å¼¦å®šç†çš„å…¬å¼å’Œè®¡ç®—æ­¥éª¤"
}}

ç¤ºä¾‹6 - å›¾è¡¨æ•°æ®ï¼š
{{
    "image_description": "å›¾ç‰‡æ˜¾ç¤ºä¸€ä¸ªæŠ˜çº¿å›¾ï¼Œæ¨ªè½´æ˜¯æ—¶é—´(2020-2024)ï¼Œçºµè½´æ˜¯é”€å”®é¢ï¼Œå±•ç¤ºäº†5å¹´çš„å¢é•¿è¶‹åŠ¿",
    "identified_concepts": ["æ—¶é—´åºåˆ—", "è¶‹åŠ¿åˆ†æ", "æ•°æ®å¯è§†åŒ–"],
    "problem_analysis": "æ ¸å¿ƒæ˜¯åˆ†ææ—¶é—´åºåˆ—æ•°æ®çš„å¢é•¿è¶‹åŠ¿ï¼Œå±äºæ•°æ®åˆ†æ/ç»Ÿè®¡å­¦é¢†åŸŸ",
    "need_search": true,
    "search_query": "time series trend analysis methods æˆ– æ—¶é—´åºåˆ— è¶‹åŠ¿åˆ†æ Python",
    "search_reason": "éœ€è¦è·å–æ—¶é—´åºåˆ—åˆ†æçš„æ–¹æ³•å’Œå·¥å…·"
}}

å…³é”®ç‚¹ï¼š
- image_descriptionè¦è¯¦ç»†å…·ä½“ï¼ŒåŒ…å«å…³é”®ç¬¦å·ã€æ•°å€¼ã€æ ‡æ³¨
- identified_conceptsæå–3-5ä¸ªæ ¸å¿ƒæ¦‚å¿µ
- search_queryåŸºäºè¯†åˆ«åˆ°çš„ä¸“ä¸šå†…å®¹ç”Ÿæˆï¼Œä½¿ç”¨å‡†ç¡®æœ¯è¯­
- å¦‚æœå›¾ç‰‡å†…å®¹ä¸æ¸…æ™°æˆ–æ— æ³•è¯†åˆ«ï¼Œåœ¨image_descriptionä¸­è¯´æ˜

ç°åœ¨è¯·åˆ†æå›¾ç‰‡å¹¶ç”Ÿæˆæœç´¢æŸ¥è¯¢ï¼š
é—®é¢˜ä¸å›¾ç‰‡ï¼š{query}\n{images}
""".format(query= query,images = image_paths)
    else:
        first_round_user = """åˆ†æé¢˜ç›®å¹¶ç”ŸæˆGoogleæœç´¢æŸ¥è¯¢ã€‚
ä»»åŠ¡ï¼š
1. åˆ†æé—®é¢˜çš„æ ¸å¿ƒéœ€æ±‚ã€å­¦ç§‘é¢†åŸŸå’Œå…·ä½“ä»»åŠ¡
2. åˆ¤æ–­æ˜¯å¦éœ€è¦æœç´¢å¤–éƒ¨èµ„æºæ¯”å¦‚åŒ–å­¦ååº”æ•°æ®åº“ï¼Œç”Ÿç‰©åŸºå› æ•°æ®åº“ç­‰
3. å¦‚æœéœ€è¦æœç´¢ï¼Œç”Ÿæˆä¼˜åŒ–çš„Googleæœç´¢æŸ¥è¯¢


æœç´¢æŸ¥è¯¢è§„åˆ™ï¼š
- åŸºäºå›¾ç‰‡å†…å®¹ç”Ÿæˆ3-6ä¸ªå…³é”®è¯
- åŒ…å«è¯†åˆ«åˆ°çš„ä¸“ä¸šæœ¯è¯­ã€ç¬¦å·ã€æ¦‚å¿µåç§°
- æ·»åŠ é™å®šè¯ï¼štutorial, example, solution, æ•™ç¨‹, è§£æ,è¯¦è§£,è§£è¯»ç­‰
- å¦‚æœå›¾ç‰‡åŒ…å«ç‰¹å®šç¬¦å·/å…¬å¼ï¼Œåœ¨æŸ¥è¯¢ä¸­ä½“ç° 
- æœ‰ç±»ä¼¼åƒçŸ¥ä¹/è´´å§/å­¦æœ¯è®ºå›/å…¬ä¼—å·è¿™æ ·çš„ç½‘å€å¯ä»¥é‡ç‚¹æŸ¥çœ‹

è¿”å›JSONæ ¼å¼ï¼š
{{
    "problem_analysis": "åˆ†æé—®é¢˜çš„æ ¸å¿ƒéœ€æ±‚å’Œç»†åˆ†å­¦ç§‘é¢†åŸŸä¸å­¦ç§‘ä»»åŠ¡",
    "need_search": true/false,
    "search_query": "ä¼˜åŒ–åçš„æœç´¢æŸ¥è¯¢ï¼ˆå…³é”®è¯å½¢å¼ï¼‰",
    "search_reason": "ä¸ºä»€ä¹ˆéœ€è¦æœç´¢ï¼ŒæœŸæœ›æ‰¾åˆ°ä»€ä¹ˆç±»å‹çš„èµ„æº"
}}

ç¤ºä¾‹1 - éœ€è¦æ•°æ®åº“ï¼š
é—®é¢˜ï¼šå¦‚ä½•è¿›è¡Œè›‹ç™½è´¨ç»“æ„é¢„æµ‹ï¼Ÿ
{{
    "problem_analysis": "æ ¸å¿ƒéœ€æ±‚æ˜¯è›‹ç™½è´¨ç»“æ„é¢„æµ‹æ–¹æ³•ï¼Œå±äºç”Ÿç‰©ä¿¡æ¯å­¦é¢†åŸŸçš„ç»“æ„é¢„æµ‹ä»»åŠ¡",
    "need_search": true,
    "search_query": "protein structure database PDB open source æˆ– AlphaFold dataset github",
    "search_reason": "éœ€è¦è·å–è›‹ç™½è´¨ç»“æ„æ•°æ®åº“ï¼ˆå¦‚PDBï¼‰å’Œå¼€æºé¢„æµ‹å·¥å…·çš„ç›¸å…³èµ„æº"
}}

ç¤ºä¾‹2 - éœ€è¦ç»éªŒæ–¹æ³•ï¼š
é—®é¢˜ï¼šæ·±åº¦å­¦ä¹ æ¨¡å‹å¦‚ä½•è°ƒå‚ï¼Ÿ
{{
    "problem_analysis": "æ ¸å¿ƒéœ€æ±‚æ˜¯æ·±åº¦å­¦ä¹ è¶…å‚æ•°ä¼˜åŒ–ï¼Œå±äºæœºå™¨å­¦ä¹ é¢†åŸŸçš„æ¨¡å‹è®­ç»ƒä»»åŠ¡",
    "need_search": true,
    "search_query": "æ·±åº¦å­¦ä¹  è°ƒå‚æŠ€å·§ çŸ¥ä¹ æˆ– hyperparameter tuning best practices",
    "search_reason": "éœ€è¦è·å–ä¸“å®¶æ€»ç»“çš„è°ƒå‚ç»éªŒå’Œå®æˆ˜æŠ€å·§"
}}

ç¤ºä¾‹3 - éœ€è¦æ•°æ®åº“å’Œæ–¹æ³•ï¼š
é—®é¢˜ï¼šå¦‚ä½•åˆ†æåŸºå› è¡¨è¾¾æ•°æ®ï¼Ÿ
{{
    "problem_analysis": "æ ¸å¿ƒéœ€æ±‚æ˜¯åŸºå› è¡¨è¾¾æ•°æ®åˆ†æï¼Œå±äºç”Ÿç‰©ä¿¡æ¯å­¦é¢†åŸŸçš„è½¬å½•ç»„å­¦åˆ†æä»»åŠ¡",
    "need_search": true,
    "search_query": "gene expression database GEO NCBI æˆ– RNA-seq analysis tutorial",
    "search_reason": "éœ€è¦è·å–åŸºå› è¡¨è¾¾æ•°æ®åº“ï¼ˆå¦‚GEOï¼‰å’Œåˆ†ææµç¨‹æ•™ç¨‹"
}}

ç¤ºä¾‹4 - ä¸éœ€è¦æœç´¢ï¼š
é—®é¢˜ï¼š1+1ç­‰äºå‡ ï¼Ÿ
{{
    "problem_analysis": "ç®€å•çš„æ•°å­¦åŠ æ³•è®¡ç®—ï¼Œå±äºåŸºç¡€ç®—æœ¯",
    "need_search": false,
    "search_query": "",
    "search_reason": ""
}}

å…³é”®ç‚¹ï¼š
- search_queryå¿…é¡»æ˜¯å…³é”®è¯ç»„åˆï¼ˆå¦‚"protein database PDB"ï¼‰ï¼Œä¸è¦å†™æˆé—®å¥
- å¯ä»¥ç”¨"æˆ–"è¿æ¥å¤šä¸ªæŸ¥è¯¢ç­–ç•¥
- ä¼˜å…ˆä½¿ç”¨é¢†åŸŸä¸“ä¸šæœ¯è¯­
- æ ¹æ®èµ„æºç±»å‹é€‰æ‹©ä¸­è‹±æ–‡

ç°åœ¨è¯·ä¸ºä»¥ä¸‹é—®é¢˜ç”Ÿæˆæœç´¢æŸ¥è¯¢ï¼š
é—®é¢˜ï¼š{query}
""".format(query = query)
   
    return [
        ConversationTurn("system", first_round_system),
        ConversationTurn("user", first_round_user, images=image_paths)
    ]

def create_second_round_conversation(query: str, answer:str,image_paths: List[str] = None, search_results: str = None, first_round_result: str = None) -> List[ConversationTurn]:
    """åˆ›å»ºç¬¬äºŒè½®å¯¹è¯ï¼šç§‘å­¦å·¥å…·ç”Ÿæˆ"""
    if image_paths is None:
        image_paths = []
    elif isinstance(image_paths, str):
        image_paths = [image_paths]
    
    # è¯»å–ç¬¬äºŒè½®çš„system prompt
    with open("../prompts/SystemPrompt_Science_Toolkit.md", 'r', encoding='utf-8') as f:
        second_round_system = f.read()
    
    # æ„å»ºç¬¬äºŒè½®çš„ç”¨æˆ·æ¶ˆæ¯
    second_round_user_parts = [
        f"ç§‘å­¦é—®é¢˜ï¼š{query}",
        f"æ ‡å‡†ç­”æ¡ˆï¼š{answer}",
        f"å›¾ç‰‡åˆ†æç»“æœï¼š{first_round_result if first_round_result else '[ç­‰å¾…ç¬¬ä¸€è½®åˆ†æç»“æœ]'}",
        f"æœç´¢å·¥å…·ç»“æœï¼š{search_results if search_results else '[ç­‰å¾…æœç´¢å·¥å…·ç»“æœ]'}"
    ]

    second_round_user = "\n\n".join(second_round_user_parts)
    
    return [
        ConversationTurn("system", second_round_system),
        ConversationTurn("user", second_round_user, images=image_paths)
    ]

def load_data_file(file_path: str) -> List[Dict[str, Any]]:
    """åŠ è½½æ•°æ®æ–‡ä»¶ï¼Œæ”¯æŒ JSON å’Œ JSONL ä¸¤ç§æ ¼å¼
    
    Args:
        file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        
    Returns:
        æ•°æ®åˆ—è¡¨
    """
    # æ ¹æ®æ–‡ä»¶æ‰©å±•ååˆ¤æ–­æ ¼å¼
    if file_path.endswith('.jsonl'):
        # JSONL æ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡
        datasets = []
        with open(file_path, "r", encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:  # è·³è¿‡ç©ºè¡Œ
                    continue
                try:
                    data = json.loads(line)
                    datasets.append(data)
                except json.JSONDecodeError as e:
                    print(f"è­¦å‘Šï¼šç¬¬ {line_num} è¡Œ JSON è§£æå¤±è´¥: {e}")
                    continue
        print(f"ä» JSONL æ–‡ä»¶åŠ è½½äº† {len(datasets)} æ¡æ•°æ®")
        return datasets
    else:
        # JSON æ ¼å¼ï¼šæ•´ä¸ªæ–‡ä»¶æ˜¯ä¸€ä¸ª JSON æ•°ç»„æˆ–å¯¹è±¡
        with open(file_path, "r", encoding='utf-8') as f:
            data = json.load(f)
            # å¦‚æœæ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
            if isinstance(data, dict):
                datasets = [data]
            elif isinstance(data, list):
                datasets = data
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ JSON æ ¼å¼ï¼šæœŸæœ›æ•°ç»„æˆ–å¯¹è±¡ï¼Œå¾—åˆ° {type(data)}")
        print(f"ä» JSON æ–‡ä»¶åŠ è½½äº† {len(datasets)} æ¡æ•°æ®")
        return datasets

def main():
    """ä¸»å‡½æ•° - ç¤ºä¾‹ç”¨æ³•"""
    # è®¾ç½®å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description='ç§‘å­¦å·¥å…·ç”Ÿæˆå™¨')
    parser.add_argument('--parser', action='store_true', help='å¯ç”¨ä»£ç è§£æåŠŸèƒ½')
    parser.add_argument('--data-file', default='./gpqa_physics_chemistry_problems_mechanics.json', help='æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒ JSON å’Œ JSONL æ ¼å¼ï¼‰')
    parser.add_argument('--stream', action='store_true', help='ä½¿ç”¨æµå¼è¾“å‡ºæ¨¡å¼ï¼ˆé¿å…é•¿å“åº”è¶…æ—¶ï¼Œé»˜è®¤ä½¿ç”¨éæµå¼ï¼‰')
    
    args = parser.parse_args()
    
    # ç¤ºä¾‹æ•°æ®
    problem = {
        "id": "C005/0009",
        "question": "æ ¹æ®åˆ†å­ç»“æ„ <image>ï¼Œè®¡ç®—äº”ä¸ªåˆ©å¹³æ–¯åŸºè§„åˆ™æŒ‡æ ‡ï¼Œå¹¶å°†å€¼å››èˆäº”å…¥åˆ°å°æ•°ç‚¹åä¸€ä½ï¼šåˆ†å­é‡ã€LogPã€æ°¢é”®ä¾›ä½“æ•°é‡ã€æ°¢é”®å—ä½“æ•°é‡å’Œå¯æ—‹è½¬é”®æ•°é‡ã€‚è¯·ä»¥JSONå­—å…¸çš„å½¢å¼è¾“å‡ºï¼Œä½¿ç”¨ä»¥ä¸‹ç²¾ç¡®çš„é”®å€¼ï¼ˆä¸åŒ…å«å•ä½ï¼‰ï¼š\n\n{\n  \"åˆ†å­è´¨é‡\": ,\n  \"XLogP\": ,\n  \"æ°¢é”®ä¾›ä½“è®¡æ•°\": ,\n  \"æ°¢é”®å—ä½“è®¡æ•°\": ,\n  \"å¯æ—‹è½¬é”®è®¡æ•°\": \n}",
        "answer": "{'åˆ†å­è´¨é‡': 518.7, 'XLogP': -0.7, 'æ°¢é”®ä¾›ä½“è®¡æ•°': 7.0, 'æ°¢é”®å—ä½“è®¡æ•°': 10.0, 'å¯æ—‹è½¬é”®è®¡æ•°': 11.0}",
        "images": [
            "data/images/C005_0009_3165cfeaca24fa54f61d8a43cb277f17.png"
        ],
        "metadata": {
            "qustion_type": "exact_match",
            "field": "chemistry",
            "lang": "",
            "image_urls": [
                "https://huggingface.co/datasets/Soptq/sfe/resolve/main/images/C005_0009_3165cfeaca24fa54f61d8a43cb277f17.png"
            ],
            "source_dataset": "sfe"
        }
    } 
    
    # åŠ è½½æ•°æ®æ–‡ä»¶ï¼ˆæ”¯æŒ JSON å’Œ JSONLï¼‰
    datasets = load_data_file(args.data_file)  
    for i, data in enumerate(datasets):
        # idx = data["index"] if data.get("index") else data["id"] 
        if data.get("index"):
            idx = data["index"] 
        elif  data.get("id"):
            idx = data["id"]  
        else: 
            idx =  i 
   
        query = data["question"]
        # å…¼å®¹ä¸åŒçš„å›¾ç‰‡å­—æ®µåï¼šimage_path æˆ– images
        # å¦‚æœ image_path æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦‚æœæ˜¯ images æ•°ç»„ï¼Œä¹Ÿæ”¯æŒ
        image_paths = data.get("image_path")
        if not image_paths:
            image_paths = data.get("images")
        # ç¡®ä¿ image_paths æ˜¯åˆ—è¡¨æ ¼å¼ï¼ˆåç»­å‡½æ•°ä¼šå¤„ç†å­—ç¬¦ä¸²è½¬åˆ—è¡¨ï¼‰
        if image_paths is None:
            image_paths = []
        elif isinstance(image_paths, str):
            image_paths = [image_paths]
        # å¦‚æœ images æ˜¯ç©ºåˆ—è¡¨ï¼Œä¿æŒä¸ºç©ºåˆ—è¡¨
        elif isinstance(image_paths, list) and len(image_paths) == 0:
            image_paths = []
        
        answer = data["answer"] 
        # å…¼å®¹ä¸åŒçš„å­é¢†åŸŸå­—æ®µï¼šclassification_subfield æˆ– metadata.subfield
        subfield = data.get("classification_subfield")
        if not subfield and data.get("metadata"):
            subfield = data["metadata"].get("subfield")
        if not subfield:
            # å¦‚æœéƒ½æ²¡æœ‰ï¼Œå°è¯•ä» metadata.field è·å–
            if data.get("metadata"):
                subfield = data["metadata"].get("field")
            if not subfield:
                print(f"è­¦å‘Šï¼šç¬¬ {i+1} æ¡æ•°æ®æœªæ‰¾åˆ°å­é¢†åŸŸå­—æ®µï¼Œä½¿ç”¨é»˜è®¤å€¼")
                subfield = "Unknown"
  

  
        # ä¸ä¼ å…¥é¢„å®šä¹‰çš„æœç´¢ç»“æœï¼Œè®©ç³»ç»Ÿè‡ªåŠ¨åˆ¤æ–­æ˜¯å¦éœ€è¦æœç´¢
        search_results = None  # è®©ç³»ç»Ÿæ ¹æ®ç¬¬ä¸€è½®ç»“æœè‡ªåŠ¨å†³å®šæ˜¯å¦æœç´¢
        
        print("=" * 60)
        print("ç§‘å­¦å·¥å…·ç”Ÿæˆå™¨ - å¸¦æœç´¢åŠŸèƒ½")
        print("=" * 60)
        print(f"é—®é¢˜: {query}")
        # print(f"å›¾ç‰‡: {image_paths}")
        print(f"æœç´¢å·¥å…·å¯ç”¨: {SEARCH_AVAILABLE}")
        print(f"Parser åŠŸèƒ½: {'å¯ç”¨' if args.parser else 'ç¦ç”¨'}")
        print(f"è¾“å‡ºæ¨¡å¼: {'æµå¼' if args.stream else 'éæµå¼ï¼ˆé»˜è®¤ï¼‰'}")
        
        # ç”Ÿæˆç§‘å­¦å·¥å…·
        result = generate_sci_tool(query, answer, image_paths, search_results, subfield, use_stream=args.stream)
        
        # ä¿å­˜ç»“æœ
        serializable_result = []
        for turn in result:
            turn_dict = {
                "role": turn.role,
                "content": turn.content,
                "images": turn.images,
                "metadata": turn.metadata
            }
            serializable_result.append(turn_dict) 

       
        save_path = f"../result_mid/chem_bench_tool_result_{idx}.json" 

        with open(save_path, "w", encoding='utf-8') as f:
            json.dump(serializable_result, f, indent=4, ensure_ascii=False)
        
        print("ç»“æœå·²ä¿å­˜åˆ°", save_path) 

        
      

if __name__ == "__main__":
    main()
