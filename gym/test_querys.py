"""
æ‰¹é‡å¤šé¢˜è°ƒè¯•è„šæœ¬ï¼ˆå®éªŒæ€§ï¼‰

åŠŸèƒ½ï¼š
- ä½¿ç”¨é‡æ„åçš„ `simple_test_query` æˆ– `test_query` + è¯„ä»·é€»è¾‘ï¼Œå¯¹
  `gym/dataset/refine_merged_questions_augmented.json` ä¸­çš„æ‰€æœ‰æ¡ˆä¾‹é€ä¸€æµ‹è¯•ã€‚
- å¯¹æ¯ä¸ªæ¡ˆä¾‹ï¼š
  - è°ƒç”¨æ¨¡å‹ï¼ˆå¯é€‰ä½¿ç”¨å·¥å…·ï¼‰
  - æå– boxed ç­”æ¡ˆæˆ–ç»“æ„åŒ–ç­”æ¡ˆ
  - ä½¿ç”¨ LLM åˆ¤é¢˜æˆ–ç»“æ„åŒ–è¯„åˆ†ï¼ˆå¯¹/é”™/åˆ†æ•°ï¼‰
  - å°†å®Œæ•´å¯¹è¯è½¨è¿¹å’Œè¯„æµ‹ç»“æœè½ç›˜åˆ°ç»Ÿä¸€çš„ traces ç›®å½•

è½ç›˜ç›®å½•ç»“æ„ï¼ˆæ ¹æ®æ•°æ®é›†æ˜¯å¦ä¸ºå•æ¨¡æ€ / å¤šæ¨¡æ€è‡ªåŠ¨é€‰æ‹©ï¼‰ï¼š

- å•æ¨¡æ€ï¼šdata_analysis/tracetoanalyze/tracesmerged_single_questions/<model>/<mode>/<id>_trace.json
- å¤šæ¨¡æ€ï¼šdata_analysis/tracetoanalyze/tracesmerged_questions/<model>/<mode>/<id>_trace.json

è¿è¡Œæ–¹å¼ï¼ˆåœ¨é¡¹ç›®æ ¹ç›®å½•ï¼‰ï¼š

    python gym/test_querys.py

æˆ–æŒ‡å®šæ¨¡å‹ / æ˜¯å¦ç”¨å·¥å…· / æµ‹è¯•ç±»å‹ï¼š

    python -m gym.test_querys

æ³¨æ„ï¼š
- æœ¬æ–‡ä»¶ä¾èµ– `gym.test_executor` æ¨¡å—
- æ”¯æŒ test_type="normal"ï¼ˆboxed ç­”æ¡ˆè¯„ä¼°ï¼‰å’Œ test_type="refine"ï¼ˆç»“æ„åŒ–è¿‡ç¨‹è¯„ä¼°ï¼‰
"""

from __future__ import annotations
import sys
from pathlib import Path
_project_root = Path(__file__).resolve().parent.parent
for _module_name in list(sys.modules.keys()):
    if _module_name == 'gym' or _module_name.startswith('gym.'):
        del sys.modules[_module_name]
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import json
from typing import Any, Dict, List, Optional, Tuple
from gym.agent import DEFAULT_MODEL
from gym.test_executor import (
    simple_test_query,
    test_query,
    _resolve_dataset_folder,
    _resolve_mode_folder,
)
from gym.core.evaluator import extract_boxed_answer, is_answer_correct
from gym.core.data_loader import ensure_metadata_summary, normalize_image_path
from gym.core.exceptions import TestSkipException
def _derive_trace_path_for_multi(
    model_name: str,
    use_tools: bool,
    case_id: Any,
    mode_name: str,
    dataset_filename: str,
) -> Path:
    """
    å¤šé¢˜æ‰¹é‡æµ‹è¯•ä½¿ç”¨çš„ç®€åŒ–ç‰ˆ trace è·¯å¾„æ¨å¯¼é€»è¾‘ã€‚

    æ ¹æ®æ•°æ®é›†æ–‡ä»¶åæ˜¯å¦åŒ…å« "single"ï¼Œå†³å®šå•æ¨¡æ€ / å¤šæ¨¡æ€çš„çˆ¶ç›®å½•ï¼š

        data_analysis/tracetoanalyze/tracesmerged_single_questions
        data_analysis/tracetoanalyze/tracesmerged_questions

    å…¶ä¸‹æŒ‰å¦‚ä¸‹ç»“æ„ç»„ç»‡ï¼š

        [çˆ¶ç›®å½•] / model_name / mode_name / {case_id}_trace.json
    """
    is_single = "single" in dataset_filename.lower()

    project_root = Path(__file__).resolve().parents[1]
    traces_root = project_root / "data_analysis" / "tracetoanalyze"

    if is_single:
        base_root = traces_root / "tracesmerged_single_questions"
    else:
        base_root = traces_root / "tracesmerged_questions"

    model_dir = model_name or DEFAULT_MODEL
    base_dir = base_root / model_dir / mode_name
    base_dir.mkdir(parents=True, exist_ok=True)

    return base_dir / f"{case_id}_trace.json"

def _evaluate_refine_from_trace(
    case: Dict[str, Any],
    model_name: str,
    use_tools: bool,
    dataset_path: Path,
) -> Tuple[Optional[float], Optional[str], Optional[Path]]:
    """
    ä» trace æ–‡ä»¶ä¸­è¯»å– refine ç±»å‹æ¡ˆä¾‹çš„è¯„ä¼°ä¿¡æ¯ã€‚
    
    è¿”å›: (score, score_summary, trace_path)
    """
    case_id = case.get("id", "unknown")
    
    try:
        from gym.config.dataset_config import get_trace_root
        
        metadata = case.get("metadata") or {}
        dataset_folder = _resolve_dataset_folder(metadata)
        mode_folder = _resolve_mode_folder(use_tools, None)
        
        # åˆ¤æ–­æ˜¯ single è¿˜æ˜¯ multi
        dataset_filename = dataset_path.name
        if "single" in dataset_filename.lower():
            data_type_folder = "orignal_data_single"
        else:
            if "single" in dataset_folder.lower():
                data_type_folder = "orignal_data_single"
            else:
                data_type_folder = "orignal_data_multi"
        
        model_name_actual = model_name or DEFAULT_MODEL
        trace_root = get_trace_root(metadata.get("dataset_key"))
        trace_path = trace_root / model_name_actual / data_type_folder / mode_folder / f"{case_id}_trace.json"
        
        if trace_path.exists():
            with open(trace_path, "r", encoding="utf-8") as f:
                trace_data = json.load(f)
            
            # æå–è¯„ä¼°ä¿¡æ¯
            evaluation_score = trace_data.get("evaluation_score")
            evaluation_summary = trace_data.get("evaluation_summary", "æœªè¯„ä¼°")
            
            if evaluation_score is not None:
                return float(evaluation_score), evaluation_summary, trace_path
    
    except Exception as e:
        print(f"  âš ï¸ æ— æ³•è¯»å–è¯„ä¼°ä¿¡æ¯: {e}")
        import traceback
        traceback.print_exc()
    
    return None, None, None


def normalize_case_image_paths(case: Dict[str, Any]) -> None:
    """ç»Ÿä¸€å¤„ç†æ¡ˆä¾‹ä¸­çš„å›¾ç‰‡è·¯å¾„ï¼Œå°†æ—§è·¯å¾„è½¬æ¢ä¸ºæ–°çš„ç»Ÿä¸€è·¯å¾„æ ¼å¼
    
    ä¿®æ”¹ case ä¸­çš„ metadata.image_pathï¼Œå°†è·¯å¾„ä»ï¼š
    - "failed_question_images/xxx.jpg"
    - "filtered_images/xxx.jpg"
    - "/sfe_images/xxx.png"
    - "/r_bench/images/xxx.png"
    
    è½¬æ¢ä¸ºï¼š
    - "gym/test_images/failed_question_images/xxx.png"
    - "gym/test_images/filtered_images/xxx.png"
    - "gym/test_images/sfe_images/xxx.png"
    - "gym/test_images/r_bench/images/xxx.png"
    
    Args:
        case: æµ‹è¯•æ¡ˆä¾‹å­—å…¸ï¼Œä¼šè¢«åŸåœ°ä¿®æ”¹
    """
    if not isinstance(case, dict):
        return
    
    metadata = case.get("metadata")
    if not isinstance(metadata, dict):
        return
    
    image_paths = metadata.get("image_path")
    if not image_paths:
        return
    
    # å¤„ç†å•ä¸ªè·¯å¾„å­—ç¬¦ä¸²
    if isinstance(image_paths, str):
        normalized = normalize_image_path(image_paths)
        metadata["image_path"] = normalized
        return
    
    # å¤„ç†è·¯å¾„åˆ—è¡¨
    if isinstance(image_paths, (list, tuple)):
        normalized_paths = []
        for path in image_paths:
            if isinstance(path, str):
                normalized_paths.append(normalize_image_path(path))
            else:
                normalized_paths.append(path)
        metadata["image_path"] = normalized_paths


def _evaluate_and_save_trace(
    case: Dict[str, Any],
    result: Dict[str, Any],
    final_answer: str,
    model_name: str,
    use_tools: bool,
    dataset_path: Path,
    test_type: str = "normal",
) -> Tuple[Optional[bool], Optional[Path]]:
    """
    å¯¹å•ä¸ªæ¡ˆä¾‹æ‰§è¡Œï¼š
    - å¦‚æœ test_type="normal": boxed ç­”æ¡ˆæå– + LLM åˆ¤é¢˜ + trace è½ç›˜
    - å¦‚æœ test_type="refine": ä» trace æ–‡ä»¶è¯»å–è¯„ä¼°ä¿¡æ¯ï¼ˆtest_query å·²å¤„ç†ï¼‰

    è¿”å›ï¼š(is_correct, trace_path)
    """
    case_id = case.get("id", "unknown")

    # refine ç±»å‹ï¼šè¯„ä¼°ä¿¡æ¯å·²åœ¨ test_query ä¸­å¤„ç†å¹¶ä¿å­˜åœ¨ trace æ–‡ä»¶ä¸­
    if test_type == "refine":
        print("=== è¯»å– refine ç±»å‹è¯„ä¼°ä¿¡æ¯ ===")
        score, score_summary, trace_path = _evaluate_refine_from_trace(
            case=case,
            model_name=model_name,
            use_tools=use_tools,
            dataset_path=dataset_path,
        )
        
        if score is not None:
            print(f"âœ… è¯„åˆ†: {score:.2f}")
            print(f"âœ… æ‘˜è¦: {score_summary}")
            # å°† score è½¬æ¢ä¸º is_correctï¼ˆ>0.8 è®¤ä¸ºæ­£ç¡®ï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´é˜ˆå€¼ï¼‰
            is_correct = score >= 0.8
            return is_correct, trace_path
        else:
            print("âš ï¸ æœªèƒ½ä» trace æ–‡ä»¶è¯»å–è¯„ä¼°ä¿¡æ¯")
            return None, trace_path

    # normal ç±»å‹ï¼šä½¿ç”¨åŸæœ‰çš„è¯„ä¼°é€»è¾‘
    print("=== å¼€å§‹è¯„ä¼°å¹¶ä¿å­˜ trace ===")

    # 1. æå– boxed answer
    boxed_answer = extract_boxed_answer(final_answer)

    # 2. è¯»å–æ ‡å‡†ç­”æ¡ˆ
    standard_answer = case.get("answer")
    if not standard_answer:
        metadata = case.get("metadata", {})
        if isinstance(metadata, dict):
            standard_answer = (
                metadata.get("golden_answer")
                or metadata.get("answer")
            )

    standard_answer_str: Optional[str] = None
    if standard_answer:
        if isinstance(standard_answer, list) and standard_answer:
            standard_answer_str = str(standard_answer[0])
        elif isinstance(standard_answer, dict):
            standard_answer_str = json.dumps(standard_answer, ensure_ascii=False)
        else:
            standard_answer_str = str(standard_answer)

    question_text = case.get("question", "")

    # 3. åˆ¤é¢˜ï¼ˆå¦‚æœæœ‰ boxed answer ä¸”æœ‰æ ‡å‡†ç­”æ¡ˆï¼‰
    is_correct: Optional[bool] = None
    if boxed_answer and standard_answer_str:
        try:
            print(f"ğŸ“‹ æ ‡å‡†ç­”æ¡ˆ: {standard_answer_str}")
            print(f"ğŸ“‹ æ¨¡å‹ç­”æ¡ˆ: {boxed_answer}")
            if question_text:
                preview = (
                    f"{question_text[:100]}..."
                    if len(question_text) > 100
                    else question_text
                )
                print(f"ğŸ“‹ é—®é¢˜: {preview}")

            is_correct = is_answer_correct(
                question_text, boxed_answer, standard_answer_str, case_id
            )

            if is_correct:
                print("åˆ¤é¢˜ç»“æœâœ… : æ­£ç¡®")
            else:
                print("åˆ¤é¢˜ç»“æœâŒ : é”™è¯¯")

        except Exception as e:
            print(f"åˆ¤é¢˜è¿‡ç¨‹å‡ºé”™âŒ : {e}")
            import traceback

            traceback.print_exc()
    else:
        if not boxed_answer:
            print("âš ï¸ æœªèƒ½ä»å›ç­”ä¸­æå– boxed answerï¼Œè·³è¿‡åˆ¤é¢˜")
        if not standard_answer_str:
            print("âš ï¸ æœªæ‰¾åˆ°æ ‡å‡†ç­”æ¡ˆï¼Œè·³è¿‡åˆ¤é¢˜")

    # 4. ä¿å­˜ trace æ–‡ä»¶
    trace_path: Optional[Path] = None
    try:
        # å…ƒæ•°æ®ä¸ dataset_key
        metadata = case.get("metadata", {})
        if not isinstance(metadata, dict):
            metadata = {}

        dataset_filename = dataset_path.name

        # æ¨å¯¼ mode_nameï¼ˆä¸ä¸»æ‰§è¡Œå™¨å¯¹é½ï¼šwith_tools_react / without_toolsï¼‰
        mode_name = "with_tools_react" if use_tools else "without_tools"

        trace_path = _derive_trace_path_for_multi(
            model_name=model_name,
            use_tools=use_tools,
            case_id=case_id,
            mode_name=mode_name,
            dataset_filename=dataset_filename,
        )

        # åºåˆ—åŒ– messagesï¼ˆsimple_test_query è¿”å›çš„ messages å·²ç»å¤§éƒ¨åˆ†æ˜¯ dictï¼‰
        serializable_messages: List[Dict[str, Any]] = []
        for m in result.get("messages", []):
            if isinstance(m, dict):
                mm = m.copy()
                if mm.get("role") == "tool" and isinstance(mm.get("content"), str):
                    try:
                        mm["content"] = json.loads(mm["content"])
                    except Exception:
                        pass
                serializable_messages.append(mm)
            else:
                serializable_messages.append(
                    {
                        "role": getattr(m, "role", None),
                        "content": getattr(m, "content", None),
                    }
                )

        metadata_summary = ensure_metadata_summary(case)

        trace_data: Dict[str, Any] = {
            "id": case_id,
            "query": question_text,
            "query_data": case,
            "model": model_name,
            "use_tools": use_tools,
            "mode": "tool_mode" if use_tools else "text_mode",
            "rounds": result.get("rounds", 1),
            "messages": serializable_messages,
            "model_raw_answer": final_answer,
        }

        if metadata:
            trace_data["metadata"] = metadata
        if metadata_summary:
            trace_data["metadata_summary"] = metadata_summary

        # æ·»åŠ  evaluation ç»“æœ
        trace_data["boxed_extraction_success"] = boxed_answer is not None
        if boxed_answer is not None:
            trace_data["model_boxed_answer"] = boxed_answer

        if is_correct is not None:
            trace_data["boxed_answer_evaluation"] = {
                "is_correct": is_correct,
                "standard_answer": standard_answer_str,
                "model_answer": boxed_answer,
                "evaluation_method": "gpt4.1_judge",
                "evaluation_success": True,
            }

        with trace_path.open("w", encoding="utf-8") as f:
            json.dump(trace_data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ Trace æ–‡ä»¶å·²ä¿å­˜: {trace_path}")

    except Exception as e:
        print(f"âš ï¸ ä¿å­˜ trace æ–‡ä»¶å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()

    print("====================\n")

    return is_correct, trace_path


def run_all_refined_cases(
    model_name: Optional[str] = None,
    use_tools: bool = True,
    test_type: str = "normal",
    force_retest: bool = False,
    load_all_topic_tools: bool = False,
    auto_infer_from_metadata: bool = True,
) -> None:
    """
    å¯¹ refine_merged_questions_augmented.json ä¸­çš„æ‰€æœ‰æ¡ˆä¾‹é€ä¸€æ‰§è¡Œæµ‹è¯• + evaluation + trace è½ç›˜ã€‚
    
    Args:
        model_name: æ¨¡å‹åç§°
        use_tools: æ˜¯å¦ä½¿ç”¨å·¥å…·
        test_type: æµ‹è¯•ç±»å‹
            - "normal": ä½¿ç”¨ simple_test_query + extract_boxed_answer + is_answer_correct
            - "refine": ä½¿ç”¨ test_query + calculate_answer_scoreï¼ˆç»“æ„åŒ–è¯„ä¼°ï¼‰
        force_retest: æ˜¯å¦å¼ºåˆ¶é‡æ–°æµ‹è¯•ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
        load_all_topic_tools: æ˜¯å¦åŠ è½½ç›¸åŒtopicçš„æ‰€æœ‰å·¥å…·ï¼ˆä»…å¯¹ refine ç±»å‹æœ‰æ•ˆï¼‰
        auto_infer_from_metadata: æ˜¯å¦æ ¹æ® metadata ä¸­çš„ subject/topic è‡ªåŠ¨æ¨æ–­å¹¶åŠ è½½å·¥å…·ç›®å½•
                                   é»˜è®¤ä¸º Trueï¼Œä¼šè‡ªåŠ¨åŠ è½½ toolkits/{subject}/{topic}/ ä¸‹çš„æ‰€æœ‰å·¥å…·
    """
    core_dir = Path(__file__).resolve().parent
    dataset_path = core_dir / "dataset" / "refine_merged_questions_augmented.json"

    if not dataset_path.exists():
        print(f"âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_path}")
        return

    # æ ¹æ® test_type é€‰æ‹©ä¸åŒçš„æ•°æ®åŠ è½½æ–¹å¼
    try:
        if test_type == "refine":
            # refine ç±»å‹ï¼šä½¿ç”¨ load_refined_test_cases_from_dataset åŠ è½½ç²¾ç‚¼ç‰ˆæ¡ˆä¾‹
            from gym.core.data_loader import load_refined_test_cases_from_dataset
            cases = load_refined_test_cases_from_dataset(dataset_path=str(dataset_path))
            print(f"âœ… ä½¿ç”¨ load_refined_test_cases_from_dataset åŠ è½½äº† {len(cases)} ä¸ªç²¾ç‚¼ç‰ˆæ¡ˆä¾‹")
        else:
            # normal ç±»å‹ï¼šç›´æ¥ä» JSON æ–‡ä»¶åŠ è½½åŸå§‹æ¡ˆä¾‹
            with dataset_path.open("r", encoding="utf-8") as f:
                cases = json.load(f)
            print(f"âœ… ç›´æ¥ä» JSON æ–‡ä»¶åŠ è½½äº† {len(cases)} ä¸ªæ¡ˆä¾‹")
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return

    if not isinstance(cases, list) or not cases:
        print("âŒ æ•°æ®é›†å†…å®¹ä¸ºç©ºæˆ–æ ¼å¼ä¸æ˜¯åˆ—è¡¨")
        return

    current_model = model_name or DEFAULT_MODEL
    total = len(cases)
    success = 0
    correct = 0
    evaluated = 0
    skipped = 0

    test_type_label = "refine (ç»“æ„åŒ–è¯„ä¼°)" if test_type == "refine" else "normal (boxedè¯„ä¼°)"
    print(f"\n=== å¼€å§‹æ‰¹é‡æµ‹è¯• refine_merged_questions_augmented.json ({total} ä¸ªæ¡ˆä¾‹) ===")
    print(f"æ¨¡å‹: {current_model} | æ¨¡å¼: {'with_tools_react' if use_tools else 'without_tools'} | ç±»å‹: {test_type_label}")

    for idx, case in enumerate(cases, start=1):
        case_id = case.get("id", f"case_{idx}") 
        ## TODO
        if idx!=5: 
            continue
        # ç»Ÿä¸€å¤„ç†å›¾ç‰‡è·¯å¾„
        normalize_case_image_paths(case)
        
        # refine ç±»å‹ï¼šæ˜¾ç¤ºåŸå§‹ ID å’Œç²¾ç‚¼ç´¢å¼•
        if test_type == "refine":
            original_id = case.get("original_id", "unknown")
            refined_index = case.get("refined_index", "unknown")
            print(f"\n--- [{idx}/{total}] æµ‹è¯•ç²¾ç‚¼ç‰ˆæ¡ˆä¾‹ ID: {case_id} (åŸå§‹: {original_id}, ç²¾ç‚¼ç´¢å¼•: {refined_index}) ---")
        else:
            print(f"\n--- [{idx}/{total}] æµ‹è¯•æ¡ˆä¾‹ ID: {case_id} ---") 

        try:
            # æ ¹æ® test_type é€‰æ‹©ä¸åŒçš„æµ‹è¯•å‡½æ•°
            if test_type == "refine":
                # refine ç±»å‹ï¼šä½¿ç”¨ test_queryï¼ˆå†…éƒ¨å·²å¤„ç†ç»“æ„åŒ–è¯„ä¼°ï¼‰
                # ç¡®ä¿ metadata ä¸­æœ‰ _dataset_filenameï¼ˆç”¨äºè·¯å¾„åˆ¤æ–­ï¼‰
                if isinstance(case.get("metadata"), dict):
                    case["metadata"]["_dataset_filename"] = dataset_path.name
                else:
                    case["metadata"] = {"_dataset_filename": dataset_path.name}
                
                final_answer = test_query(
                    case,
                    model_name=current_model,
                    use_tools=use_tools,
                    force_retest=force_retest,
                    load_all_topic_tools=load_all_topic_tools,
                    auto_infer_from_metadata=auto_infer_from_metadata,
                )
                
                # test_query è¿”å›çš„æ˜¯å­—ç¬¦ä¸²ï¼Œè¯„ä¼°ä¿¡æ¯å·²åœ¨ trace æ–‡ä»¶ä¸­
                # åˆ›å»ºä¸€ä¸ªç»“æœå­—å…¸ä»¥ä¾¿ç»Ÿä¸€å¤„ç†
                result = {"final_answer": final_answer}
                
            else:
                # normal ç±»å‹ï¼šä½¿ç”¨ simple_test_query
                result = simple_test_query(
                    case,
                    model_name=current_model,
                    use_tools=use_tools,
                    auto_infer_from_metadata=auto_infer_from_metadata,
                )
                final_answer = result.get("final_answer", "")
            
            success += 1

            # è¯„ä¼°å’Œä¿å­˜ trace
            is_correct, _ = _evaluate_and_save_trace(
                case=case,
                result=result,
                final_answer=final_answer,
                model_name=current_model,
                use_tools=use_tools,
                dataset_path=dataset_path,
                test_type=test_type,
            )

            if is_correct is not None:
                evaluated += 1
                if is_correct:
                    correct += 1

        except TestSkipException as skip_exc:
            print(f"â­ï¸ æ¡ˆä¾‹ {case_id} è¢«è·³è¿‡ï¼ŒåŸå› ï¼š{skip_exc.reason}")
            if skip_exc.details:
                print(f"   è¯¦æƒ…: {json.dumps(skip_exc.details, ensure_ascii=False)}")
            skipped += 1
            continue
        except Exception as e:
            print(f"âŒ æ¡ˆä¾‹ {case_id} æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback

            traceback.print_exc()
            continue 
     

    print("\n=== æ‰¹é‡æµ‹è¯•å®Œæˆ ===")
    print(f"æ€»æ¡ˆä¾‹æ•°      : {total}")
    print(f"æˆåŠŸæ‰§è¡Œ      : {success}")
    print(f"è·³è¿‡æ¡ˆä¾‹      : {skipped}")
    print(f"å·²å‚ä¸åˆ¤é¢˜æ¡ˆä¾‹: {evaluated}")
    print(f"åˆ¤å®šä¸ºæ­£ç¡®    : {correct}")
    if evaluated:
        acc = correct / evaluated * 100.0
        print(f"åˆ¤é¢˜å‡†ç¡®ç‡    : {acc:.2f}%")
    else:
        print("åˆ¤é¢˜å‡†ç¡®ç‡    : æ— æœ‰æ•ˆè¯„æµ‹ç»“æœ")


if __name__ == "__main__":
    # é»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„ DEFAULT_MODELï¼Œå¼€å¯å·¥å…·æ¨¡å¼
    # run_all_refined_cases(model_name="glm-4.6v", use_tools=True) 
    
    #run_all_refined_cases(model_name="Qwen/Qwen3-VL-235B-A22B-Thinking", use_tools=True) 
    # run_all_refined_cases(model_name="gpt-5", use_tools=True)  
    # run_all_refined_cases(model_name="gpt-4o", use_tools=True) 
    # run_all_refined_cases(model_name="qwen3-vl-8b-thinking", use_tools=True)
    run_all_refined_cases(model_name="claude-sonnet-4-20250514", use_tools=True)
    # run_all_refined_cases(model_name="gemini-2.5-pro-thinking-2048", use_tools=True) 
    # run_all_refined_cases(model_name="glm-4.6v", use_tools=True, test_type="refine")






