"""
è¯„ä¼°æœåŠ¡æ¨¡å— - ç‹¬ç«‹çš„æ¨¡å‹å›ç­”è¯„ä¼°åŠŸèƒ½

è¿™ä¸ªæ¨¡å—ä¸“é—¨è´Ÿè´£å¯¹å·²ä¿å­˜çš„æ¨¡å‹å›ç­”è¿›è¡Œè¯„ä¼°ï¼Œä¸æµ‹è¯•æ‰§è¡Œå®Œå…¨åˆ†ç¦»ã€‚
å®ç°é«˜å†…èšä½è€¦åˆçš„è®¾è®¡åŸåˆ™ã€‚
"""
import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any

# Add project root to path so we can import modules
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from gym.core.data_loader import extract_structured_answer_from_response, _post_process_extracted_answer
from gym.core.evaluator import calculate_answer_score, compute_segment_scores_from_details
from gym.config.dataset_config import get_trace_root, list_available_datasets

TRACE_ROOT_NAMES = {entry.trace_root.name for entry in list_available_datasets()}


class EvaluationService:
    """è¯„ä¼°æœåŠ¡ç±» - è´Ÿè´£å¯¹æ¨¡å‹å›ç­”è¿›è¡Œç»“æ„åŒ–æå–å’Œè¯„åˆ†"""

    def __init__(self, lenient_mode: bool = True):
        """
        åˆå§‹åŒ–è¯„ä¼°æœåŠ¡

        Args:
            lenient_mode: æ˜¯å¦ä½¿ç”¨å®½æ¾æ¨¡å¼ï¼ˆå¯¹æ•°å€¼æ¯”è¾ƒæ›´å®½æ¾ï¼‰
        """
        self.lenient_mode = lenient_mode
        self.tolerance = 0.05 if lenient_mode else 0.01

    def extract_answer_from_trace(self, trace_data: Dict) -> Optional[Dict]:
        """
        ä»è½¨è¿¹æ•°æ®ä¸­æå–ç»“æ„åŒ–ç­”æ¡ˆ

        Args:
            trace_data: å®Œæ•´çš„è½¨è¿¹æ•°æ®å­—å…¸

        Returns:
            æå–çš„ç»“æ„åŒ–ç­”æ¡ˆï¼Œå¦‚æœæå–å¤±è´¥è¿”å›None
        """
        model_answer = None

        # 1. å°è¯•é‡æ–°æå–ç­”æ¡ˆï¼ˆå¦‚æœæœ‰åŸå§‹å›ç­”ï¼‰
        raw_answer = trace_data.get('model_raw_answer')
        if not raw_answer or not isinstance(raw_answer, str) or not raw_answer.strip():
            # å¦‚æœæ²¡æœ‰åŸå§‹å›ç­”ï¼Œå°è¯•ä»messagesä¸­æå–æœ€åä¸€æ¡assistantæ¶ˆæ¯
            raw_answer = self._extract_last_assistant_text(trace_data)

        if raw_answer and isinstance(raw_answer, str) and raw_answer.strip():
            # é‡æ–°æå–ç»“æ„åŒ–ç­”æ¡ˆ
            extracted = extract_structured_answer_from_response(raw_answer)
            if extracted is not None:
                model_answer = extracted

        # 2. å¦‚æœé‡æ–°æå–å¤±è´¥ï¼Œä½¿ç”¨å·²æœ‰çš„ç»“æ„åŒ–ç­”æ¡ˆ
        if model_answer is None:
            model_answer = trace_data.get('model_structured_answer')

        # 3. å¯¹ç­”æ¡ˆè¿›è¡Œåå¤„ç†ï¼Œä¿®å¤å¸¸è§é—®é¢˜
        if model_answer is not None:
            processed_answer = _post_process_extracted_answer(model_answer)
            model_answer = processed_answer

        return model_answer

    def evaluate_single_trace(self, trace_file_path: str, force_reextract: bool = False) -> Dict:
        """
        è¯„ä¼°å•ä¸ªè½¨è¿¹æ–‡ä»¶

        Args:
            trace_file_path: è½¨è¿¹æ–‡ä»¶è·¯å¾„
            force_reextract: æ˜¯å¦å¼ºåˆ¶é‡æ–°æå–ç­”æ¡ˆ

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸ï¼ŒåŒ…å«åˆ†æ•°ã€æ‘˜è¦å’Œè¯¦ç»†ä¿¡æ¯
        """
        try:
            with open(trace_file_path, 'r', encoding='utf-8') as f:
                trace_data = json.load(f)
        except Exception as e:
            return {
                'success': False,
                'error': f'è¯»å–è½¨è¿¹æ–‡ä»¶å¤±è´¥: {e}',
                'trace_file': trace_file_path
            }

        # æå–æ¨¡å‹ç­”æ¡ˆ
        if force_reextract or 'model_structured_answer' not in trace_data:
            model_answer = self.extract_answer_from_trace(trace_data)
            # æ›´æ–°è½¨è¿¹æ–‡ä»¶ä¸­çš„ç»“æ„åŒ–ç­”æ¡ˆ
            if model_answer is not None:
                trace_data['model_structured_answer'] = model_answer
        else:
            model_answer = trace_data.get('model_structured_answer')
            # å¯¹ç°æœ‰ç­”æ¡ˆä¹Ÿåº”ç”¨åå¤„ç†
            if model_answer is not None:
                processed_answer = _post_process_extracted_answer(model_answer)
                if processed_answer != model_answer:
                    trace_data['model_structured_answer'] = processed_answer
                    model_answer = processed_answer

        # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡å‡†ç­”æ¡ˆ
        golden_standard = trace_data.get('golden_standard')
        if not golden_standard:
            return {
                'success': False,
                'error': 'ç¼ºå°‘æ ‡å‡†ç­”æ¡ˆ(golden_standard)',
                'trace_file': trace_file_path
            }

        # è¿›è¡Œè¯„åˆ†
        if model_answer is not None:
            score, summary, details = self._calculate_answer_score(model_answer, golden_standard)

            # æ›´æ–°è½¨è¿¹æ–‡ä»¶
            trace_data['evaluation_score'] = score
            trace_data['evaluation_summary'] = summary
            trace_data['evaluation_details'] = details
            segment_scores = compute_segment_scores_from_details(golden_standard, details)
            if segment_scores:
                trace_data['evaluation_score_segments'] = segment_scores
            if 'evaluation_failure_reason' in trace_data:
                del trace_data['evaluation_failure_reason']  # ç§»é™¤å¤±è´¥æ ‡è®°

            # ä¿å­˜æ›´æ–°åçš„è½¨è¿¹æ–‡ä»¶
            try:
                with open(trace_file_path, 'w', encoding='utf-8') as f:
                    json.dump(trace_data, f, ensure_ascii=False, indent=2)
            except Exception as e:
                print(f"âš ï¸ ä¿å­˜è½¨è¿¹æ–‡ä»¶å¤±è´¥ {trace_file_path}: {e}")

            return {
                'success': True,
                'score': score,
                'summary': summary,
                'details': details,
                'trace_file': trace_file_path
            }
        else:
            # æå–å¤±è´¥
            score = 0.0
            summary = "ç¼ºå°‘æˆ–æ— æ³•æå–ç»“æ„åŒ–ç­”æ¡ˆ"
            details = {"error": "model_structured_answer is missing, null, or could not be extracted from raw answer"}

            # æ›´æ–°è½¨è¿¹æ–‡ä»¶
            trace_data['evaluation_score'] = score
            trace_data['evaluation_summary'] = summary
            trace_data['evaluation_details'] = details
            segment_scores = compute_segment_scores_from_details(golden_standard, details)
            if segment_scores:
                trace_data['evaluation_score_segments'] = segment_scores
            trace_data['evaluation_failure_reason'] = 'extraction_failed_or_missing'

            # ä¿å­˜æ›´æ–°åçš„è½¨è¿¹æ–‡ä»¶
            try:
                with open(trace_file_path, 'w', encoding='utf-8') as f:
                    json.dump(trace_data, f, ensure_ascii=False, indent=2)
            except Exception as e:
                print(f"âš ï¸ ä¿å­˜è½¨è¿¹æ–‡ä»¶å¤±è´¥ {trace_file_path}: {e}")

            return {
                'success': True,
                'score': score,
                'summary': summary,
                'details': details,
                'trace_file': trace_file_path
            }

    def evaluate_trace_directory(
        self,
        trace_dir: str,
        model_name: Optional[str] = None,
        force_reextract: bool = False,
        match_type_filter: Optional[str] = None,
        include_missing_only: bool = False,
    ) -> Dict:
        """
        è¯„ä¼°è½¨è¿¹ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶

        Args:
            trace_dir: è½¨è¿¹ç›®å½•è·¯å¾„
            model_name: æŒ‡å®šæ¨¡å‹åç§°ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰æ¨¡å‹
            force_reextract: æ˜¯å¦å¼ºåˆ¶é‡æ–°æå–ç­”æ¡ˆ
            match_type_filter: ä»…é‡æ–°è¯„æµ‹ evaluation_details ä¸­åŒ…å«æŒ‡å®š match_type çš„è½¨è¿¹æ–‡ä»¶
            include_missing_only: æ˜¯å¦åªé‡æ–°è¯„æµ‹ evaluation_score ç¼ºå¤±æˆ–è¯„ä¼°å¤±è´¥çš„è½¨è¿¹

        Returns:
            è¯„ä¼°ç»Ÿè®¡ç»“æœ
        """
        if not os.path.exists(trace_dir):
            return {
                'success': False,
                'error': f'è½¨è¿¹ç›®å½•ä¸å­˜åœ¨: {trace_dir}'
            }

        print(f"ğŸ“Š å¼€å§‹è¯„ä¼°è½¨è¿¹æ–‡ä»¶: {trace_dir}")

        # æ”¶é›†è½¨è¿¹æ–‡ä»¶
        trace_files = self._collect_trace_files(trace_dir, model_name)

        # å¯é€‰ï¼šä»…ä¿ç•™åŒ…å«æŒ‡å®š match_type çš„è½¨è¿¹
        if match_type_filter:
            original_count = len(trace_files)
            trace_files = self._filter_trace_files_by_match_type(trace_files, match_type_filter)
            print(f"ğŸ¯ è¿‡æ»¤ match_type={match_type_filter} çš„è½¨è¿¹: {len(trace_files)}/{original_count} ä¸ª")

        if not trace_files:
            return {
                'success': False,
                'error': 'æœªæ‰¾åˆ°ä»»ä½•è½¨è¿¹æ–‡ä»¶' if not match_type_filter else f'æœªæ‰¾åˆ°åŒ…å« match_type={match_type_filter} çš„è½¨è¿¹æ–‡ä»¶'
            }

        print(f"æ‰¾åˆ° {len(trace_files)} ä¸ªè½¨è¿¹æ–‡ä»¶")

        # è¯„ä¼°ç»Ÿè®¡
        total_score = 0
        evaluated_count = 0
        model_stats = {}
        evaluation_results = []

        def is_missing_or_failed(trace_path: str) -> bool:
            """æ£€æŸ¥è½¨è¿¹æ˜¯å¦ç¼ºå°‘è¯„åˆ†æˆ–æ ‡è®°ä¸ºå¤±è´¥ã€‚"""
            try:
                with open(trace_path, 'r', encoding='utf-8') as f_in:
                    data = json.load(f_in)
            except Exception:
                return True

            if 'evaluation_score' not in data:
                return True

            failure_reason = data.get('evaluation_failure_reason')
            if failure_reason:
                return True

            return False

        filtered_trace_files = []
        for trace_file in trace_files:
            if include_missing_only and not is_missing_or_failed(trace_file):
                continue
            filtered_trace_files.append(trace_file)

        if not filtered_trace_files:
            return {
                'success': False,
                'error': 'æœªæ‰¾åˆ°éœ€è¦é‡æ–°è¯„ä¼°çš„è½¨è¿¹æ–‡ä»¶'
            }

        trace_files_to_process = filtered_trace_files

        for trace_file in trace_files_to_process:
            # ä»æ–‡ä»¶è·¯å¾„ä¸­æå–æ¨¡å‹å’Œæ¡ˆä¾‹ä¿¡æ¯
            model, case_id, mode = self._extract_trace_info(trace_file)
            display_id = f"{model} - æ¡ˆä¾‹{case_id}"
            if mode != 'unknown':
                display_id += f" ({mode})"

            # è¯„ä¼°å•ä¸ªè½¨è¿¹
            result = self.evaluate_single_trace(trace_file, force_reextract)

            if result['success']:
                score = result['score']
                summary = result['summary']

                previously_scored = not is_missing_or_failed(trace_file)
                if force_reextract or not previously_scored:
                    print(f"ğŸ”„ {display_id}: {score:.2%} (å·²é‡æ–°è¯„ä¼°)")
                else:
                    print(f"âœ… {display_id}: {score:.2%} (å·²è¯„åˆ†)")

                # ç»Ÿè®¡
                stats_key = f"{model}"
                if mode != 'unknown':
                    stats_key += f" ({mode})"

                if stats_key not in model_stats:
                    model_stats[stats_key] = {'scores': [], 'count': 0}
                model_stats[stats_key]['scores'].append(score)
                model_stats[stats_key]['count'] += 1

                total_score += score
                evaluated_count += 1

                evaluation_results.append({
                    'model': model,
                    'case_id': case_id,
                    'mode': mode,
                    'score': score,
                    'summary': summary
                })
            else:
                print(f"âŒ å¤„ç†è½¨è¿¹æ–‡ä»¶å¤±è´¥ {trace_file}: {result.get('error', 'Unknown error')}")

        # è¾“å‡ºç»Ÿè®¡ç»“æœ
        if evaluated_count > 0:
            overall_avg = total_score / evaluated_count
            print(f"\nğŸ“ˆ è¯„ä¼°ç»Ÿè®¡:")
            print(f"æ€»ä½“å¹³å‡åˆ†: {overall_avg:.2%} ({evaluated_count}ä¸ªæ¡ˆä¾‹)")

            # æŒ‰æ¨¡å‹åˆ†ç»„æ˜¾ç¤ºç»Ÿè®¡
            for stats_key, stats in sorted(model_stats.items()):
                if stats['count'] > 0:
                    model_avg = sum(stats['scores']) / stats['count']
                    print(f"  {stats_key}: {model_avg:.2%} ({stats['count']}ä¸ªæ¡ˆä¾‹)")
        else:
            print("æ²¡æœ‰å¯è¯„ä¼°çš„æ¡ˆä¾‹")

        return {
            'success': True,
            'total_evaluated': evaluated_count,
            'overall_average': total_score / evaluated_count if evaluated_count > 0 else 0,
            'model_stats': model_stats,
            'evaluation_results': evaluation_results
        }

    def _extract_last_assistant_text(self, trace_data: Dict) -> Optional[str]:
        """ä»traceçš„messagesä¸­æå–æœ€åä¸€æ¡assistantæ–‡æœ¬å†…å®¹"""
        try:
            # ä¼˜å…ˆä½¿ç”¨å•ç‹¬ä¿å­˜çš„æœ€åä¸€æ¡assistantæ¶ˆæ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            last_assist = trace_data.get('model_last_assistant_message')
            if isinstance(last_assist, dict):
                content = last_assist.get('content')
                if isinstance(content, str) and content.strip():
                    return content.strip()
                if isinstance(content, list):
                    parts = []
                    for part in content:
                        try:
                            if isinstance(part, dict) and part.get('type') == 'text':
                                t = part.get('text', '')
                                if isinstance(t, str) and t.strip():
                                    parts.append(t.strip())
                        except Exception:
                            continue
                    if parts:
                        return "\n\n".join(parts)

            messages = trace_data.get('messages', []) or []
            # åå‘éå†ï¼Œæ‰¾åˆ°æœ€åä¸€æ¡assistantæ¶ˆæ¯
            for msg in reversed(messages):
                if not isinstance(msg, dict):
                    continue
                if msg.get('role') != 'assistant':
                    continue
                content = msg.get('content')
                # ç›´æ¥å­—ç¬¦ä¸²
                if isinstance(content, str):
                    txt = content.strip()
                    if txt:
                        return txt
                # å¤šæ¨¡æ€åˆ—è¡¨
                if isinstance(content, list):
                    parts = []
                    for part in content:
                        try:
                            if isinstance(part, dict) and part.get('type') == 'text':
                                t = part.get('text', '')
                                if isinstance(t, str) and t.strip():
                                    parts.append(t.strip())
                        except Exception:
                            continue
                    if parts:
                        return "\n\n".join(parts)
        except Exception:
            pass
        return None

    def _collect_trace_files(self, directory: str, model_name: Optional[str] = None) -> List[str]:
        """é€’å½’æ”¶é›†è½¨è¿¹æ–‡ä»¶ï¼Œå…¼å®¹ with_tools_react / with_all_tools ç­‰åç¼€"""
        files: List[str] = []
        allowed_prefixes = ("with_tools", "without_tools", "with_all_tools")

        try:
            for item in os.listdir(directory):
                item_path = os.path.join(directory, item)

                if os.path.isfile(item_path) and item.endswith('_trace.json'):
                    files.append(item_path)
                elif os.path.isdir(item_path):
                    # å…¼å®¹ with_tools_react / without_tools_react / with_all_tools_react ç­‰ç›®å½•å
                    if item.startswith(allowed_prefixes):
                        files.extend(self._collect_trace_files(item_path, model_name))
                    else:
                        # å§‹ç»ˆå‘ä¸‹é€’å½’ï¼Œè¿™æ ·æ¨¡å‹ç›®å½•ä¸‹çš„å…¶å®ƒå±‚çº§ä¹Ÿèƒ½è¢«æ‰«æåˆ°
                        files.extend(self._collect_trace_files(item_path, model_name if model_name else item))
        except PermissionError:
            print(f"âš ï¸ æ— æ³•è®¿é—®ç›®å½•: {directory}")
        except Exception as e:
            print(f"âš ï¸ è¯»å–ç›®å½•æ—¶å‡ºé”™ {directory}: {e}")

        return files

    def _extract_trace_info(self, trace_file: str) -> Tuple[str, str, str]:
        """ä»è½¨è¿¹æ–‡ä»¶è·¯å¾„ä¸­æå–æ¨¡å‹åç§°ã€æ¡ˆä¾‹IDå’Œæ¨¡å¼"""
        try:
            with open(trace_file, 'r', encoding='utf-8') as f:
                trace_data = json.load(f)

            model = trace_data.get('model', 'unknown')
            case_id = trace_data.get('id', 'unknown')
            mode = trace_data.get('mode', 'unknown')

            if model == 'unknown':
                # å°è¯•ä»æ–‡ä»¶è·¯å¾„æ¨æ–­æ¨¡å‹åç§°
                path_parts = trace_file.replace(os.sep, '/').split('/')
                for i, part in enumerate(path_parts):
                    if part in TRACE_ROOT_NAMES and i + 1 < len(path_parts):
                        model = path_parts[i + 1]
                        break

            return model, case_id, mode
        except Exception:
            return 'unknown', 'unknown', 'unknown'

    def _calculate_answer_score(self, model_answer: Dict, golden_standard: Dict) -> Tuple[float, str, Dict]:
        """è®¡ç®—æ¨¡å‹ç­”æ¡ˆä¸æ ‡å‡†ç­”æ¡ˆçš„åŒ¹é…åˆ†æ•°"""

        return calculate_answer_score(model_answer, golden_standard, self.tolerance)

    def _filter_trace_files_by_match_type(self, trace_files: List[str], match_type: str) -> List[str]:
        """è¿‡æ»¤å‡º evaluation_detailsï¼ˆåŠå…¶ä»–åµŒå¥—ç»“æ„ï¼‰ä¸­åŒ…å«æŒ‡å®š match_type çš„è½¨è¿¹æ–‡ä»¶"""

        def contains_match_type(obj: Any) -> bool:
            if isinstance(obj, dict):
                if obj.get('match_type') == match_type:
                    return True
                for value in obj.values():
                    if contains_match_type(value):
                        return True
            elif isinstance(obj, (list, tuple)):
                for item in obj:
                    if contains_match_type(item):
                        return True
            return False

        filtered_files: List[str] = []
        for file_path in trace_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    trace_data = json.load(f)
            except Exception as exc:
                print(f"âš ï¸ æ— æ³•è¯»å– {file_path} è¿›è¡Œè¿‡æ»¤: {exc}")
                continue

            if contains_match_type(trace_data.get('evaluation_details')) or contains_match_type(trace_data.get('attempts')):
                filtered_files.append(file_path)
        return filtered_files


# ä¾¿æ·å‡½æ•°
def evaluate_traces(trace_dir: str = None, model_name: str = None,
                   force_reextract: bool = False, lenient_mode: bool = True,
                   match_type_filter: Optional[str] = None,
                   include_missing_only: bool = False) -> Dict:
    """
    ä¾¿æ·å‡½æ•°ï¼šè¯„ä¼°è½¨è¿¹æ–‡ä»¶

    Args:
        trace_dir: è½¨è¿¹ç›®å½•ï¼Œé»˜è®¤ä¸ºå½“å‰æ•°æ®é›†çš„ trace ç›®å½•
        model_name: æŒ‡å®šæ¨¡å‹åç§°ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰æ¨¡å‹
        force_reextract: æ˜¯å¦å¼ºåˆ¶é‡æ–°æå–ç­”æ¡ˆ
        lenient_mode: æ˜¯å¦ä½¿ç”¨å®½æ¾æ¨¡å¼
        match_type_filter: ä»…é‡æ–°è¯„æµ‹åŒ…å«æŒ‡å®š match_type çš„è½¨è¿¹
        include_missing_only: æ˜¯å¦ä»…è¯„æµ‹ evaluation_score ç¼ºå¤±æˆ–å­˜åœ¨è¯„ä¼°å¤±è´¥åŸå› çš„è½¨è¿¹

    Returns:
        è¯„ä¼°ç»Ÿè®¡ç»“æœ
    """
    if trace_dir is None:
        trace_root = get_trace_root()
        if model_name:
            trace_dir = str(trace_root / model_name)
        else:
            trace_dir = str(trace_root)

    service = EvaluationService(lenient_mode=lenient_mode)
    return service.evaluate_trace_directory(
        trace_dir,
        model_name,
        force_reextract,
        match_type_filter=match_type_filter,
        include_missing_only=include_missing_only,
    )


def evaluate_single_trace_file(trace_file: str, force_reextract: bool = False,
                              lenient_mode: bool = True) -> Dict:
    """
    ä¾¿æ·å‡½æ•°ï¼šè¯„ä¼°å•ä¸ªè½¨è¿¹æ–‡ä»¶

    Args:
        trace_file: è½¨è¿¹æ–‡ä»¶è·¯å¾„
        force_reextract: æ˜¯å¦å¼ºåˆ¶é‡æ–°æå–ç­”æ¡ˆ
        lenient_mode: æ˜¯å¦ä½¿ç”¨å®½æ¾æ¨¡å¼

    Returns:
        è¯„ä¼°ç»“æœ
    """
    service = EvaluationService(lenient_mode=lenient_mode)
    return service.evaluate_single_trace(trace_file, force_reextract)


if __name__ == "__main__":
    """ä½¿ç”¨ç¤ºä¾‹"""
    import sys

    if len(sys.argv) > 1:
        model_name = sys.argv[1]
        print(f"ğŸ“Š è¯„ä¼°æ¨¡å‹: {model_name}")
        result = evaluate_traces(model_name=model_name, force_reextract=True, lenient_mode=True)
    else:
        print("ğŸ“Š è¯„ä¼°æ‰€æœ‰æ¨¡å‹")
        result = evaluate_traces(force_reextract=True, lenient_mode=True)

    if result['success']:
        print(f"\nâœ… è¯„ä¼°å®Œæˆï¼Œæ€»è®¡è¯„ä¼° {result['total_evaluated']} ä¸ªæ¡ˆä¾‹")
    else:
        print(f"\nâŒ è¯„ä¼°å¤±è´¥: {result.get('error', 'Unknown error')}")
