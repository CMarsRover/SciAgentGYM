"""
ç²¾ç®€ç‰ˆæµ‹è¯•æ‰§è¡Œæ¨¡å—ï¼ˆå®éªŒæ€§ï¼‰

ç›®æ ‡ï¼š
- åœ¨ä¸å½±å“ç°æœ‰ `gym/test_executor.py` è¡Œä¸ºçš„å‰æä¸‹ï¼Œæä¾›ä¸€å¥—æ›´æ¸…æ™°ã€èŒè´£å•ä¸€çš„æ‰§è¡Œæµç¨‹ã€‚
- ä¼˜å…ˆæŠŠã€Œç¯å¢ƒ & å·¥å…·åŠ è½½ã€è¿™éƒ¨åˆ†é€»è¾‘ç®€åŒ–å‡ºæ¥ï¼Œä¾¿äºåç»­è¿­ä»£ã€‚

è®¾è®¡è¦ç‚¹ï¼š
- å¤ç”¨æ—§æ¨¡å—ä¸­å·²ç»éªŒè¯è¿‡çš„å·¥å…·ä¸ç¯å¢ƒæ„å»ºé€»è¾‘ï¼š`_build_env_and_tools_from_loaded`
- å°è£…ä¸€ä¸ªæœ€å°å¯ç”¨çš„å•é¢˜è°ƒç”¨å…¥å£ï¼š`simple_test_query`
  - è¾“å…¥ï¼šå•ä¸ª `query_data`ï¼ˆåŒ…å« question / metadata / usage_tool_protocol ç­‰ï¼‰
  - è¾“å‡ºï¼šæ¨¡å‹æœ€ç»ˆè‡ªç„¶è¯­è¨€å›ç­”ï¼ˆçº¯æ–‡æœ¬ï¼‰ï¼Œä»¥åŠå¯é€‰çš„å¯¹è¯è½¨è¿¹ï¼ˆè¿”å›å€¼ä¸­ï¼‰

æ³¨æ„ï¼š
- æœ¬æ–‡ä»¶å½“å‰ä¸å‚ä¸åŸæœ‰æ‰¹é‡æµ‹è¯• / è¯„åˆ† / pass@k ç­‰æµç¨‹ï¼Œåªç”¨äºæ¢ç´¢å’ŒéªŒè¯æ–°çš„ç»“æ„ã€‚
- åç»­å¦‚æœæ•ˆæœç¨³å®šï¼Œå¯ä»¥é€æ­¥ä»è¿™é‡ŒæŠ½å–å…¬å…±ç»„ä»¶ï¼Œåå‘ç®€åŒ–è€çš„ `test_executor.py`ã€‚
"""

from __future__ import annotations

import json
import base64
import time
from copy import deepcopy
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

# ç›´æ¥å¤ç”¨ç°æœ‰ agent / data_loader / tool_loader ç­‰æ¨¡å—
from gym.agent import (
    get_client,
    DEFAULT_MODEL,
    REACT_TOOL_SYSTEM_PROMPT,
    TOOL_TRACE_SUFFIX,
    _should_use_react_prompt,
    _coerce_truthy_flag,
)
from gym.core.tool_loader import load_tools_for_case, register_tools_to_env, run_tool_call, prepare_env_from_query
from gym.config.config import SUPPORTED_MODELS
from gym.config.dataset_config import get_trace_root
from gym.env import MinimalSciEnv
from gym.core.data_loader import (
    process_question_with_images_from_metadata,
    extract_golden_answer_template,
    extract_augmented_answer_template,
    extract_structured_answer_from_response,
    ensure_metadata_summary,
    group_cases_by_topic,
    aggregate_usage_tool_protocol_for_cases,
    load_test_cases_from_dataset,
    load_augmented_test_cases_from_dataset,
    load_refined_test_cases_from_dataset,
)
from gym.core.exceptions import TestSkipException

CORE_DIR = Path(__file__).resolve().parent
_ROOT = Path(__file__).resolve().parents[1]


def _get_tool_mode_answer_prompt() -> str:
    """
    å·¥å…·æ¨¡å¼ä¸‹ä½¿ç”¨çš„ç®€åŒ–ç­”æ¡ˆæ ¼å¼æ¨¡æ¿ï¼ˆä»…ä¿ç•™ Answer éƒ¨åˆ†ï¼‰ã€‚
    ç›®å‰ç»Ÿä¸€ä¸ºï¼šè¦æ±‚è¿”å›åŸè¯­è¨€ã€æœ€ç»ˆç­”æ¡ˆç”¨ LaTeX \\boxed{} åŒ…è£¹ã€‚
    """
    return (
        "You should strictly respond in this exact format and answer the question in its original language:\n"
        "example:###Answer###\n"
        "$\\boxed{}$"
    )


def _get_text_mode_reasoning_answer_prompt() -> str:
    """
    çº¯æ–‡æœ¬æ¨¡å¼ä¸‹ä½¿ç”¨çš„ç­”æ¡ˆæ ¼å¼æ¨¡æ¿ï¼ˆåŒ…å« Reasoning Process + Answerï¼‰ã€‚
    """
    return (
        "You should strictly respond in this exact format and answer the question in its original language:\n"
        "###Reasoning Process###\n"
        "{Your step by step reasoning process here}\n"
        "\n"
        "###Answer###\n"
        "{The final answer wrapped in LaTeX boxed format $\\boxed{}$}"
    )


def _get_react_tool_system_prompt() -> str:
    """
    ReAct å·¥å…·æ¨¡å¼ä¸‹ä½¿ç”¨çš„ system promptã€‚
    ç›´æ¥å¤ç”¨ agent.REACT_TOOL_SYSTEM_PROMPTã€‚
    """
    return REACT_TOOL_SYSTEM_PROMPT


def _build_env_and_tools_for_case(
    query_data: Dict[str, Any],
    auto_infer_from_metadata: bool = True,
) -> Tuple[Any, List[Dict[str, Any]], Dict[str, Any]]:
    """
    åŸºäºå•ä¸ªæ¡ˆä¾‹ï¼Œæ„å»ºæœ€å°ç¯å¢ƒ + tools schema + tool_registryã€‚

    ç®€åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨ prepare_env_from_query() å•ä¸€å…¥å£ï¼Œ
    é€šè¿‡ metadata ä¸­çš„ subject/topic è‡ªåŠ¨æ¨æ–­å¹¶åŠ è½½ toolkits/{subject}/{topic}/ ä¸‹çš„æ‰€æœ‰å·¥å…·ã€‚
    
    Args:
        query_data: æµ‹è¯•æ¡ˆä¾‹æ•°æ®
        auto_infer_from_metadata: æ˜¯å¦æ ¹æ® metadata ä¸­çš„ subject/topic è‡ªåŠ¨æ¨æ–­å¹¶åŠ è½½å·¥å…·ç›®å½•
                                   é»˜è®¤ä¸º Trueï¼ˆæ¨èï¼‰ï¼Œä½¿ç”¨è·¯å¾„æ¨æ–­æ‰¹é‡åŠ è½½
    """
    # åˆå§‹åŒ–å˜é‡ï¼ˆç”¨äºè°ƒè¯•è¾“å‡ºå’Œå…¼å®¹æ€§ï¼‰
    tool_protocols = None
    function_map = None
    tool_instances = None
    
    if auto_infer_from_metadata:
        # ä½¿ç”¨ç®€åŒ–å…¥å£ï¼šé€šè¿‡è·¯å¾„æ¨æ–­æ‰¹é‡åŠ è½½åŒä¸€å­ç±»ä¸‹çš„æ‰€æœ‰å·¥å…·
        env, tool_instances, tools_schema, tool_registry = prepare_env_from_query(query_data)
    else:
        # å…¼å®¹æ¨¡å¼ï¼šä½¿ç”¨æ—§çš„åŠ è½½æ–¹å¼
        tool_protocols, function_map = load_tools_for_case(query_data)
        
        # æå– case_id å’Œ domain
        metadata = query_data.get("metadata") or {}
        if not isinstance(metadata, dict):
            metadata = {}
        
        case_id = (
            query_data.get("id")
            or metadata.get("id")
            or metadata.get("case_id")
            or metadata.get("question_id")
            or metadata.get("original_question_id")
        )
        if case_id:
            case_id = str(case_id)
        
        subject = metadata.get("subject", "").lower()
        domain = None
        if subject:
            domain_mapping = {
                "structural biology": "structural_biology",
                "molecular biology": "molecular_biology",
                "quantum physics": "quantum_physics",
                "life science": "life_science",
                "earth science": "earth_science",
                "computer science": "computer_science",
            }
            domain = domain_mapping.get(subject) or subject.replace(" ", "_")
        
        env, tool_instances, tools_schema, tool_registry = register_tools_to_env(
            tool_protocols,
            function_map,
            case_id=case_id,
            domain=domain,
            query_data=query_data,
            auto_infer_from_metadata=False,
        )

    # å…¼å®¹ã€Œå½“å‰æ¡ˆä¾‹æ²¡æœ‰ä»»ä½•å·¥å…·ã€æˆ–æ—§å®ç°è¿”å› None çš„æƒ…å†µ
    if env is None:
        env = MinimalSciEnv(tool_names=None)
    if tools_schema is None:
        tools_schema = []
    if tool_registry is None:
        tool_registry = {}

    # è°ƒè¯•è¾“å‡ºï¼šå½“å‰æ¡ˆä¾‹å®é™…å¯ç”¨çš„å·¥å…·æƒ…å†µ
    tool_count = len(tool_instances) if tool_instances else len(tool_protocols or [])
    func_count = len(function_map or {}) if function_map else tool_count
    print(
        f"[simple_test_query] å·²åŠ è½½å·¥å…·æ•°: {tool_count}ï¼Œ"
        f"ç¯å¢ƒä¸­å·¥å…·æ•°: {len(tool_registry or {})}"
    )
    if tool_registry:
        names_preview = ", ".join(list(tool_registry.keys())[:10])
        print(f"[simple_test_query] å¯ç”¨å·¥å…·åç§°(å‰10): {names_preview}")
    else:
        print("[simple_test_query] å½“å‰æ¡ˆä¾‹æœªæ³¨å†Œä»»ä½•å·¥å…·ï¼Œå°†é€€åŒ–ä¸ºçº¯å¯¹è¯æ¨¡å¼ã€‚")

    return env, tools_schema, tool_registry


def _content_to_text(content: Any) -> str:
    """å°†æ¶ˆæ¯å†…å®¹å®‰å…¨è½¬æ¢ä¸ºæ–‡æœ¬ï¼ˆå…¼å®¹å­—ç¬¦ä¸²å’Œå¤šæ¨¡æ€ç»“æ„ï¼‰ã€‚"""
    if isinstance(content, str):
        return content
    if isinstance(content, list):
        parts: List[str] = []
        for part in content:
            try:
                if isinstance(part, dict) and part.get("type") == "text":
                    text_value = part.get("text", "")
                    if isinstance(text_value, str) and text_value.strip():
                        parts.append(text_value.strip())
            except Exception:
                continue
        return "\n\n".join(parts)
    try:
        return str(content)
    except Exception:
        return ""


def _extract_choice_message(response, context_label: str):
    """
    æå–å“åº”ä¸­çš„ç¬¬ä¸€æ¡æ¶ˆæ¯ï¼Œè‹¥choicesä¸ºç©ºåˆ™æŠ›å‡ºæ›´æ˜“æ’æŸ¥çš„å¼‚å¸¸ï¼ˆç²¾ç®€ç‰ˆï¼‰ã€‚
    """
    choices = getattr(response, "choices", None)
    if not choices:
        debug_payload = None
        if hasattr(response, "model_dump"):
            try:
                debug_payload = response.model_dump()
            except Exception:
                debug_payload = None
        if debug_payload is None:
            try:
                debug_payload = response.__dict__
            except Exception:
                debug_payload = repr(response)
        raise RuntimeError(f"{context_label} æ¨¡å‹å“åº”æœªåŒ…å«choicesï¼ŒåŸå§‹å“åº”: {debug_payload}")
    return choices[0].message


def _is_supported_image_path(path: Optional[str]) -> bool:
    """
    ç²—ç•¥åˆ¤æ–­å­—ç¬¦ä¸²æ˜¯å¦å¯èƒ½æ˜¯å›¾ç‰‡è·¯å¾„ï¼ˆåç¼€æ£€æŸ¥ï¼‰ã€‚
    ç”¨äº simple_test_query ä¸­çš„æœ€å°å›¾ç‰‡åµŒå…¥é€»è¾‘ã€‚
    """
    if not path or not isinstance(path, str):
        return False
    lowered = path.strip().lower()
    return lowered.endswith((".png", ".jpg", ".jpeg", ".gif", ".bmp", ".webp"))


def _resolve_existing_path(path_str: Optional[str]) -> Optional["Path"]:
    """
    ç®€åŒ–ç‰ˆè·¯å¾„è§£æï¼šåªåšæœ€åŸºæœ¬çš„ exists æ£€æŸ¥ï¼Œé¿å…ä¾èµ– test_executor ä¸­çš„å¤æ‚æœç´¢é€»è¾‘ã€‚
    """
    if not path_str or not isinstance(path_str, str):
        return None
    candidate = Path(path_str).expanduser()
    if candidate.exists():
        return candidate
    return None


def _refresh_preview_image(image_path: Optional[str]) -> None:
    """
    ç®€åŒ–ç‰ˆé¢„è§ˆæ›´æ–°ï¼šå½“å‰ä»…æ‰“å°æç¤ºï¼Œä¸åšå®é™…æ–‡ä»¶å¤åˆ¶ã€‚
    é¿å…ä¸ä¸»æ‰§è¡Œå™¨çš„é¢„è§ˆé€»è¾‘è€¦åˆã€‚
    """
    if not image_path:
        return
    print(f"ğŸ–¼ï¸ å·¥å…·ç”Ÿæˆå›¾ç‰‡: {image_path}")


def _derive_trace_path_for_debug(
    model_name: str,
    use_tools: bool,
    case_id: Any,
    mode_name: str,
    metadata: Dict[str, Any],
    dataset_filename: str,
) -> Path:
    """
    è°ƒè¯•ç”¨çš„ç²¾ç®€ç‰ˆ trace è·¯å¾„æ¨å¯¼å‡½æ•°ï¼Œå°½é‡ç®€å•ç›´è§‚ï¼š

    æ ¹æ®æ•°æ®é›†æ˜¯å¦ä¸ºå•æ¨¡æ€ / å¤šæ¨¡æ€ï¼Œè‡ªåŠ¨é€‰æ‹©å›ºå®šçš„çˆ¶ç›®å½•ï¼š

        data_analysis/tracetoanalyze/tracesmerged_single_questions
        data_analysis/tracetoanalyze/tracesmerged_questions

    å¹¶åœ¨å…¶ä¸‹æŒ‰å¦‚ä¸‹ç»“æ„ç»„ç»‡ï¼š

        [çˆ¶ç›®å½•] / model_name / mode_name / {case_id}_trace.json

    æ³¨æ„ï¼šè¿™é‡Œæœ‰æ„ä¸ä¾èµ– gym.test_executor å†…éƒ¨çš„è·¯å¾„æ¨å¯¼é€»è¾‘ï¼Œé¿å…å¤æ‚çš„å¯¼å…¥å’Œ sys.modules å‰¯ä½œç”¨ã€‚
    """
    # åˆ¤æ–­å•æ¨¡æ€ / å¤šæ¨¡æ€ï¼šä»…æ ¹æ®æ•°æ®é›†æ–‡ä»¶åæ˜¯å¦åŒ…å« "single"
    is_single = "single" in dataset_filename.lower()

    # å·¥ç¨‹æ ¹ç›®å½•ï¼šgym ä¸Šä¸€å±‚
    project_root = Path(__file__).resolve().parents[1]
    traces_root = project_root / "data_analysis" / "tracetoanalyze"

    if is_single:
        base_root = traces_root / "tracesmerged_single_questions"
    else:
        base_root = traces_root / "tracesmerged_questions"

    # è¿›ä¸€æ­¥æŒ‰æ¨¡å‹åä¸æ¨¡å¼åˆ†å­ç›®å½•
    model_dir = model_name or DEFAULT_MODEL
    base_dir = base_root / model_dir / mode_name
    base_dir.mkdir(parents=True, exist_ok=True)

    return base_dir / f"{case_id}_trace.json"


def _resolve_trace_root(metadata: Optional[Dict[str, Any]]) -> Path:
    """Determine the base trace directory for a given case."""
    if not isinstance(metadata, dict):
        metadata = {}
    trace_root_override = metadata.get('trace_root')
    if isinstance(trace_root_override, str) and trace_root_override.strip():
        override_path = Path(trace_root_override)
        if not override_path.is_absolute():
            override_path = Path(trace_root_override)
        return override_path
    dataset_key = metadata.get('dataset_key')
    return get_trace_root(dataset_key)


def _resolve_dataset_folder(metadata: Optional[Dict[str, Any]]) -> str:
    """
    æ ¹æ® dataset_key æ˜¯å¦åŒ…å« "single" æ¥åŒºåˆ†å•æ¨¡æ€å’Œå¤šæ¨¡æ€æ•°æ®é›†æ–‡ä»¶å¤¹ã€‚
    
    è§„åˆ™ï¼š
    - dataset_key åŒ…å« "single" -> "merged_single_questions" (å•æ¨¡æ€)
    - å¦åˆ™ -> "merged_questions" (å¤šæ¨¡æ€)
    
    ä¼˜å…ˆçº§ï¼š
    1. metadata['dataset_key'] (å¦‚æœå­˜åœ¨)
    2. ä» get_current_dataset_key() è·å–
    3. é»˜è®¤ 'merged_questions' (å¤šæ¨¡æ€)
    """
    from gym.config.dataset_config import get_current_dataset_key
    
    dataset_key = None
    if isinstance(metadata, dict):
        dataset_key = metadata.get('dataset_key')
    
    if not dataset_key:
        dataset_key = get_current_dataset_key()
    
    # æ ¹æ® dataset_key ä¸­æ˜¯å¦åŒ…å« "single" æ¥åˆ¤æ–­
    if dataset_key and "single" in dataset_key.lower():
        return "merged_single_questions"
    else:
        return "merged_questions"


def _sanitize_trace_tag(tag: str) -> str:
    """å°†traceæ ‡ç­¾è½¬æ¢ä¸ºå®‰å…¨çš„æ–‡ä»¶å¤¹åç§°"""
    cleaned = ''.join(c if c.isalnum() or c in ('-', '_') else '_' for c in tag.strip())
    return cleaned or "default"


def _resolve_mode_folder(use_tools: bool, override: Optional[str] = None) -> str:
    """
    è¿”å›æ¨¡å¼ç›®å½•åç§°ã€‚
    - æ˜¾å¼ override æ—¶å°Šé‡ä¼ å…¥å€¼ï¼ˆå¯å¸¦/ä¸å¸¦ react åç¼€ï¼‰ã€‚
    - é»˜è®¤ with_tools ä¸å†è‡ªåŠ¨è¿½åŠ åç¼€ï¼›react ç‰ˆæœ¬éœ€æ˜¾å¼æŒ‡å®šå¸¦åç¼€çš„ overrideã€‚
    """
    if override:
        return override
    base = "with_tools" if use_tools else "without_tools"
    return base


def _derive_trace_path(
    model_name: str,
    use_tools: bool,
    case_id,
    trace_tag: Optional[str] = None,
    mode_name: Optional[str] = None,
    metadata: Optional[Dict[str, Any]] = None,
    dataset_filename: Optional[str] = None,
):
    """æ ¹æ®æ¨¡å‹ã€æ¨¡å¼å’Œæ ‡ç­¾ç”Ÿæˆtraceè·¯å¾„
    
    Args:
        model_name: æ¨¡å‹åç§°
        use_tools: æ˜¯å¦ä½¿ç”¨å·¥å…·
        case_id: æ¡ˆä¾‹ID
        trace_tag: è½¨è¿¹æ ‡ç­¾
        mode_name: æ¨¡å¼åç§°
        metadata: å…ƒæ•°æ®
        dataset_filename: æ•°æ®é›†æ–‡ä»¶åï¼ˆç”¨äºåˆ¤æ–­ single/multiï¼‰ï¼Œå¦‚æœæä¾›åˆ™ä¼˜å…ˆä½¿ç”¨
    """
    mode_folder = _resolve_mode_folder(use_tools, mode_name)
    sanitized_tag = _sanitize_trace_tag(trace_tag) if trace_tag else None
    
    # æ ¹æ®æ•°æ®é›†æ–‡ä»¶ååˆ¤æ–­æ˜¯ single è¿˜æ˜¯ multi
    # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ dataset_filenameï¼Œå…¶æ¬¡ä½¿ç”¨ metadata ä¸­çš„ _dataset_filename
    filename_to_check = dataset_filename
    if not filename_to_check and isinstance(metadata, dict):
        filename_to_check = metadata.get('_dataset_filename')
    
    if filename_to_check and "single" in filename_to_check.lower():
        data_type_folder = "orignal_data_single"
    else:
        # å¦‚æœæ²¡æœ‰æä¾›æ–‡ä»¶åï¼Œä½¿ç”¨ metadata ä¸­çš„ dataset_key åˆ¤æ–­
        dataset_folder = _resolve_dataset_folder(metadata)
        if "single" in dataset_folder.lower():
            data_type_folder = "orignal_data_single"
        else:
            data_type_folder = "orignal_data_multi"
    
    base_dir = _resolve_trace_root(metadata) / model_name / data_type_folder / mode_folder
    if sanitized_tag:
        base_dir /= sanitized_tag
    trace_path = base_dir / f"{case_id}_trace.json" 
    
    return trace_path, sanitized_tag


def _record_skip_event(
    trace_path: Optional[Path],
    case_id: Any,
    model_name: str,
    mode_desc: str,
    reason: str,
    extra: Optional[Dict[str, Any]] = None,
) -> None:
    """Persist skip information alongside printing a friendly log."""
    case_label = case_id if case_id is not None else "unknown"
    print(f"â­ï¸ æ¡ˆä¾‹ {case_label} ({model_name} Â· {mode_desc}) è·³è¿‡ï¼ŒåŸå› ï¼š{reason}")
    payload = {
        "id": case_id,
        "model": model_name,
        "mode": mode_desc,
        "reason": reason,
        "timestamp": time.time(),
    }
    if extra:
        payload["details"] = extra

    if trace_path is None:
        return

    trace_path.parent.mkdir(parents=True, exist_ok=True)
    base_name = trace_path.name
    if base_name.endswith("_trace.json"):
        skip_name = base_name.replace("_trace.json", "_skip.json")
    else:
        skip_name = f"{trace_path.stem}_skip.json"
    skip_path = trace_path.with_name(skip_name)
    try:
        with skip_path.open("w", encoding="utf-8") as f:
            json.dump(payload, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“ è·³è¿‡è®°å½•å·²ä¿å­˜: {skip_path}")
    except Exception as exc:
        print(f"âš ï¸ ä¿å­˜è·³è¿‡è®°å½•å¤±è´¥ ({skip_path}): {exc}")


def _parse_glm_text_tool_calls(content: str, provider: Optional[str] = None) -> Optional[List[Any]]:
    """
    è§£æ GLM æ¨¡å‹è¿”å›çš„æ–‡æœ¬æ ¼å¼å·¥å…·è°ƒç”¨ã€‚
    
    GLM åœ¨ä½¿ç”¨ ReAct prompt æ—¶ï¼Œå¯èƒ½è¿”å›å¦‚ä¸‹æ ¼å¼ï¼š
    Action: tool_name
    <arg_key>param1</arg_key>
    <arg_value>value1</arg_value>
    ...
    </tool_call>
    
    Args:
        content: åŠ©æ‰‹æ¶ˆæ¯çš„æ–‡æœ¬å†…å®¹
        provider: æ¨¡å‹æä¾›å•†ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦è§£æï¼‰
    
    Returns:
        è§£æåçš„ tool_calls åˆ—è¡¨ï¼Œæ ¼å¼ä¸ OpenAI å…¼å®¹ï¼Œå¦‚æœæ— æ³•è§£æåˆ™è¿”å› None
    """
    import re
    import uuid
    
    # åªå¯¹ GLM/ZhipuAI è¿›è¡Œè§£æ
    if provider not in ("zhipuai", "glm"):
        return None
    
    if not content or not isinstance(content, str):
        return None
    
    # æŸ¥æ‰¾ Action: å¼€å¤´çš„å·¥å…·è°ƒç”¨
    # åŒ¹é…æ¨¡å¼ï¼šAction: tool_name åè·Ÿå‚æ•°ï¼Œç›´åˆ° </tool_call> æˆ–ä¸‹ä¸€ä¸ª Action/Thought/Final Answer
    action_pattern = r'Action:\s*(\w+)\s*\n(.*?)(?=</tool_call>|\nAction:|\nThought:|\nFinal Answer:|$)'
    matches = list(re.finditer(action_pattern, content, re.DOTALL | re.IGNORECASE))
    
    # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œå°è¯•æ›´å®½æ¾çš„æ¨¡å¼ï¼ˆä¸è¦æ±‚æ¢è¡Œï¼‰
    if not matches:
        action_pattern = r'Action:\s*(\w+)\s*(.*?)(?=</tool_call>|\nAction:|\nThought:|\nFinal Answer:|$)'
        matches = list(re.finditer(action_pattern, content, re.DOTALL | re.IGNORECASE))
    
    if not matches:
        # è°ƒè¯•ï¼šæ‰“å°éƒ¨åˆ†å†…å®¹ä»¥ä¾¿æ’æŸ¥
        content_preview = content[:500] if len(content) > 500 else content
        print(f"âš ï¸  æœªæ‰¾åˆ° Action: æ¨¡å¼ï¼Œå†…å®¹é¢„è§ˆ: {repr(content_preview)}")
        return None
    
    tool_calls = []
    for match in matches:
        tool_name = match.group(1).strip()
        args_text = match.group(2).strip()
        
        if not tool_name:
            continue
        
        print(f"ğŸ” æ‰¾åˆ°å·¥å…·è°ƒç”¨: {tool_name}ï¼Œå‚æ•°æ–‡æœ¬: {args_text[:200]}...")
        
        # è§£æå‚æ•°ï¼šæŸ¥æ‰¾ <arg_key> å’Œ <arg_value> å¯¹
        arg_pattern = r'<arg_key>([^<]+)</arg_key>\s*<arg_value>([^<]+)</arg_value>'
        arg_matches = list(re.finditer(arg_pattern, args_text))
        
        arguments = {}
        for arg_match in arg_matches:
            key = arg_match.group(1).strip()
            value_str = arg_match.group(2).strip()
            
            # å°è¯•è§£æå€¼ï¼ˆå¯èƒ½æ˜¯ JSONã€æ•°å­—ã€å­—ç¬¦ä¸²ç­‰ï¼‰
            try:
                # å°è¯•è§£æä¸º JSONï¼ˆå¤„ç†æ•°ç»„ã€å¯¹è±¡ç­‰ï¼‰
                value = json.loads(value_str)
            except json.JSONDecodeError:
                # å°è¯•è§£æä¸ºæ•°å­—
                try:
                    if '.' in value_str:
                        value = float(value_str)
                    else:
                        value = int(value_str)
                except ValueError:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ null
                    if value_str.lower() in ('null', 'none'):
                        value = None
                    else:
                        # ä¿æŒä¸ºå­—ç¬¦ä¸²
                        value = value_str
            
            arguments[key] = value
            print(f"   å‚æ•°: {key} = {value} (ç±»å‹: {type(value).__name__})")
        
        if not arguments:
            print(f"âš ï¸  å·¥å…· {tool_name} æœªæ‰¾åˆ°ä»»ä½•å‚æ•°ï¼ˆå¯èƒ½æ˜¯æ— å‚æ•°å·¥å…·ï¼‰")
        
        # å¦‚æœæ‰¾åˆ°äº†å·¥å…·åï¼Œåˆ›å»º tool_call å¯¹è±¡ï¼ˆå³ä½¿æ²¡æœ‰å‚æ•°ä¹Ÿå¯ä»¥ï¼‰
        if tool_name:
            # åˆ›å»ºç±»ä¼¼ OpenAI æ ¼å¼çš„ tool_call
            tool_call_dict = {
                'id': f'call_{uuid.uuid4().hex[:16]}',
                'type': 'function',
                'function': {
                    'name': tool_name,
                    'arguments': json.dumps(arguments, ensure_ascii=False)
                }
            }
            
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„å¯¹è±¡æ¥æ¨¡æ‹Ÿ tool_call
            class ToolCall:
                def __init__(self, data):
                    self.id = data['id']
                    self.type = data['type']
                    self.function = type('Function', (), {
                        'name': data['function']['name'],
                        'arguments': data['function']['arguments']
                    })()
            
            tool_calls.append(ToolCall(tool_call_dict))
            print(f"âœ… è§£æåˆ°å·¥å…·è°ƒç”¨: {tool_name}ï¼Œå‚æ•°: {arguments}")
    
    return tool_calls if tool_calls else None


def _detect_provider(model_name: str) -> Optional[str]:
    """
    æ ¹æ®æ¨¡å‹åç§°åœ¨ SUPPORTED_MODELS ä¸­æŸ¥æ‰¾ providerï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦èµ° GLM æ–‡æœ¬å·¥å…·è°ƒç”¨è§£æé€»è¾‘ã€‚
    """
    cfg = SUPPORTED_MODELS.get(model_name) or {}
    provider = cfg.get("provider")
    if isinstance(provider, str):
        return provider.lower()
    return None


def _build_basic_user_message(
    query_data: Dict[str, Any],
    test_type: str,
) -> Tuple[Dict[str, Any], Dict[str, Any]]:
    """
    æ„é€ æœ€åŸºç¡€çš„ç”¨æˆ·æ¶ˆæ¯ï¼š
    - ä½¿ç”¨ metadata ä¸­çš„å›¾ç‰‡ä¿¡æ¯ï¼Œé€šè¿‡ process_question_with_images_from_metadata ç»Ÿä¸€å¤„ç†
    - å¯¹äºæœ‰å›¾ç‰‡çš„æƒ…å†µï¼Œå›¾ç‰‡åœ¨å‰ã€æ–‡æœ¬åœ¨å
    - æš‚ä¸æ³¨å…¥å¤æ‚çš„ã€Œæ¨¡æ¿ / ç»“æ„åŒ– JSON æ ¼å¼è¦æ±‚ã€ï¼Œåªä¿è¯é—®é¢˜æœ¬èº«è¢«æ¸…æ´—ä¸ä¿ç•™
    """
    question_data = process_question_with_images_from_metadata(query_data)
    user_text = question_data.get("text") or str(query_data.get("question", ""))

    if not user_text.strip():
        raise ValueError("é—®é¢˜å†…å®¹ä¸ºç©ºï¼Œæ— æ³•æ„é€ ç”¨æˆ·æ¶ˆæ¯ã€‚")

    images = question_data.get("images") or []
    if images:
        content_parts: List[Dict[str, Any]] = []
        # å…ˆæ”¾æ‰€æœ‰å›¾ç‰‡
        for img in images:
            mime_type = img.get("mime_type") or "image/png"
            content_parts.append(
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:{mime_type};base64,{img['base64']}",
                    },
                }
            )
        # å†æ”¾æ–‡æœ¬
        content_parts.append({"type": "text", "text": user_text})
        user_message: Dict[str, Any] = {"role": "user", "content": content_parts}
    else:
        user_message = {"role": "user", "content": user_text}

    return question_data, user_message


def simple_test_query(
    query_data: Dict[str, Any],
    model_name: Optional[str] = None,
    use_tools: bool = True,
    max_rounds: int = 50,
    auto_infer_from_metadata: bool = True,
) -> Dict[str, Any]:
    """
    ç²¾ç®€ç‰ˆå•é¢˜æ‰§è¡Œå…¥å£ï¼š

    åŠŸèƒ½ï¼š
    - ä¸ºå•ä¸ªæ¡ˆä¾‹åŠ è½½æœ¬åœ°å·¥å…·ä¸ç¯å¢ƒï¼ˆé€šè¿‡ `_build_env_and_tools_from_loaded`ï¼‰
    - æ„é€ æœ€åŸºç¡€çš„å¤šæ¨¡æ€ç”¨æˆ·æ¶ˆæ¯
    - åœ¨æœ‰å·¥å…·çš„æƒ…å†µä¸‹ï¼Œæ‰§è¡Œä¸€ä¸ªã€Œå•å·¥å…·è°ƒç”¨é“¾ã€çš„å¤šè½®å¯¹è¯å¾ªç¯
    - è¿”å›ï¼š
        {
          "final_answer": <str>,       # æ¨¡å‹æœ€ç»ˆè‡ªç„¶è¯­è¨€å›ç­”ï¼ˆæ–‡æœ¬ï¼‰
          "messages": [...],           # å¯¹è¯è½¨è¿¹ï¼ˆå¯é€‰ï¼Œç”¨äºè°ƒè¯•ï¼‰
          "model_name": <str>,
          "used_tools": bool,
        }
    è¯´æ˜ï¼š
    - ä¸åšæ¨¡æ¿ / è¯„åˆ† / trace è½ç›˜ï¼Œåªå…³æ³¨ã€Œé—®é¢˜ â†’ å·¥å…·è°ƒç”¨ â†’ å›ç­”ã€ä¸»å¹²æµç¨‹ã€‚
    
    Args:
        query_data: æµ‹è¯•æ¡ˆä¾‹æ•°æ®
        model_name: æŒ‡å®šçš„æ¨¡å‹åç§°
        use_tools: æ˜¯å¦ä½¿ç”¨å·¥å…·
        max_rounds: æœ€å¤§å¯¹è¯è½®æ•°
        auto_infer_from_metadata: æ˜¯å¦æ ¹æ® metadata ä¸­çš„ subject/topic è‡ªåŠ¨æ¨æ–­å¹¶åŠ è½½å·¥å…·ç›®å½•
                                   é»˜è®¤ä¸º Trueï¼Œä¼šè‡ªåŠ¨åŠ è½½ toolkits/{subject}/{topic}/ ä¸‹çš„æ‰€æœ‰å·¥å…·
    """
    current_model = model_name or DEFAULT_MODEL
    client = get_client(current_model)

    # 1. åŸºäº_build_env_and_tools_for_caseå§‹ç»ˆæ„å»ºç¯å¢ƒä¸ï¼ˆå¯èƒ½ä¸ºç©ºçš„ï¼‰å·¥å…·æ³¨å†Œè¡¨ï¼ˆæ”¯æŒè‡ªåŠ¨æ¨æ–­ï¼‰
    env, tools_schema, tool_registry = _build_env_and_tools_for_case(
        query_data, auto_infer_from_metadata=auto_infer_from_metadata
    )

    # 2. åŸºäº_build_basic_user_messageæ„é€ ç”¨æˆ·æ¶ˆæ¯ï¼ˆå¤šæ¨¡æ€ï¼‰â€”â€”ä¸æ—§ç‰ˆ test_query å¤ç”¨åŒä¸€æ¨¡å—
    metadata = query_data.get("metadata") or {}
    if not isinstance(metadata, dict):
        metadata = {}
    test_type = metadata.get("test_type", "normal")
    question_data, user_message = _build_basic_user_message(query_data, test_type)

    # æ³¨å…¥æ ¼å¼æ¨¡æ¿è¦æ±‚ï¼Œç¡®ä¿æ¨¡å‹è¾“å‡ºç¬¦åˆ extract_boxed_answer çš„é¢„æœŸæ ¼å¼
    user_content_base = question_data.get("text") or ""
    if not user_content_base.strip():
        raise ValueError("é—®é¢˜å†…å®¹ä¸ºç©ºï¼Œæ— æ³•æ„é€ ç”¨æˆ·æ¶ˆæ¯ã€‚")

    # å¦‚æœä½¿ç”¨å·¥å…·ï¼Œç§»é™¤æ ¼å¼è¦æ±‚ä¸­çš„æ¨ç†è¿‡ç¨‹éƒ¨åˆ†
    if use_tools and tools_schema:
        # æŸ¥æ‰¾æ ¼å¼è¦æ±‚çš„èµ·å§‹ä½ç½®
        format_start = user_content_base.find("You should strictly respond in this exact format")
        if format_start != -1:
            # æå–æ ¼å¼è¦æ±‚ä¹‹å‰çš„é—®é¢˜éƒ¨åˆ†
            question_part = user_content_base[:format_start].strip()

            # æ„å»ºç®€åŒ–çš„æ ¼å¼è¦æ±‚ï¼ˆåªä¿ç•™Answeréƒ¨åˆ†ï¼‰
            simplified_format = _get_tool_mode_answer_prompt()

            # ç»„åˆæ–°çš„ç”¨æˆ·å†…å®¹
            user_content = question_part + "\n" + simplified_format

            print("âœ‚ï¸ å·²ç§»é™¤æ¨ç†è¿‡ç¨‹æ ¼å¼è¦æ±‚")
        else:
            # æ„å»ºç®€åŒ–çš„æ ¼å¼è¦æ±‚ï¼ˆåªä¿ç•™Answeréƒ¨åˆ†ï¼‰
            simplified_format = _get_tool_mode_answer_prompt()

            user_content = user_content_base + "\n" + simplified_format
    else:
        # çº¯æ–‡æœ¬æ¨¡å¼ï¼šæ„å»ºåŒ…å«æ¨ç†è¿‡ç¨‹çš„æ ¼å¼è¦æ±‚
        simplified_format = _get_text_mode_reasoning_answer_prompt()

        user_content = user_content_base + "\n" + simplified_format

    # æ›´æ–°ç”¨æˆ·æ¶ˆæ¯ä¸­çš„æ–‡æœ¬éƒ¨åˆ†ï¼ˆä¿æŒå¤šæ¨¡æ€ç»“æ„ä¸å˜ï¼‰
    if isinstance(user_message.get("content"), list):
        # å¤šæ¨¡æ€ï¼šæ‰¾åˆ°æœ€åä¸€ä¸ª text ç±»å‹ç‰‡æ®µå¹¶æ›¿æ¢ä¸ºå¸¦æ ¼å¼çš„æ–‡æœ¬
        replaced = False
        for part in reversed(user_message["content"]):
            if isinstance(part, dict) and part.get("type") == "text":
                part["text"] = user_content
                replaced = True
                break
        if not replaced:
            user_message["content"].append({"type": "text", "text": user_content})
    else:
        # çº¯æ–‡æœ¬ï¼šç›´æ¥æ›¿æ¢ä¸ºå¸¦æ ¼å¼çš„æ–‡æœ¬
        user_message["content"] = user_content

    # å¦‚æœä½¿ç”¨å·¥å…·ï¼Œæ·»åŠ  ReAct system prompt ä»¥æ›´å¥½åœ°å¼•å¯¼å·¥å…·è°ƒç”¨
    messages: List[Dict[str, Any]] = []
    if use_tools and tools_schema:
        messages.append({"role": "system", "content": _get_react_tool_system_prompt()})
    messages.append(user_message)

    # 3. ä¸»å¾ªç¯ï¼šæœ‰å·¥å…· vs æ— å·¥å…·
    if not use_tools or not tools_schema:
        # çº¯æ–‡æœ¬ / çº¯å¤šæ¨¡æ€å•è½®è°ƒç”¨
        response = client.chat_completions_create(messages=messages)
        assistant_message = _extract_choice_message(response, "simple_text_only")
        final_content = _content_to_text(getattr(assistant_message, "content", None))
        messages.append(
            {
                "role": "assistant",
                "content": getattr(assistant_message, "content", None),
            }
        )
        return {
            "final_answer": final_content,
            "messages": messages,
            "model_name": current_model,
            "used_tools": False,
        }

    # 4. å·¥å…·æ¨¡å¼å¤šè½®å¯¹è¯
    provider = _detect_provider(current_model)
    round_count = 0
    final_answer_text: str = ""

    while round_count < max_rounds:
        round_count += 1

        response = client.chat_completions_create(
            messages=messages,
            tools=tools_schema,
            tool_choice="auto",
            parallel_tool_calls=False,
        )

        assistant_message = _extract_choice_message(response, f"simple_round_{round_count}")
        messages.append(assistant_message)

        # æ‰“å° / è®°å½•æ–‡æœ¬å½¢å¼ï¼Œæ–¹ä¾¿è°ƒç”¨æ–¹è°ƒè¯•
        assistant_text = _content_to_text(getattr(assistant_message, "content", None))

        # å°è¯•è¯»å–å·¥å…·è°ƒç”¨
        tool_calls = getattr(assistant_message, "tool_calls", None)

        # é’ˆå¯¹ GLMï¼šå¦‚æœ tool_calls ä¸ºç©ºï¼Œå°è¯•ä»æ–‡æœ¬ä¸­è§£æ
        if not tool_calls and provider in ("zhipuai", "glm"):
            print(f"[simple_test_query] GLM æ–‡æœ¬æ ¼å¼è§£æï¼šprovider={provider}, å†…å®¹é•¿åº¦={len(assistant_text)}")
            if "Action:" in assistant_text:
                print(f"[simple_test_query] æ£€æµ‹åˆ° Action: æ¨¡å¼ï¼Œå¼€å§‹è§£æ...")
            parsed = _parse_glm_text_tool_calls(assistant_text, provider)
            if parsed:
                print(f"[simple_test_query] âœ… æˆåŠŸè§£æ GLM æ–‡æœ¬æ ¼å¼å·¥å…·è°ƒç”¨ï¼Œæ•°é‡: {len(parsed)}")
                tool_calls = parsed
            else:
                print(f"[simple_test_query] âš ï¸ GLM æ–‡æœ¬æ ¼å¼è§£æå¤±è´¥ï¼Œå†…å®¹é¢„è§ˆ: {assistant_text[:300]}")

        # è°ƒè¯•è¾“å‡ºï¼šæœ¬è½®å·¥å…·è°ƒç”¨æ•°é‡
        if tool_calls:
            print(f"[simple_test_query] ç¬¬ {round_count} è½®è¿”å›å·¥å…·è°ƒç”¨æ•°: {len(tool_calls)}")
        else:
            print(f"[simple_test_query] ç¬¬ {round_count} è½®æœªè¿”å›å·¥å…·è°ƒç”¨ï¼Œç»“æŸå¯¹è¯ã€‚")

        # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè§†ä¸ºå¯¹è¯ç»“æŸ
        if not tool_calls:
            final_answer_text = assistant_text
            break

        # å½“å‰å®ç°ï¼šé¡ºåºæ‰§è¡Œæ¯ä¸€ä¸ªå·¥å…·è°ƒç”¨
        for tool_call in tool_calls:
            raw_arguments = getattr(tool_call.function, "arguments", "{}")
            try:
                arguments = json.loads(raw_arguments)
            except Exception:
                arguments = {}

            tool_name = getattr(tool_call.function, "name", None)
            if not tool_name or tool_name not in (tool_registry or {}):
                # æœªçŸ¥å·¥å…·ï¼šç›´æ¥æŠŠé”™è¯¯åé¦ˆç»™æ¨¡å‹
                tool_msg = {
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "content": json.dumps(
                        {"error": f"æœªçŸ¥å·¥å…·: {tool_name}"},
                        ensure_ascii=False,
                    ),
                }
                messages.append(tool_msg)
                continue

            # è°ƒç”¨ç¯å¢ƒä¸­çš„å·¥å…·
            try:
                from gym.core.tool_loader import run_tool_call

                tool_result = run_tool_call(env, tool_name, arguments, tool_call.id)
                result = tool_result.get("result") 
                print("tool execute result",result)
            except Exception as e:
                tool_result = {"status": "error", "error": str(e)}
                result = tool_result

            # å°è¯•å¤„ç†ã€Œå·¥å…·è¿”å›è·¯å¾„ â†’ å›¾ç‰‡ / æ–‡ä»¶åµŒå…¥ã€çš„æƒ…å†µï¼ˆå¤ç”¨è€é€»è¾‘ï¼‰
            # æ³¨æ„ï¼šè¿™é‡Œåªåšæœ€å°åŒ–å¤„ç†ï¼Œä¸ä¿®æ”¹åŸå§‹ç»“æœç»“æ„
            try:
                if isinstance(result, dict) and "filename" in result:
                    fname = str(result["filename"])
                    resolved = _resolve_existing_path(fname)
                    if resolved:
                        import base64

                        with open(resolved, "rb") as f:
                            b64 = base64.b64encode(f.read()).decode("ascii")
                        result["_embedded_file_base64"] = b64
                        result["_embedded_file_name"] = resolved.name
                        result["_generated_file_path"] = str(resolved)
                        _refresh_preview_image(str(resolved))
                elif isinstance(result, str):
                    candidate_path = result.strip().strip('"').strip("'")
                    if _is_supported_image_path(candidate_path):
                        resolved_candidate = _resolve_existing_path(candidate_path)
                        if resolved_candidate and resolved_candidate.exists():
                            import base64

                            with open(resolved_candidate, "rb") as f:
                                b64 = base64.b64encode(f.read()).decode("ascii")
                            result = {
                                "original_result": candidate_path,
                                "_embedded_file_base64": b64,
                                "_embedded_file_name": resolved_candidate.name,
                                "_generated_file_path": str(resolved_candidate),
                            }
                            _refresh_preview_image(str(resolved_candidate))
            except Exception:
                # å›¾ç‰‡å¤„ç†å¤±è´¥ä¸åº”å½±å“ä¸»æµç¨‹
                pass

            tool_content = json.dumps(result, ensure_ascii=False, default=str)

            tool_message = {
                "role": "tool",
                "tool_call_id": tool_call.id,
                "content": tool_content,
            }
            messages.append(tool_message)

    if round_count >= max_rounds and not final_answer_text:
        final_answer_text = assistant_text + "\n[å¯¹è¯è½®æ•°è¾¾åˆ°ä¸Šé™ï¼Œè‡ªåŠ¨ç»ˆæ­¢]"

    return {
        "final_answer": final_answer_text,
        "messages": messages,
        "model_name": current_model,
        "used_tools": True,
        "rounds": round_count,
    }


def simple_test_refine_query(
    query_data: Dict[str, Any],
    model_name: Optional[str] = None,
    use_tools: bool = True,
    max_rounds: int = 50,
) -> Dict[str, Any]:
    """
    ç²¾ç®€ç‰ˆç²¾ç‚¼æ•°æ®æµ‹è¯•æ‰§è¡Œå…¥å£ï¼š
    
    åŠŸèƒ½ï¼š
    - ä¸“é—¨ç”¨äºæµ‹è¯•ç²¾ç‚¼ç‰ˆæ•°æ®ï¼ˆrefined_versionsï¼‰ï¼Œè¿™äº›æ•°æ®å·²ç»é€šè¿‡ load_refined_test_cases_from_dataset é‡æ„
    - ä¸ºå•ä¸ªæ¡ˆä¾‹åŠ è½½æœ¬åœ°å·¥å…·ä¸ç¯å¢ƒ
    - æ„é€ å¤šæ¨¡æ€ç”¨æˆ·æ¶ˆæ¯
    - æ‰§è¡Œå·¥å…·è°ƒç”¨å¾ªç¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    - æå–ç»“æ„åŒ–ç­”æ¡ˆå¹¶è¿›è¡Œè¿‡ç¨‹è¯„åˆ†
    
    è¿”å›ï¼š
        {
          "final_answer": <str>,              # æ¨¡å‹æœ€ç»ˆè‡ªç„¶è¯­è¨€å›ç­”ï¼ˆæ–‡æœ¬ï¼‰
          "structured_answer": <dict>,       # æå–çš„ç»“æ„åŒ–ç­”æ¡ˆï¼ˆç”¨äºè¯„åˆ†ï¼‰
          "score": <float>,                  # è¯„åˆ†ç»“æœï¼ˆ0.0-1.0ï¼‰
          "score_summary": <str>,            # è¯„åˆ†æ‘˜è¦
          "score_details": <dict>,           # è¯„åˆ†è¯¦ç»†ä¿¡æ¯
          "messages": [...],                 # å¯¹è¯è½¨è¿¹
          "model_name": <str>,
          "used_tools": bool,
        }
    
    è¯´æ˜ï¼š
    - ç²¾ç‚¼ç‰ˆæ•°æ®å·²ç»é€šè¿‡ load_refined_test_cases_from_dataset é‡æ„ï¼Œquery_data ç»“æ„åŒ…å«ï¼š
      - question: refined_question
      - answer: final_answer (å­—ç¬¦ä¸²)
      - metadata.golden_answer: final_answer (åˆ—è¡¨æ ¼å¼)
      - metadata.test_type: 'refined'
    - ä½¿ç”¨ calculate_answer_score è¿›è¡Œè¿‡ç¨‹è¯„åˆ†ï¼Œéœ€è¦ç»“æ„åŒ–ç­”æ¡ˆ
    """
    current_model = model_name or DEFAULT_MODEL
    client = get_client(current_model)

    # 1. åŸºäº_build_env_and_tools_for_caseå§‹ç»ˆæ„å»ºç¯å¢ƒä¸ï¼ˆå¯èƒ½ä¸ºç©ºçš„ï¼‰å·¥å…·æ³¨å†Œè¡¨
    env, tools_schema, tool_registry = _build_env_and_tools_for_case(query_data)

    # 2. åŸºäº_build_basic_user_messageæ„é€ ç”¨æˆ·æ¶ˆæ¯ï¼ˆå¤šæ¨¡æ€ï¼‰
    # æ³¨æ„ï¼šç²¾ç‚¼ç‰ˆæ•°æ®å·²ç»é€šè¿‡ load_refined_test_cases_from_dataset é‡æ„ï¼Œ
    # query_data çš„ç»“æ„ä¸æ™®é€šæ¡ˆä¾‹ç±»ä¼¼ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ _build_basic_user_message
    metadata = query_data.get("metadata") or {}
    if not isinstance(metadata, dict):
        metadata = {}
    test_type = metadata.get("test_type", "refined")  # ç²¾ç‚¼ç‰ˆé»˜è®¤ä¸º 'refined'
    question_data, user_message = _build_basic_user_message(query_data, test_type)

    # æ³¨å…¥æ ¼å¼æ¨¡æ¿è¦æ±‚ï¼Œç¡®ä¿æ¨¡å‹è¾“å‡ºç»“æ„åŒ–ç­”æ¡ˆ
    user_content_base = question_data.get("text") or ""
    if not user_content_base.strip():
        raise ValueError("é—®é¢˜å†…å®¹ä¸ºç©ºï¼Œæ— æ³•æ„é€ ç”¨æˆ·æ¶ˆæ¯ã€‚")

    # ç²¾ç‚¼ç‰ˆæ•°æ®éœ€è¦ç»“æ„åŒ–ç­”æ¡ˆï¼Œæ‰€ä»¥æ ¼å¼è¦æ±‚åº”è¯¥å¼•å¯¼æ¨¡å‹è¾“å‡º JSON æ ¼å¼
    # å¦‚æœä½¿ç”¨å·¥å…·ï¼Œç§»é™¤æ ¼å¼è¦æ±‚ä¸­çš„æ¨ç†è¿‡ç¨‹éƒ¨åˆ†
    if use_tools and tools_schema:
        # æŸ¥æ‰¾æ ¼å¼è¦æ±‚çš„èµ·å§‹ä½ç½®
        format_start = user_content_base.find("You should strictly respond in this exact format")
        if format_start != -1:
            # æå–æ ¼å¼è¦æ±‚ä¹‹å‰çš„é—®é¢˜éƒ¨åˆ†
            question_part = user_content_base[:format_start].strip()
            # æ„å»ºç®€åŒ–çš„æ ¼å¼è¦æ±‚ï¼ˆåªä¿ç•™Answeréƒ¨åˆ†ï¼‰
            simplified_format = _get_tool_mode_answer_prompt()
            user_content = question_part + "\n" + simplified_format
            print("âœ‚ï¸ å·²ç§»é™¤æ¨ç†è¿‡ç¨‹æ ¼å¼è¦æ±‚")
        else:
            simplified_format = _get_tool_mode_answer_prompt()
            user_content = user_content_base + "\n" + simplified_format
    else:
        # çº¯æ–‡æœ¬æ¨¡å¼ï¼šæ„å»ºåŒ…å«æ¨ç†è¿‡ç¨‹çš„æ ¼å¼è¦æ±‚
        simplified_format = _get_text_mode_reasoning_answer_prompt()
        user_content = user_content_base + "\n" + simplified_format

    # æ›´æ–°ç”¨æˆ·æ¶ˆæ¯ä¸­çš„æ–‡æœ¬éƒ¨åˆ†ï¼ˆä¿æŒå¤šæ¨¡æ€ç»“æ„ä¸å˜ï¼‰
    if isinstance(user_message.get("content"), list):
        # å¤šæ¨¡æ€ï¼šæ‰¾åˆ°æœ€åä¸€ä¸ª text ç±»å‹ç‰‡æ®µå¹¶æ›¿æ¢ä¸ºå¸¦æ ¼å¼çš„æ–‡æœ¬
        replaced = False
        for part in reversed(user_message["content"]):
            if isinstance(part, dict) and part.get("type") == "text":
                part["text"] = user_content
                replaced = True
                break
        if not replaced:
            user_message["content"].append({"type": "text", "text": user_content})
    else:
        # çº¯æ–‡æœ¬ï¼šç›´æ¥æ›¿æ¢ä¸ºå¸¦æ ¼å¼çš„æ–‡æœ¬
        user_message["content"] = user_content

    # 3. å¦‚æœä½¿ç”¨å·¥å…·ï¼Œæ·»åŠ  ReAct system prompt ä»¥æ›´å¥½åœ°å¼•å¯¼å·¥å…·è°ƒç”¨
    messages: List[Dict[str, Any]] = []
    if use_tools and tools_schema:
        messages.append({"role": "system", "content": _get_react_tool_system_prompt()})
    messages.append(user_message)

    # 4. ä¸»å¾ªç¯ï¼šæœ‰å·¥å…· vs æ— å·¥å…·
    if not use_tools or not tools_schema:
        # çº¯æ–‡æœ¬ / çº¯å¤šæ¨¡æ€å•è½®è°ƒç”¨
        response = client.chat_completions_create(messages=messages)
        assistant_message = _extract_choice_message(response, "simple_refine_text_only")
        final_content = _content_to_text(getattr(assistant_message, "content", None))
        messages.append(
            {
                "role": "assistant",
                "content": getattr(assistant_message, "content", None),
            }
        )
        
        # æå–ç»“æ„åŒ–ç­”æ¡ˆå¹¶è¿›è¡Œè¯„åˆ†
        structured_answer, score, score_summary, score_details = _evaluate_refined_answer(
            final_content, query_data
        )
        
        return {
            "final_answer": final_content,
            "structured_answer": structured_answer,
            "score": score,
            "score_summary": score_summary,
            "score_details": score_details,
            "messages": messages,
            "model_name": current_model,
            "used_tools": False,
        }

    # 5. å·¥å…·æ¨¡å¼å¤šè½®å¯¹è¯
    provider = _detect_provider(current_model)
    round_count = 0
    final_answer_text: str = ""

    while round_count < max_rounds:
        round_count += 1

        response = client.chat_completions_create(
            messages=messages,
            tools=tools_schema,
            tool_choice="auto",
            parallel_tool_calls=False,
        )

        assistant_message = _extract_choice_message(response, f"simple_refine_round_{round_count}")
        messages.append(assistant_message)

        # æ‰“å° / è®°å½•æ–‡æœ¬å½¢å¼ï¼Œæ–¹ä¾¿è°ƒç”¨æ–¹è°ƒè¯•
        assistant_text = _content_to_text(getattr(assistant_message, "content", None))

        # å°è¯•è¯»å–å·¥å…·è°ƒç”¨
        tool_calls = getattr(assistant_message, "tool_calls", None)

        # é’ˆå¯¹ GLMï¼šå¦‚æœ tool_calls ä¸ºç©ºï¼Œå°è¯•ä»æ–‡æœ¬ä¸­è§£æ
        if not tool_calls and provider in ("zhipuai", "glm"):
            print(f"[simple_test_refine_query] GLM æ–‡æœ¬æ ¼å¼è§£æï¼šprovider={provider}, å†…å®¹é•¿åº¦={len(assistant_text)}")
            if "Action:" in assistant_text:
                print(f"[simple_test_refine_query] æ£€æµ‹åˆ° Action: æ¨¡å¼ï¼Œå¼€å§‹è§£æ...")
            parsed = _parse_glm_text_tool_calls(assistant_text, provider)
            if parsed:
                print(f"[simple_test_refine_query] âœ… æˆåŠŸè§£æ GLM æ–‡æœ¬æ ¼å¼å·¥å…·è°ƒç”¨ï¼Œæ•°é‡: {len(parsed)}")
                tool_calls = parsed
            else:
                print(f"[simple_test_refine_query] âš ï¸ GLM æ–‡æœ¬æ ¼å¼è§£æå¤±è´¥ï¼Œå†…å®¹é¢„è§ˆ: {assistant_text[:300]}")

        # è°ƒè¯•è¾“å‡ºï¼šæœ¬è½®å·¥å…·è°ƒç”¨æ•°é‡
        if tool_calls:
            print(f"[simple_test_refine_query] ç¬¬ {round_count} è½®è¿”å›å·¥å…·è°ƒç”¨æ•°: {len(tool_calls)}")
        else:
            print(f"[simple_test_refine_query] ç¬¬ {round_count} è½®æœªè¿”å›å·¥å…·è°ƒç”¨ï¼Œç»“æŸå¯¹è¯ã€‚")

        # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè§†ä¸ºå¯¹è¯ç»“æŸ
        if not tool_calls:
            final_answer_text = assistant_text
            break

        # å½“å‰å®ç°ï¼šé¡ºåºæ‰§è¡Œæ¯ä¸€ä¸ªå·¥å…·è°ƒç”¨
        for tool_call in tool_calls:
            raw_arguments = getattr(tool_call.function, "arguments", "{}")
            try:
                arguments = json.loads(raw_arguments)
            except Exception:
                arguments = {}

            tool_name = getattr(tool_call.function, "name", None)
            if not tool_name or tool_name not in (tool_registry or {}):
                # æœªçŸ¥å·¥å…·ï¼šç›´æ¥æŠŠé”™è¯¯åé¦ˆç»™æ¨¡å‹
                tool_msg = {
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "content": json.dumps(
                        {"error": f"æœªçŸ¥å·¥å…·: {tool_name}"},
                        ensure_ascii=False,
                    ),
                }
                messages.append(tool_msg)
                continue

            # è°ƒç”¨ç¯å¢ƒä¸­çš„å·¥å…·
            try:
                from gym.core.tool_loader import run_tool_call

                tool_result = run_tool_call(env, tool_name, arguments, tool_call.id)
                result = tool_result.get("result")
                print("tool execute result", result)
            except Exception as e:
                tool_result = {"status": "error", "error": str(e)}
                result = tool_result

            # å°è¯•å¤„ç†ã€Œå·¥å…·è¿”å›è·¯å¾„ â†’ å›¾ç‰‡ / æ–‡ä»¶åµŒå…¥ã€çš„æƒ…å†µ
            try:
                if isinstance(result, dict) and "filename" in result:
                    fname = str(result["filename"])
                    resolved = _resolve_existing_path(fname)
                    if resolved:
                        import base64

                        with open(resolved, "rb") as f:
                            b64 = base64.b64encode(f.read()).decode("ascii")
                        result["_embedded_file_base64"] = b64
                        result["_embedded_file_name"] = resolved.name
                        result["_generated_file_path"] = str(resolved)
                        _refresh_preview_image(str(resolved))
                elif isinstance(result, str):
                    candidate_path = result.strip().strip('"').strip("'")
                    if _is_supported_image_path(candidate_path):
                        resolved_candidate = _resolve_existing_path(candidate_path)
                        if resolved_candidate and resolved_candidate.exists():
                            import base64

                            with open(resolved_candidate, "rb") as f:
                                b64 = base64.b64encode(f.read()).decode("ascii")
                            result = {
                                "original_result": candidate_path,
                                "_embedded_file_base64": b64,
                                "_embedded_file_name": resolved_candidate.name,
                                "_generated_file_path": str(resolved_candidate),
                            }
                            _refresh_preview_image(str(resolved_candidate))
            except Exception:
                # å›¾ç‰‡å¤„ç†å¤±è´¥ä¸åº”å½±å“ä¸»æµç¨‹
                pass

            tool_content = json.dumps(result, ensure_ascii=False, default=str)

            tool_message = {
                "role": "tool",
                "tool_call_id": tool_call.id,
                "content": tool_content,
            }
            messages.append(tool_message)

    if round_count >= max_rounds and not final_answer_text:
        final_answer_text = assistant_text + "\n[å¯¹è¯è½®æ•°è¾¾åˆ°ä¸Šé™ï¼Œè‡ªåŠ¨ç»ˆæ­¢]"

    # æå–ç»“æ„åŒ–ç­”æ¡ˆå¹¶è¿›è¡Œè¯„åˆ†
    structured_answer, score, score_summary, score_details = _evaluate_refined_answer(
        final_answer_text, query_data
    )

    return {
        "final_answer": final_answer_text,
        "structured_answer": structured_answer,
        "score": score,
        "score_summary": score_summary,
        "score_details": score_details,
        "messages": messages,
        "model_name": current_model,
        "used_tools": True,
        "rounds": round_count,
    }


def _evaluate_refined_answer(
    final_answer_text: str,
    query_data: Dict[str, Any],
) -> Tuple[Optional[Dict[str, Any]], float, str, Dict[str, Any]]:
    """
    è¯„ä¼°ç²¾ç‚¼ç‰ˆç­”æ¡ˆï¼šæå–ç»“æ„åŒ–ç­”æ¡ˆå¹¶ä½¿ç”¨ calculate_answer_score è¿›è¡Œè¯„åˆ†ã€‚
    
    Args:
        final_answer_text: æ¨¡å‹çš„æœ€ç»ˆå›ç­”æ–‡æœ¬
        query_data: æµ‹è¯•æ¡ˆä¾‹æ•°æ®ï¼ŒåŒ…å« metadata.golden_answer
    
    Returns:
        Tuple: (structured_answer, score, score_summary, score_details)
        - structured_answer: æå–çš„ç»“æ„åŒ–ç­”æ¡ˆï¼ˆå­—å…¸ï¼‰ï¼Œå¦‚æœæå–å¤±è´¥åˆ™ä¸º None
        - score: è¯„åˆ†ï¼ˆ0.0-1.0ï¼‰
        - score_summary: è¯„åˆ†æ‘˜è¦ï¼ˆå­—ç¬¦ä¸²ï¼‰
        - score_details: è¯„åˆ†è¯¦ç»†ä¿¡æ¯ï¼ˆå­—å…¸ï¼‰
    """
    from gym.core.data_loader import extract_structured_answer_from_response
    from gym.core.evaluator import calculate_answer_score

    # æå–ç»“æ„åŒ–ç­”æ¡ˆ
    structured_answer = extract_structured_answer_from_response(final_answer_text)
    
    if structured_answer is None:
        print("âš ï¸ æ— æ³•ä»æ¨¡å‹å›ç­”ä¸­æå–ç»“æ„åŒ–ç­”æ¡ˆï¼Œè¯„åˆ†å°†å¤±è´¥")
        return None, 0.0, "æ— æ³•æå–ç»“æ„åŒ–ç­”æ¡ˆ", {}

    # è·å–æ ‡å‡†ç­”æ¡ˆï¼ˆgolden_answerï¼‰
    metadata = query_data.get("metadata", {})
    golden_answer_list = metadata.get("golden_answer", [])
    
    if not golden_answer_list:
        print("âš ï¸ æœªæ‰¾åˆ°æ ‡å‡†ç­”æ¡ˆï¼ˆgolden_answerï¼‰ï¼Œæ— æ³•è¿›è¡Œè¯„åˆ†")
        return structured_answer, 0.0, "æœªæ‰¾åˆ°æ ‡å‡†ç­”æ¡ˆ", {}

    # golden_answer æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
    golden_answer = golden_answer_list[0] if isinstance(golden_answer_list, list) else golden_answer_list
    
    # å¦‚æœ golden_answer æ˜¯å­—å…¸ä¸”åŒ…å« 'final_answer' é”®ï¼Œæå–å®ƒ
    if isinstance(golden_answer, dict) and "final_answer" in golden_answer:
        golden_answer = golden_answer["final_answer"]
    
    # ç¡®ä¿ golden_answer æ˜¯å­—å…¸æ ¼å¼ï¼ˆcalculate_answer_score éœ€è¦ï¼‰
    if not isinstance(golden_answer, dict):
        # å°è¯•å°†å­—ç¬¦ä¸²æˆ–å…¶ä»–æ ¼å¼è½¬æ¢ä¸ºå­—å…¸
        try:
            if isinstance(golden_answer, str):
                golden_answer = json.loads(golden_answer)
            else:
                # åŒ…è£…ä¸ºå­—å…¸
                golden_answer = {"answer": golden_answer}
        except Exception:
            golden_answer = {"answer": golden_answer}

    # ç¡®ä¿ structured_answer ä¹Ÿæ˜¯å­—å…¸æ ¼å¼
    if not isinstance(structured_answer, dict):
        structured_answer = {"answer": structured_answer}

    # ä½¿ç”¨ calculate_answer_score è¿›è¡Œè¯„åˆ†
    try:
        score, score_summary, score_details = calculate_answer_score(
            model_answer=structured_answer,
            golden_standard=golden_answer,
            tolerance=0.05,
        )
        print(f"âœ… è¯„åˆ†å®Œæˆ: {score:.2f} - {score_summary}")
        return structured_answer, score, score_summary, score_details
    except Exception as e:
        print(f"âŒ è¯„åˆ†è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return structured_answer, 0.0, f"è¯„åˆ†å‡ºé”™: {e}", {}


def _load_tools_and_build_env_for_case(
    query_data: Dict[str, Any],
    use_tools: bool,
    load_all_topic_tools: bool,
    test_type: str,
    auto_infer_from_metadata: bool = True,
) -> Tuple[
    Dict[str, Any],
    Optional["MinimalSciEnv"],
    Optional[List[Dict[str, Any]]],
    Optional[Dict[str, Any]],
    Optional[List[Dict[str, Any]]],
    Optional[Dict[str, Any]],
]:
    """
    ç»Ÿä¸€å°è£…ã€Œæ ¹æ®æ¡ˆä¾‹åŠ è½½å·¥å…·å¹¶æ„å»ºç¯å¢ƒã€çš„é€»è¾‘ã€‚

    èŒè´£ï¼š
    - æ ¹æ® load_all_topic_tools / metadata å†³å®šæ˜¯å¦åš topic çº§å·¥å…·èšåˆ
    - å¦‚æœ auto_infer_from_metadata=Trueï¼Œæ ¹æ® metadata ä¸­çš„ subject/topic è‡ªåŠ¨æ¨æ–­å·¥å…·ç›®å½•
    - è°ƒç”¨ gym.tool_loader.load_tools_for_case / register_tools_to_env
    - è¿”å›æ›´æ–°åçš„ query_dataï¼ˆå¯èƒ½åŒ…å«åˆå¹¶åçš„ usage_tool_protocolï¼‰å’Œï¼š
        env, tools_schema, tool_registry, tool_protocols, function_map

    å½“ use_tools=False æ—¶ï¼š
    - ä¸åŠ è½½ä»»ä½•å·¥å…·ï¼Œä»…è¿”å›åŸå§‹ query_dataï¼Œå…¶ä½™è¿”å› Noneã€‚
    
    Args:
        query_data: æµ‹è¯•æ¡ˆä¾‹æ•°æ®
        use_tools: æ˜¯å¦ä½¿ç”¨å·¥å…·
        load_all_topic_tools: æ˜¯å¦åŠ è½½ç›¸åŒ topic çš„æ‰€æœ‰å·¥å…·
        test_type: æµ‹è¯•ç±»å‹
        auto_infer_from_metadata: æ˜¯å¦æ ¹æ® metadata ä¸­çš„ subject/topic è‡ªåŠ¨æ¨æ–­å¹¶åŠ è½½å·¥å…·ç›®å½•
                                   é»˜è®¤ä¸º Trueï¼Œä¼šè‡ªåŠ¨åŠ è½½ toolkits/{subject}/{topic}/ ä¸‹çš„æ‰€æœ‰å·¥å…·
    """
    if not use_tools:
        print("ğŸ“ çº¯æ–‡æœ¬æ¨¡å¼ï¼Œä¸ä½¿ç”¨ä»»ä½•å·¥å…·")
        return query_data, None, None, None, None, None

    metadata = query_data.get("metadata") or {}
    if not isinstance(metadata, dict):
        metadata = {}
    query_data["metadata"] = metadata

    # æ£€æŸ¥æ˜¯å¦éœ€è¦åŠ è½½ç›¸åŒ topic çš„æ‰€æœ‰å·¥å…·
    should_load_all_topic_tools = load_all_topic_tools or _coerce_truthy_flag(
        metadata.get("load_all_topic_tools")
    )

    if should_load_all_topic_tools:
        current_topic = metadata.get("topic") or (query_data.get("metadata_summary") or {}).get(
            "topic"
        )

        if current_topic:
            print(f"ğŸ”§ å¯ç”¨å…¨topicå·¥å…·åŠ è½½æ¨¡å¼ï¼Œtopic: {current_topic}")

            # æ ¹æ® test_type é€‰æ‹©åˆé€‚çš„æ•°æ®åŠ è½½å‡½æ•°
            all_cases: List[Dict[str, Any]] = []
            try:
                if test_type == "augmented":
                    all_cases = load_augmented_test_cases_from_dataset()
                elif test_type == "refined":
                    dataset_key = metadata.get("dataset_key")
                    all_cases = load_refined_test_cases_from_dataset(dataset_key=dataset_key)
                else:
                    all_cases = load_test_cases_from_dataset()

                topic_map = group_cases_by_topic(all_cases)
                topic_cases = topic_map.get(current_topic, [])

                if topic_cases:
                    print(f"ğŸ“š æ‰¾åˆ° {len(topic_cases)} ä¸ªç›¸åŒtopicçš„æ¡ˆä¾‹")
                    aggregated_protocol = aggregate_usage_tool_protocol_for_cases(topic_cases)

                    if aggregated_protocol:
                        print(f"ğŸ“¦ èšåˆå¾—åˆ° {len(aggregated_protocol)} ä¸ªå·¥å…·åè®®")
                        current_protocols = query_data.get("usage_tool_protocol", []) or []
                        merged_protocols = list(current_protocols) + list(aggregated_protocol)

                        temp_case = deepcopy(query_data)
                        temp_case["usage_tool_protocol"] = merged_protocols
                        temp_metadata = temp_case.get("metadata") or {}
                        temp_metadata["topic_protocol_scope"] = current_topic
                        temp_metadata["with_all_tools"] = True
                        temp_case["metadata"] = temp_metadata
                        query_data = temp_case

                        print(f"âœ… å·²åˆå¹¶å·¥å…·åè®®ï¼Œæ€»è®¡ {len(merged_protocols)} ä¸ªå·¥å…·åè®®")
                    else:
                        print(
                            f"âš ï¸ topic {current_topic} æœªæ‰¾åˆ°ä»»ä½•å·¥å…·åè®®ï¼Œ"
                            f"ä½¿ç”¨å½“å‰æ¡ˆä¾‹çš„å·¥å…·"
                        )
                else:
                    print(f"âš ï¸ æœªæ‰¾åˆ°topic {current_topic} çš„å…¶ä»–æ¡ˆä¾‹ï¼Œä½¿ç”¨å½“å‰æ¡ˆä¾‹çš„å·¥å…·")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½å…¨topicå·¥å…·å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨å½“å‰æ¡ˆä¾‹çš„å·¥å…·")
                import traceback
                traceback.print_exc()
        else:
            print("âš ï¸ å½“å‰æ¡ˆä¾‹æ²¡æœ‰topicä¿¡æ¯ï¼Œæ— æ³•åŠ è½½å…¨topicå·¥å…·ï¼Œä½¿ç”¨å½“å‰æ¡ˆä¾‹çš„å·¥å…·")

    # æ­¥éª¤1: åŠ è½½å·¥å…·åè®®å’Œå‡½æ•°ï¼ˆä» usage_tool_protocol å­—æ®µï¼‰
    tool_protocols, function_map = load_tools_for_case(query_data)

    # æ­¥éª¤2: æ³¨å†Œåˆ°ç¯å¢ƒï¼ŒåŒæ—¶æ”¯æŒè‡ªåŠ¨ä» metadata æ¨æ–­å·¥å…·ç›®å½•
    env, tool_instances, tools_schema, tool_registry = register_tools_to_env(
        tool_protocols,
        function_map,
        query_data=query_data,
        auto_infer_from_metadata=auto_infer_from_metadata,
    )
    tools = tools_schema

    tool_count = len(tools_schema) if tools_schema else 0
    print(
        f"âœ… åŠ è½½äº† {tool_count} ä¸ªå·¥å…·åè®®ï¼Œ{len(function_map)} ä¸ªå‡½æ•°ï¼Œ"
        f"æ„å»ºç¯å¢ƒä¸­çš„å·¥å…·æ•°: {len(tool_registry)}"
    )

    if tool_count > 150:
        print(
            f"âš ï¸  è­¦å‘Šï¼šå·¥å…·æ•°é‡ ({tool_count}) è¾ƒå¤šï¼Œå¯èƒ½å¯¼è‡´è¾“å…¥è¶…å‡ºæ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶"
        )
        print(f"   å¦‚æœé‡åˆ° 'Input is too long' é”™è¯¯ï¼Œè¯·è€ƒè™‘ï¼š")
        print(f"   1. å…³é—­ load_all_topic_tools é€‰é¡¹ï¼ˆä»…ä½¿ç”¨å½“å‰æ¡ˆä¾‹çš„å·¥å…·ï¼‰")
        print(f"   2. ä½¿ç”¨æ”¯æŒæ›´å¤§ä¸Šä¸‹æ–‡çš„æ¨¡å‹")
        print(f"   3. æ£€æŸ¥æ˜¯å¦çœŸçš„éœ€è¦åŠ è½½è¿™ä¹ˆå¤šå·¥å…·")
    elif tool_count > 100:
        print(
            f"â„¹ï¸  æç¤ºï¼šå·¥å…·æ•°é‡ ({tool_count}) è¾ƒå¤šï¼Œ"
            f"å¦‚æœé‡åˆ°è¾“å…¥é•¿åº¦é—®é¢˜ï¼Œå¯è€ƒè™‘å…³é—­ load_all_topic_tools"
        )

    return query_data, env, tools, tool_registry, tool_protocols, function_map


def test_query(
    query_data,
    model_name=None,
    use_tools=True,
    trace_tag: Optional[str] = None,
    mode_name: Optional[str] = None,
    force_retest: bool = False,
    load_all_topic_tools: bool = False,
    auto_infer_from_metadata: bool = True,
):
    """æµ‹è¯•å•ä¸ªæŸ¥è¯¢ - æ”¯æŒé€‰æ‹©æ˜¯å¦ä½¿ç”¨å·¥å…·

    Args:
        query_data: æµ‹è¯•æ¡ˆä¾‹æ•°æ®
        model_name: æŒ‡å®šçš„æ¨¡å‹åç§°
        use_tools: æ˜¯å¦ä½¿ç”¨å·¥å…·ï¼ŒTrueè¡¨ç¤ºä½¿ç”¨å·¥å…·ï¼ŒFalseè¡¨ç¤ºçº¯æ–‡æœ¬å¯¹è¯
        trace_tag: è½¨è¿¹å­ç›®å½•æ ‡ç­¾
        mode_name: æŒ‡å®šæ¨¡å¼ç›®å½•åç§°
        force_retest: æ˜¯å¦å¿½ç•¥ç°æœ‰ç¼“å­˜å¹¶å¼ºåˆ¶é‡æ–°è°ƒç”¨æ¨¡å‹
        load_all_topic_tools: æ˜¯å¦åŠ è½½ç›¸åŒtopicçš„æ‰€æœ‰å·¥å…·ï¼ˆé»˜è®¤Falseï¼‰
        auto_infer_from_metadata: æ˜¯å¦æ ¹æ® metadata ä¸­çš„ subject/topic è‡ªåŠ¨æ¨æ–­å¹¶åŠ è½½å·¥å…·ç›®å½•
                                   é»˜è®¤ä¸º Trueï¼Œä¼šè‡ªåŠ¨åŠ è½½ toolkits/{subject}/{topic}/ ä¸‹çš„æ‰€æœ‰å·¥å…·
    """ 
    ## æ³¨å†ŒAgent
    test_client = get_client(model_name) if model_name else get_client(DEFAULT_MODEL)
    current_model = model_name or DEFAULT_MODEL

    metadata = query_data.get('metadata') or {}
    if not isinstance(metadata, dict):
        metadata = {}
    query_data['metadata'] = metadata

    effective_mode_name = mode_name
    if effective_mode_name is None:
        meta_override = metadata.get('mode_name') or metadata.get('mode_folder')
        if meta_override:
            effective_mode_name = str(meta_override)
        elif metadata.get('with_all_tools'):
            effective_mode_name = "with_all_tools"
    ## Agentçš„è°ƒç”¨èŒƒå¼
    use_react_prompt = _should_use_react_prompt(use_tools, effective_mode_name, metadata)
    if use_react_prompt and effective_mode_name in (None, "with_tools"):
        suffix = (TOOL_TRACE_SUFFIX or "").strip() or "_react"
        effective_mode_name = f"with_tools{suffix}"

    mode_desc = effective_mode_name or ("ä½¿ç”¨å·¥å…·" if use_tools else "ä¸ä½¿ç”¨å·¥å…·")
    test_type = metadata.get('test_type', 'normal')

    case_id = (
        query_data.get('id')
        or metadata.get('id')
        or metadata.get('case_id')
        or metadata.get('question_id')
    )
    trace_path = None
    sanitized_tag = None
    if case_id is None:
        print("âš ï¸ å½“å‰æ¡ˆä¾‹ç¼ºå°‘å¯è¯†åˆ«çš„IDï¼Œæ— æ³•è¿›è¡Œç¼“å­˜æ£€æŸ¥ã€‚")
    else:
        # ä» metadata ä¸­è·å–æ•°æ®é›†æ–‡ä»¶åï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        dataset_filename = metadata.get('_dataset_filename') if isinstance(metadata, dict) else None
        
        trace_path, sanitized_tag = _derive_trace_path(
            current_model,
            use_tools if effective_mode_name is None else True,
            case_id,
            trace_tag,
            mode_name=effective_mode_name,
            metadata=metadata,
            dataset_filename=dataset_filename,
        )
        cache_available = trace_path.exists()
        metadata_force_flag = (
            _coerce_truthy_flag(metadata.get("force_retest")) or
            _coerce_truthy_flag(metadata.get("force_reload"))
        )
        force_retest = bool(force_retest or metadata_force_flag)

    ## å¼€å§‹æµ‹è¯• 
    print(f"\nStart:*** å¼€å§‹æµ‹è¯•ï¼š\n æµ‹è¯•é¢˜ç›®: {query_data['id']} \n(æ¨¡å‹: {current_model}, \n æ¨¡å¼: {mode_desc}, \n ç±»å‹: {test_type}) ===")

    # æ ¹æ®æµ‹è¯•ç±»å‹é€‰æ‹©åˆé€‚çš„æ¨¡æ¿æå–å‡½æ•°
    if test_type == 'augmented':
        answer_template, golden_standard = extract_augmented_answer_template(query_data)
        print(f"ğŸ“‹ å¢å¼ºç‰ˆæµ‹è¯•æ¡ˆä¾‹ï¼ŒåŸå§‹ID: {query_data.get('original_id')}")
    else:
        answer_template, golden_standard = extract_golden_answer_template(query_data)
    
    if answer_template:
        print(f"ğŸ“‹ è§„èŒƒå›ç­”æ¨¡æ¿: {json.dumps(answer_template, ensure_ascii=False, indent=2)}")
        print(f"ğŸ¯ æ ‡å‡†ç­”æ¡ˆç»“æ„: {json.dumps(golden_standard, ensure_ascii=False, indent=2)}")

    # ä½¿ç”¨ç»Ÿä¸€çš„å·¥å…·åŠ è½½å‡½æ•°ï¼ˆæ”¯æŒè‡ªåŠ¨ä» metadata æ¨æ–­å·¥å…·ç›®å½•ï¼‰
    query_data, env, tools, tool_registry, tool_protocols, function_map = _load_tools_and_build_env_for_case(
        query_data,
        use_tools,
        load_all_topic_tools,
        test_type,
        auto_infer_from_metadata=auto_infer_from_metadata,
    )

    # å¤„ç†å¯èƒ½åŒ…å«å›¾ç‰‡çš„é—®é¢˜ï¼ˆç»Ÿä¸€é€šè¿‡ç‹¬ç«‹æ¨¡å—æ„é€ åŸºç¡€ç”¨æˆ·æ¶ˆæ¯ï¼‰
    question_data, user_message = _build_basic_user_message(query_data, test_type)

    # æ ¡éªŒå›¾ç‰‡åŠ è½½æƒ…å†µï¼šå¦‚æœé¢˜ç›®éœ€è¦å›¾ç‰‡ä½†å­˜åœ¨æœªåŠ è½½çš„å›¾ç‰‡ï¼Œç›´æ¥ä¸­æ­¢
    expected_images = question_data.get('expected_image_count') or 0
    missing_images = question_data.get('missing_images') or []
    if expected_images > 0 and missing_images:
        reason = "å›¾ç‰‡åŠ è½½å¤±è´¥"
        extra = {
            "expected_images": expected_images,
            "missing_images": missing_images,
        }
        _record_skip_event(trace_path, case_id, current_model, mode_desc, reason, extra)
        details = {"case_id": case_id}
        details.update(extra)
        raise TestSkipException(reason, details)
    
    # åˆ›å»ºæ¶ˆæ¯ - åœ¨åŸºç¡€ç”¨æˆ·é—®é¢˜ä¸Šè¿½åŠ æ ¼å¼è¦æ±‚
    user_content_base = question_data['text']  # ä½¿ç”¨æ¸…ç†åçš„æ–‡æœ¬

    # å¦‚æœä½¿ç”¨å·¥å…·ï¼Œç§»é™¤åŸé—®é¢˜ä¸­çš„æ—§æ ¼å¼è¦æ±‚éƒ¨åˆ†ï¼Œä»…ä¿ç•™ Answer æ¨¡æ¿
    if use_tools:
        # æŸ¥æ‰¾æ ¼å¼è¦æ±‚çš„èµ·å§‹ä½ç½®
        format_start = user_content_base.find("You should strictly respond in this exact format")
        if format_start != -1:
            # æå–æ ¼å¼è¦æ±‚ä¹‹å‰çš„é—®é¢˜éƒ¨åˆ†
            question_part = user_content_base[:format_start].strip()

            # æ„å»ºç®€åŒ–çš„æ ¼å¼è¦æ±‚ï¼ˆåªä¿ç•™Answeréƒ¨åˆ†ï¼‰
            simplified_format = _get_tool_mode_answer_prompt()

            # ç»„åˆæ–°çš„ç”¨æˆ·å†…å®¹
            user_content = question_part + "\n" + simplified_format

            print("âœ‚ï¸ å·²ç§»é™¤æ¨ç†è¿‡ç¨‹æ ¼å¼è¦æ±‚")
        else:
            # æ„å»ºç®€åŒ–çš„æ ¼å¼è¦æ±‚ï¼ˆåªä¿ç•™Answeréƒ¨åˆ†ï¼‰
            simplified_format = _get_tool_mode_answer_prompt()
            user_content = user_content_base + "\n" + simplified_format
    else:
        # çº¯æ–‡æœ¬æ¨¡å¼ï¼šä¿ç•™æ¨ç†è¿‡ç¨‹ + ç­”æ¡ˆ
        simplified_format = _get_text_mode_reasoning_answer_prompt()
        user_content = user_content_base + "\n" + simplified_format

    print(f"ğŸ“ {'å·¥å…·æ¨¡å¼' if use_tools else 'æ–‡æœ¬æ¨¡å¼'}é—®é¢˜å†…å®¹:")
    print(f"åŸå§‹é—®é¢˜é•¿åº¦: {len(user_content_base)}")
    print(f"å¤„ç†åé•¿åº¦: {len(user_content)}")

    # æ£€æŸ¥ user_content æ˜¯å¦ä¸ºç©º
    if not user_content or not user_content.strip():
        print("âŒ é”™è¯¯ï¼šç”¨æˆ·å†…å®¹ä¸ºç©ºï¼Œæ— æ³•æ„é€ æ¶ˆæ¯")
        return None

    # åœ¨åŸºç¡€æ¶ˆæ¯ä¸Šæ›´æ–°æ–‡æœ¬éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯é‡æ–°æ„é€ æ•´ä¸ªå¤šæ¨¡æ€ç»“æ„
    if isinstance(user_message.get("content"), list):
        # å¤šæ¨¡æ€ï¼šæ‰¾åˆ°æœ€åä¸€ä¸ª text ç±»å‹ç‰‡æ®µå¹¶æ›¿æ¢ä¸ºå¸¦æ ¼å¼çš„æ–‡æœ¬
        replaced = False
        for part in reversed(user_message["content"]):
            if isinstance(part, dict) and part.get("type") == "text":
                part["text"] = user_content
                replaced = True
                break
        if not replaced:
            user_message["content"].append({"type": "text", "text": user_content})
    else:
        # çº¯æ–‡æœ¬ï¼šç›´æ¥æ›¿æ¢ä¸ºå¸¦æ ¼å¼çš„æ–‡æœ¬
        user_message["content"] = user_content

    messages = []
    if use_tools and use_react_prompt:
        messages.append({"role": "system", "content": _get_react_tool_system_prompt()})
    messages.append(user_message)
    print(user_content) 

    # å¦‚æœä¸ä½¿ç”¨å·¥å…·ï¼Œç›´æ¥è¿›è¡Œå•è½®å¯¹è¯
    if not use_tools:
        print(f"\n--- çº¯æ–‡æœ¬æ¨¡å¼å¯¹è¯ ---")

        # è°ƒç”¨æ¨¡å‹ï¼ˆä¸ä¼ å…¥toolså‚æ•°ï¼‰
        response = test_client.chat_completions_create(
            messages=messages
        )

        assistant_message = _extract_choice_message(response, "çº¯æ–‡æœ¬å¯¹è¯")
        # ç¡®ä¿è·å–å®Œæ•´çš„å†…å®¹ï¼Œé¿å…æˆªæ–­
        final_content = getattr(assistant_message, 'content', None)
        if final_content is None:
            final_content = "æ¨¡å‹å›ç­”ä¸ºç©º"
        else:
            final_content = _content_to_text(final_content)

        print(f"åŠ©æ‰‹å›å¤: {final_content}")
        print(f"\n=== çº¯æ–‡æœ¬å¯¹è¯ç»“æŸ ===")

        # å°†ç®€å•çš„æ¶ˆæ¯è½¨è¿¹ä¿å­˜
        messages.append({"role": "assistant", "content": final_content})
        round_count = 1

    else:
        # åŸæœ‰çš„å·¥å…·è°ƒç”¨é€»è¾‘
        # å¾ªç¯è°ƒç”¨æœºåˆ¶ï¼šæ¯æ¬¡åªå¤„ç†ä¸€ä¸ªå·¥å…·è°ƒç”¨
        round_count = 0
        max_rounds = 50  # é˜²æ­¢æ— é™å¾ªç¯

        while round_count < max_rounds:
            round_count += 1
            print(f"\n--- ç¬¬ {round_count} è½®APIè°ƒç”¨ ---")

            # ç¬¬ä¸€è½®å¼ºåˆ¶ä½¿ç”¨å·¥å…·ï¼Œåç»­è½®æ¬¡è®©æ¨¡å‹è‡ªç”±é€‰æ‹©
            tool_choice = "auto"

            # è°ƒç”¨æ¨¡å‹
            response = test_client.chat_completions_create(
                messages=messages,
                tools=tools,
                tool_choice=tool_choice,
                parallel_tool_calls=False  # å…³é—­å¹¶è¡Œå·¥å…·è°ƒç”¨ï¼Œç¡®ä¿ä¸€æ¬¡åªè¿”å›ä¸€ä¸ªå·¥å…·è°ƒç”¨
            )

            assistant_message = _extract_choice_message(response, f"ç¬¬ {round_count} è½® API è°ƒç”¨")
            
            # ç¡®ä¿åŠ©æ‰‹æ¶ˆæ¯å†…å®¹ä¸ä¸ºç©ºï¼Œè¿™å¯¹äºè½¨è¿¹ä¿å­˜å¾ˆé‡è¦
            # ä½†æ˜¯è¦ä¿ç•™åŸå§‹å†…å®¹ï¼Œé¿å…æˆªæ–­çœŸå®çš„æ¨¡å‹å›ç­”
            original_content = getattr(assistant_message, 'content', None)
            if not original_content:
                # åªæœ‰åœ¨çœŸæ­£ä¸ºç©ºæ—¶æ‰è®¾ç½®é»˜è®¤å†…å®¹
                if hasattr(assistant_message, 'tool_calls') and assistant_message.tool_calls:
                    assistant_message.content = "æˆ‘å°†ä½¿ç”¨å·¥å…·æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚"
                else:
                    assistant_message.content = "æ²¡æœ‰å›å¤"
            else:
                # ä¿ç•™åŸå§‹å¤šæ¨¡æ€ç»“æ„ï¼Œä½†æ—¥å¿—æ‰“å°æ—¶è½¬ä¸ºæ–‡æœ¬
                assistant_message.content = original_content
            
            messages.append(assistant_message)

            # è°ƒè¯•ä¿¡æ¯
            print(f"åŠ©æ‰‹å›å¤: {_content_to_text(assistant_message.content)}")

            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            tool_calls = getattr(assistant_message, 'tool_calls', None)
            
            # å¯¹äº GLM æ¨¡å‹ï¼Œå¦‚æœä½¿ç”¨ ReAct prompt ä½†è¿”å›çš„æ˜¯æ–‡æœ¬æ ¼å¼çš„å·¥å…·è°ƒç”¨ï¼Œéœ€è¦è§£æ
            # å³ä½¿ tool_calls ä¸º Noneï¼Œä¹Ÿè¦æ£€æŸ¥æ˜¯å¦æ˜¯ GLM çš„æ–‡æœ¬æ ¼å¼
            if not tool_calls and use_tools:
                content_text = _content_to_text(assistant_message.content)
                # è·å– providerï¼ˆä» test_client æˆ– current_model é…ç½®ä¸­ï¼‰
                provider = _detect_provider(current_model)
                
                # æ£€æŸ¥å†…å®¹ä¸­æ˜¯å¦åŒ…å« Action: æ¨¡å¼ï¼ˆGLM æ–‡æœ¬æ ¼å¼å·¥å…·è°ƒç”¨çš„ç‰¹å¾ï¼‰
                if provider in ("zhipuai", "glm") or ("Action:" in content_text and "<arg_key>" in content_text):
                    print(f"ğŸ” æ£€æµ‹åˆ°å¯èƒ½çš„ GLM æ–‡æœ¬æ ¼å¼å·¥å…·è°ƒç”¨ (provider: {provider}, use_react_prompt: {use_react_prompt})")
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ GLM çš„æ–‡æœ¬æ ¼å¼å·¥å…·è°ƒç”¨
                    parsed_tool_calls = _parse_glm_text_tool_calls(content_text, provider)
                    if parsed_tool_calls:
                        print(f"âœ… æˆåŠŸè§£æ GLM æ–‡æœ¬æ ¼å¼å·¥å…·è°ƒç”¨ï¼Œå·²è§£æä¸º {len(parsed_tool_calls)} ä¸ªå·¥å…·è°ƒç”¨")
                        # å°†è§£æçš„å·¥å…·è°ƒç”¨è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                        tool_calls = parsed_tool_calls
                    else:
                        print(f"âš ï¸  æœªè§£æåˆ°å·¥å…·è°ƒç”¨ï¼ˆå¯èƒ½æ ¼å¼ä¸åŒ¹é…ï¼‰")
            
            if not tool_calls:
                print("æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œå¯¹è¯ç»“æŸ")
                print(f"æœ€ç»ˆå›å¤: {assistant_message.content}")
                break

            # å¤„ç†æ‰€æœ‰å·¥å…·è°ƒç”¨
            print(f"å·¥å…·è°ƒç”¨æ•°é‡: {len(tool_calls)}")

            for i, tool_call in enumerate(tool_calls):
                print(f"--- æ‰§è¡Œå·¥å…· {i+1}/{len(tool_calls)}: {tool_call.function.name} ---")

                try:
                    # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼Œæ˜¾ç¤ºåŸå§‹å‚æ•°å­—ç¬¦ä¸²
                    raw_arguments = tool_call.function.arguments
                    print(f"åŸå§‹å‚æ•°å­—ç¬¦ä¸²: {repr(raw_arguments)}")
                    
                    # å°è¯•è§£æJSONå‚æ•°
                    arguments = json.loads(raw_arguments)
                    print(f"å‚æ•°: {arguments}")

                    if tool_call.function.name in (tool_registry or {}):
                        try:
                            # é€šè¿‡ MinimalSciEnv + run_tool_call æ‰§è¡Œå·¥å…·
                            tool_result = run_tool_call(env, tool_call.function.name, arguments, tool_call.id)
                            print(f"å·¥å…·æ‰§è¡Œç»“æœ(raw): {tool_result}")
                            result = tool_result.get("result")

                            # ä¿æŒåŸæœ‰çš„å›¾ç‰‡åµŒå…¥ä¸é¢„è§ˆé€»è¾‘
                            try:
                                # æƒ…å†µ1ï¼šè¿”å›ç»“æœä¸­åŒ…å« filename å­—æ®µ
                                if isinstance(result, dict) and 'filename' in result:
                                    fname = str(result['filename'])
                                    resolved = _resolve_existing_path(fname)
                                    if resolved:
                                        with open(resolved, 'rb') as f:
                                            b64 = base64.b64encode(f.read()).decode('ascii')
                                        result['_embedded_file_base64'] = b64
                                        result['_embedded_file_name'] = resolved.name
                                        result['_generated_file_path'] = str(resolved)
                                        _refresh_preview_image(str(resolved))
                                    else:
                                        result['_embedded_file_error'] = f"file not found: {fname}"

                                # æƒ…å†µ2ï¼šæ£€æŸ¥å‚æ•°ä¸­æ˜¯å¦æœ‰ save_pathï¼Œå¹¶ä¸”æ–‡ä»¶ç¡®å®è¢«ç”Ÿæˆäº†
                                elif isinstance(arguments, dict) and 'save_path' in arguments:
                                    save_path = str(arguments['save_path'])
                                    resolved_save_path = _resolve_existing_path(save_path)
                                    if resolved_save_path and resolved_save_path.exists():
                                        with open(resolved_save_path, 'rb') as f:
                                            b64 = base64.b64encode(f.read()).decode('ascii')
                                        if result is None:
                                            result = {}
                                        elif not isinstance(result, dict):
                                            result = {'original_result': result}

                                        result['_embedded_file_base64'] = b64
                                        result['_embedded_file_name'] = resolved_save_path.name
                                        result['_generated_file_path'] = str(resolved_save_path)
                                        print(f"æ£€æµ‹åˆ°ç”Ÿæˆçš„æ–‡ä»¶: {resolved_save_path}")
                                        _refresh_preview_image(str(resolved_save_path))
                                    else:
                                        if result is None:
                                            result = {}
                                        elif not isinstance(result, dict):
                                            result = {'original_result': result}
                                        result['_embedded_file_error'] = f"expected file not found: {save_path}"
                                elif isinstance(result, str):
                                    candidate_path = result.strip().strip('"').strip("'")
                                    if _is_supported_image_path(candidate_path):
                                        resolved_candidate = _resolve_existing_path(candidate_path)
                                        if resolved_candidate and resolved_candidate.exists():
                                            with open(resolved_candidate, 'rb') as f:
                                                b64 = base64.b64encode(f.read()).decode('ascii')
                                            result = {
                                                'original_result': candidate_path,
                                                '_embedded_file_base64': b64,
                                                '_embedded_file_name': resolved_candidate.name,
                                                '_generated_file_path': str(resolved_candidate),
                                            }
                                            print(f"æ£€æµ‹åˆ°å·¥å…·è¿”å›çš„å›¾ç‰‡è·¯å¾„: {resolved_candidate}")
                                            _refresh_preview_image(str(resolved_candidate))
                                        else:
                                            result = {
                                                'original_result': candidate_path,
                                                '_embedded_file_error': f"file not found: {candidate_path}"
                                            }

                            except Exception as e_file:
                                if result is None:
                                    result = {}
                                elif not isinstance(result, dict):
                                    result = {'original_result': result}
                                result['_embedded_file_error'] = str(e_file)

                            # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡æ•°æ®ï¼Œå¦‚æœæœ‰åˆ™æ„é€ å¤šæ¨¡æ€æ¶ˆæ¯
                            if isinstance(result, dict) and '_embedded_file_base64' in result:
                                content_parts = []
                                
                                text_result = {k: v for k, v in result.items() 
                                             if not k.startswith('_embedded_file_')}
                                text_content = json.dumps(text_result, default=str, ensure_ascii=False)
                                
                                if text_content and text_content.strip() not in ['{}', 'null']:
                                    content_parts.append({
                                        "type": "text",
                                        "text": f"å·¥å…·æ‰§è¡Œç»“æœ: {text_content}"
                                    })
                                else:
                                    content_parts.append({
                                        "type": "text", 
                                        "text": "å·¥å…·æ‰§è¡Œå®Œæˆï¼Œç”Ÿæˆäº†ä»¥ä¸‹å›¾ç‰‡ï¼š"
                                    })
                                
                                file_name = result.get('_embedded_file_name', '')
                                if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.webp')):
                                    if file_name.lower().endswith('.png'):
                                        mime_type = 'image/png'
                                    elif file_name.lower().endswith(('.jpg', '.jpeg')):
                                        mime_type = 'image/jpeg'
                                    elif file_name.lower().endswith('.gif'):
                                        mime_type = 'image/gif'
                                    elif file_name.lower().endswith('.bmp'):
                                        mime_type = 'image/bmp'
                                    elif file_name.lower().endswith('.webp'):
                                        mime_type = 'image/webp'
                                    else:
                                        mime_type = 'image/png'
                                else:
                                    mime_type = 'image/png'
                                
                                print(f"ğŸ–¼ï¸ å·¥å…·ç”Ÿæˆå›¾ç‰‡: {file_name} (ç±»å‹: {mime_type})")
                                if hasattr(test_client, 'provider') and test_client.provider == "anthropic":
                                    content_parts.append({
                                        "type": "image",
                                        "source": {
                                            "type": "base64",
                                            "media_type": mime_type,
                                            "data": result['_embedded_file_base64']
                                        }
                                    })
                                else:
                                    content_parts.append({
                                        "type": "image_url",
                                        "image_url": {
                                            "url": f"data:{mime_type};base64,{result['_embedded_file_base64']}"
                                        }
                                    })
                                
                                function_message = {
                                    "role": "tool",
                                    "tool_call_id": tool_call.id,
                                    "content": content_parts
                                }
                            else:
                                result_content = json.dumps(result, default=str, ensure_ascii=False)
                                if not result_content or result_content.strip() in ['', '{}', 'null']:
                                    result_content = json.dumps({"result": "å·¥å…·æ‰§è¡Œå®Œæˆï¼Œæ— è¿”å›å†…å®¹"}, ensure_ascii=False)
                                
                                function_message = {
                                    "role": "tool",
                                    "tool_call_id": tool_call.id,
                                    "content": result_content
                                }
                            
                            print(function_message)
                            messages.append(function_message)

                        except Exception as e:
                            print(f"å·¥å…·æ‰§è¡Œé”™è¯¯: {e}")
                            error_content = json.dumps({"error": str(e)}, default=str, ensure_ascii=False)
                            if not error_content or error_content.strip() in ['', '{}', 'null']:
                                error_content = json.dumps({"error": "å·¥å…·æ‰§è¡Œå‘ç”ŸæœªçŸ¥é”™è¯¯"}, ensure_ascii=False)
                                
                            function_message = {
                                "role": "tool",
                                "tool_call_id": tool_call.id,
                                "content": error_content
                            }
                            messages.append(function_message)
                    else:
                        print(f"æœªçŸ¥å·¥å…·: {tool_call.function.name}")
                        print(f"å¯ç”¨å·¥å…·: {list((tool_registry or {}).keys())}")
                        error_content = json.dumps({"error": f"æœªçŸ¥å·¥å…·: {tool_call.function.name}"}, default=str, ensure_ascii=False)
                        if not error_content or error_content.strip() in ['', '{}', 'null']:
                            error_content = json.dumps({"error": "å·¥å…·ä¸å­˜åœ¨"}, ensure_ascii=False)
                            
                        function_message = {
                            "role": "tool",
                            "tool_call_id": tool_call.id,
                            "content": error_content
                        }
                        messages.append(function_message)

                except Exception as e:
                    print(f"è§£æå·¥å…·è°ƒç”¨å‚æ•°å¤±è´¥: {e}")
                    error_content = json.dumps({"error": f"å‚æ•°è§£æå¤±è´¥: {str(e)}"}, default=str, ensure_ascii=False)
                    if not error_content or error_content.strip() in ['', '{}', 'null']:
                        error_content = json.dumps({"error": "å‚æ•°è§£æå¤±è´¥"}, ensure_ascii=False)
                        
                    function_message = {
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": error_content
                    }
                    messages.append(function_message)

        if round_count >= max_rounds:
            print(f"\nâš ï¸ è¾¾åˆ°æœ€å¤§è½®æ•°é™åˆ¶ ({max_rounds})ï¼Œå¼ºåˆ¶ç»“æŸå¯¹è¯")
            # ç¡®ä¿è·å–å®Œæ•´çš„å†…å®¹ï¼Œå³ä½¿è¾¾åˆ°è½®æ•°é™åˆ¶
            base_content = getattr(assistant_message, 'content', '') or ''
            final_content = base_content + "å¯¹è¯è½®æ•°è¶…è¿‡é™åˆ¶ï¼Œè‡ªåŠ¨ç»“æŸ"
        else:
            # ç¡®ä¿è·å–å®Œæ•´çš„åŠ©æ‰‹å›ç­”å†…å®¹
            final_content = getattr(assistant_message, 'content', None)
            if final_content is None:
                final_content = "æ¨¡å‹å›ç­”ä¸ºç©º"
            else:
                final_content = _content_to_text(final_content)

        print(f"\n=== å¯¹è¯ç»“æŸï¼Œå…±è¿›è¡Œäº† {round_count} è½® ===")

    # åœ¨ä¿å­˜å‰å°è¯•"æ”¶æ•›åˆ°ä¸¥æ ¼ JSON"ï¼šè‹¥æœŸæœ›ç»“æ„å­˜åœ¨ä½†å½“å‰æ— æ³•æŠ½å–ï¼Œåˆ™è¿½åŠ ä¸€è½®åªè¾“å‡ºJSONçš„æç¤º
    if answer_template:
        try:
            # parsed_once = extract_structured_answer_from_response(final_content)
            parsed_once = None
        except Exception:
            parsed_once = None
        if not parsed_once:
            finalize_prompt = (
                "ç°åœ¨è¯·ä»…è¾“å‡ºä¸¥æ ¼ JSONï¼Œå¿…é¡»å®Œå…¨åŒ¹é…ä»¥ä¸‹ç»“æ„ï¼ˆé”®åã€å±‚çº§ã€å­—æ®µé½å…¨ï¼‰ï¼Œ"
                "æ‰€æœ‰æ•°å€¼ç”¨æ•°å­—è¡¨ç¤ºï¼ˆä¿ç•™2-3ä½å°æ•°ï¼‰ï¼Œå¸ƒå°”ç”¨ true/falseï¼Œå­—ç¬¦ä¸²å¡«å†™è¯´æ˜æ–‡å­—ã€‚"
                "ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šæ–‡å­—æˆ–å‰åç¼€ï¼Œä¹Ÿä¸è¦å¤šä½™å­—æ®µï¼›è‹¥æ— æ³•è®¡ç®—æŸå€¼è¯·ç»™å‡ºå¯è§£æçš„è¿‘ä¼¼å€¼ã€‚\n\n"
                f"{json.dumps(answer_template, ensure_ascii=False, indent=2)}"
            )
            print("âš™ï¸ å°è¯•è¿›è¡Œæœ€ç»ˆæ”¶æ•›æç¤ºï¼Œè¦æ±‚æ¨¡å‹ä»…è¾“å‡º JSON ...")
            # è¿½åŠ ç”¨æˆ·æŒ‡ä»¤ï¼Œä¸å†ä¼  tools
            messages.append({"role": "user", "content": finalize_prompt})
            response = test_client.chat_completions_create(messages=messages)
            final_msg = _extract_choice_message(response, "æœ€ç»ˆæ”¶æ•›æç¤º")
            # è®°å½•æ”¶æ•›è½®çš„å›å¤
            messages.append({"role": "assistant", "content": getattr(final_msg, 'content', None)})
            # æ›´æ–°æœ€ç»ˆæ–‡æœ¬
            final_content = _content_to_text(getattr(final_msg, 'content', None)) or "æ¨¡å‹å›ç­”ä¸ºç©º"
            # å†å°è¯•è§£æ
            try:
                parsed_once = extract_structured_answer_from_response(final_content)
            except Exception:
                parsed_once = None
            if parsed_once:
                print("âœ… æ”¶æ•›æˆåŠŸï¼Œå·²è·å¾—å¯è§£æçš„ç»“æ„åŒ– JSONã€‚")
            else:
                print("âš ï¸ æ”¶æ•›åä»æœªèƒ½æŠ½å–åˆ°ç»“æ„åŒ– JSONã€‚")

    # åœ¨æ¯æ¬¡æµ‹è¯•ç»“æŸåï¼ŒæŠŠæ•´ä¸ªæ¶ˆæ¯è½¨è¿¹ä¿å­˜ä¸º JSON æ–‡ä»¶ï¼Œä¾¿äºåç»­è¯„åˆ†
    try:
        # ä» metadata ä¸­è·å–æ•°æ®é›†æ–‡ä»¶åï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        dataset_filename = metadata.get('_dataset_filename') if isinstance(metadata, dict) else None
        
        trace_path, sanitized_tag = _derive_trace_path(
            current_model,
            use_tools,
            query_data['id'],
            trace_tag,
            mode_name=effective_mode_name,
            metadata=metadata,
            dataset_filename=dataset_filename,
        )
        trace_path.parent.mkdir(parents=True, exist_ok=True)

        # å°† messages ä¸­çš„å¯¹è±¡åºåˆ—åŒ–ï¼šè‹¥å·²æ˜¯ dict/list/strï¼Œç›´æ¥å†™å…¥ï¼›å¦åˆ™è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        serializable_messages = []
        for m in messages:
            mm = {}
            # æ”¯æŒä¸¤ç§æ¶ˆæ¯å½¢å¼ï¼šdictï¼ˆæˆ‘ä»¬è‡ªå·±åˆ›å»ºçš„ï¼‰æˆ–å¸¦å±æ€§çš„å¯¹è±¡ï¼ˆä¾‹å¦‚ SDK è¿”å›çš„æ¶ˆæ¯å¯¹è±¡ï¼‰
            if isinstance(m, dict):
                role = m.get('role')
                content = m.get('content')
                tool_call_id = m.get('tool_call_id') if 'tool_call_id' in m else None
                tool_calls = m.get('tool_calls')
            else:
                # å®‰å…¨åœ°å°è¯•ä»å¯¹è±¡è¯»å–å¸¸ç”¨å±æ€§
                role = getattr(m, 'role', None)
                # SDK çš„ message.content å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–å¯¹è±¡
                content = getattr(m, 'content', None)
                tool_call_id = getattr(m, 'tool_call_id', None)
                tool_calls = getattr(m, 'tool_calls', None)

            mm['role'] = role

            # å¤„ç†content
            if isinstance(content, (str, dict, list)):
                mm['content'] = content
            elif content is None:
                mm['content'] = None
            else:
                # å°è¯•è·å– .content å±æ€§çš„å­—ç¬¦ä¸²ï¼ˆå¦‚æœæ˜¯ SDK å¯¹è±¡ï¼‰
                try:
                    mm['content'] = str(content)
                except Exception:
                    mm['content'] = None

            # å¤„ç†tool_callsï¼ˆå¯¹äºassistantæ¶ˆæ¯ï¼‰
            if tool_calls is not None:
                if isinstance(tool_calls, list):
                    serialized_tool_calls = []
                    for tc in tool_calls:
                        if isinstance(tc, dict):
                            serialized_tool_calls.append(tc)
                        else:
                            # ä»SDKå¯¹è±¡æå–tool_callä¿¡æ¯
                            tc_dict = {
                                'id': getattr(tc, 'id', None),
                                'type': getattr(tc, 'type', 'function'),
                                'function': {
                                    'name': getattr(tc.function, 'name', None) if hasattr(tc, 'function') else None,
                                    'arguments': getattr(tc.function, 'arguments', None) if hasattr(tc, 'function') else None
                                }
                            }
                            serialized_tool_calls.append(tc_dict)
                    mm['tool_calls'] = serialized_tool_calls

            # å¯¹ tool æ¶ˆæ¶ˆæ¯ï¼Œcontent é€šå¸¸æ˜¯ JSON å­—ç¬¦ä¸²ï¼ˆæˆ‘ä»¬æ·»åŠ çš„ï¼‰ï¼Œå°è¯•è§£æ
            if role == 'tool' and isinstance(content, str):
                try:
                    mm['content'] = json.loads(content)
                except Exception:
                    mm['content'] = content

            if tool_call_id is not None:
                mm['tool_call_id'] = tool_call_id

            serializable_messages.append(mm)

        metadata_summary = ensure_metadata_summary(query_data)
        metadata_full = query_data.get('metadata') if isinstance(query_data.get('metadata'), dict) else None
        serialized_query_payload = None
        payload_source = query_data
        if use_tools and tool_protocols:
            try:
                payload_source = deepcopy(query_data)
                payload_source['usage_tool_protocol'] = tool_protocols
            except Exception:
                payload_source = query_data
        try:
            # æ·±æ‹·è´æµ‹è¯•æ¡ˆä¾‹ï¼Œç¡®ä¿å†™å…¥çš„JSONå¯åºåˆ—åŒ–
            serialized_query_payload = json.loads(json.dumps(payload_source, ensure_ascii=False, default=str))
        except Exception:
            # å¦‚æœåºåˆ—åŒ–å¤±è´¥ï¼Œé€€åŒ–ä¸ºåªä¿ç•™åŸºç¡€å­—æ®µ
            serialized_query_payload = {
                'id': query_data.get('id'),
                'question': query_data.get('question'),
                'answer': query_data.get('answer'),
            }

        with open(trace_path, 'w', encoding='utf-8') as f:
            trace_data = {
                'id': query_data['id'],
                'query': query_data['question'],
                'query_data': serialized_query_payload,
                'model': current_model,
                'use_tools': use_tools,  # æ–°å¢ï¼šæ ‡è®°æ˜¯å¦ä½¿ç”¨å·¥å…·
                'mode': "tool_mode" if use_tools else "text_mode",  # æ–°å¢ï¼šæµ‹è¯•æ¨¡å¼
                'rounds': round_count if use_tools else 1,  # æ·»åŠ è½®æ•°ä¿¡æ¯
                'messages': serializable_messages
            }

            if metadata_full:
                trace_data['metadata'] = metadata_full
            if metadata_summary:
                trace_data['metadata_summary'] = metadata_summary

            # æ·»åŠ è§„èŒƒç­”æ¡ˆä¿¡æ¯ç”¨äºè¯„åˆ†
            if answer_template and golden_standard:
                trace_data['answer_template'] = answer_template
                trace_data['golden_standard'] = golden_standard
                trace_data['expected_format'] = "structured_json"

                # å°è¯•ä»æœ€ç»ˆå›ç­”ä¸­æå–ç»“æ„åŒ–æ•°æ®
                try:
                    final_answer_structured = extract_structured_answer_from_response(final_content)
                    if final_answer_structured:
                        trace_data['model_structured_answer'] = final_answer_structured
                        trace_data['answer_extraction_success'] = True
                        # æ— è®ºæå–æ˜¯å¦æˆåŠŸï¼Œéƒ½ä¿å­˜åŸå§‹å›ç­”æ–‡æœ¬ï¼›å¹¶é¢å¤–ä¿å­˜æœ€åä¸€æ¡ assistant çš„åŸå§‹å¤šæ¨¡æ€ç»“æ„ï¼ˆè‹¥æœ‰ï¼‰
                        trace_data['model_raw_answer'] = final_content
                        # æŠŠæœ€åä¸€æ¡ assistant æ¶ˆæ¯é™„åŠ ä¿å­˜ï¼Œä¾¿äºåç»­ç²¾ç¡®å›é€€
                        try:
                            last_assist_msg = None
                            for m in reversed(serializable_messages):
                                if m.get('role') == 'assistant':
                                    last_assist_msg = m
                                    break
                            if last_assist_msg is not None:
                                trace_data['model_last_assistant_message'] = last_assist_msg
                        except Exception:
                            pass

                        # ç«‹å³è¿›è¡Œè¯„åˆ†
                        from gym.core.evaluator import calculate_answer_score
                        score, summary, details = calculate_answer_score(final_answer_structured, golden_standard)
                        trace_data['evaluation_score'] = score
                        trace_data['evaluation_summary'] = summary
                        trace_data['evaluation_details'] = details

                        print(f"ğŸ“Š è‡ªåŠ¨è¯„åˆ†ç»“æœ: {summary}")
                        if score < 0.8:  # å¦‚æœåˆ†æ•°è¾ƒä½ï¼Œæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                            print(f"ğŸ“‹ è¯„åˆ†è¯¦æƒ…: {json.dumps(details, ensure_ascii=False, indent=2)}")
                    else:
                        trace_data['answer_extraction_success'] = False
                        trace_data['model_raw_answer'] = final_content
                        try:
                            last_assist_msg = None
                            for m in reversed(serializable_messages):
                                if m.get('role') == 'assistant':
                                    last_assist_msg = m
                                    break
                            if last_assist_msg is not None:
                                trace_data['model_last_assistant_message'] = last_assist_msg
                        except Exception:
                            pass
                        print("âš ï¸ æœªèƒ½ä»å›ç­”ä¸­æå–ç»“æ„åŒ–æ•°æ®ï¼Œæ— æ³•è‡ªåŠ¨è¯„åˆ†")
                except Exception as e:
                    trace_data['answer_extraction_success'] = False
                    trace_data['answer_extraction_error'] = str(e)
                    trace_data['model_raw_answer'] = final_content
                    try:
                        last_assist_msg = None
                        for m in reversed(serializable_messages):
                            if m.get('role') == 'assistant':
                                last_assist_msg = m
                                break
                        if last_assist_msg is not None:
                            trace_data['model_last_assistant_message'] = last_assist_msg
                    except Exception:
                        pass
                    print(f"âŒ ç­”æ¡ˆæå–é”™è¯¯: {e}")
            else:
                trace_data['expected_format'] = "free_text"
                trace_data['model_raw_answer'] = final_content
                try:
                    last_assist_msg = None
                    for m in reversed(serializable_messages):
                        if m.get('role') == 'assistant':
                            last_assist_msg = m
                            break
                    if last_assist_msg is not None:
                        trace_data['model_last_assistant_message'] = last_assist_msg
                except Exception:
                    pass

            trace_data['mode_folder'] = trace_path.parent.name
            if sanitized_tag:
                trace_data['trace_tag'] = sanitized_tag

            json.dump(trace_data, f, ensure_ascii=False, indent=2)
        print(f"å¯¹è¯è½¨è¿¹å·²ä¿å­˜ä¸º: {trace_path}")
    except Exception as e_save:
        print(f"ä¿å­˜è½¨è¿¹å¤±è´¥: {e_save}")

    return final_content


def debug_simple_test_query_with_first_refined_case(
    model_name: Optional[str] = None,
    use_tools: bool = True,
) -> Optional[str]:
    """
    æœ€å°å•å…ƒæµ‹è¯•å‡½æ•°ï¼š
    - ä» refine_merged_questions_augmented.json ä¸­è¯»å–ç¬¬ä¸€ä¸ªæ¡ˆä¾‹
    - è°ƒç”¨ simple_test_query
    - æ‰“å°å¹¶è¿”å›æœ€ç»ˆå›ç­”æ–‡æœ¬

    ä½¿ç”¨æ–¹å¼ï¼ˆåœ¨é¡¹ç›®æ ¹ç›®å½•ï¼‰ï¼š
        python -c "from gym.test_executor import debug_simple_test_query_with_first_refined_case as f; f('glm-4.6v')"
    """
    import json
    from pathlib import Path

    core_dir = Path(__file__).resolve().parent
    dataset_path = core_dir / "dataset" / "refine_merged_questions_augmented.json"
    if not dataset_path.exists():
        print(f"è­¦å‘Šï¼šæ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_path}")
        return None

    try:
        with dataset_path.open("r", encoding="utf-8") as f:
            cases = json.load(f)
    except Exception as e:
        print(f"LOAD ERROR  åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        return None

    if not isinstance(cases, list) or not cases:
        print("TYPE ERROR  æ•°æ®é›†å†…å®¹ä¸ºç©ºæˆ–æ ¼å¼ä¸æ˜¯åˆ—è¡¨")
        return None

    case = cases[0]
    case_id = case.get("id", "unknown")
    print(f"ğŸ“ ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¡ˆä¾‹è¿›è¡Œ simple_test_query è°ƒè¯•ï¼ŒID: {case_id}")

    result = simple_test_query(
        case,
        model_name=model_name,
        use_tools=use_tools,
    )
    final_answer = result.get("final_answer", "")
    print("\n=== æ¨¡å‹æœ€ç»ˆå›ç­” ===")
    print(final_answer)
    print("====================\n")
    
    # æ·»åŠ  evaluationï¼šæå– boxed answer å¹¶è¿›è¡Œåˆ¤é¢˜
    from gym.core.evaluator import extract_boxed_answer, is_answer_correct
    
    print("=== å¼€å§‹è¯„ä¼° ===")
    
    # æå– boxed answer
    boxed_answer = extract_boxed_answer(final_answer)
    
    # è¯»å–æ ‡å‡†ç­”æ¡ˆ
    standard_answer = case.get("answer")
    if not standard_answer:
        # å°è¯•ä» metadata ä¸­è·å–
        metadata = case.get("metadata", {})
        standard_answer = metadata.get("golden_answer") or metadata.get("answer")
    
    # å¤„ç†æ ‡å‡†ç­”æ¡ˆå¯èƒ½æ˜¯åˆ—è¡¨æˆ–å­—å…¸çš„æƒ…å†µ
    standard_answer_str = None
    if standard_answer:
        if isinstance(standard_answer, list) and standard_answer:
            standard_answer_str = str(standard_answer[0])
        elif isinstance(standard_answer, dict):
            standard_answer_str = json.dumps(standard_answer, ensure_ascii=False)
        else:
            standard_answer_str = str(standard_answer)
    
    # è·å–é—®é¢˜æ–‡æœ¬
    question_text = case.get("question", "")
    
    # è¿›è¡Œåˆ¤é¢˜ï¼ˆå¦‚æœæœ‰ boxed answer å’Œæ ‡å‡†ç­”æ¡ˆï¼‰
    is_correct = None
    if boxed_answer and standard_answer_str:
        try:
            print(f"ğŸ“‹ æ ‡å‡†ç­”æ¡ˆ: {standard_answer_str}")
            print(f"ğŸ“‹ æ¨¡å‹ç­”æ¡ˆ: {boxed_answer}")
            print(f"ğŸ“‹ é—®é¢˜: {question_text[:100]}..." if len(question_text) > 100 else f"ğŸ“‹ é—®é¢˜: {question_text}")
            
            is_correct = is_answer_correct(question_text, boxed_answer, standard_answer_str, case_id)
            
            if is_correct:
                print("åˆ¤é¢˜ç»“æœâœ… : æ­£ç¡®")
            else:
                print("åˆ¤é¢˜ç»“æœâŒ : é”™è¯¯")
                
        except Exception as e:
            print(f"åˆ¤é¢˜è¿‡ç¨‹å‡ºé”™âŒ : {e}")
            import traceback
            traceback.print_exc()
    else:
        if not boxed_answer:
            print("âš ï¸ æœªèƒ½ä»å›ç­”ä¸­æå– boxed answerï¼Œè·³è¿‡åˆ¤é¢˜")
        if not standard_answer_str:
            print("âš ï¸ æœªæ‰¾åˆ°æ ‡å‡†ç­”æ¡ˆï¼Œè·³è¿‡åˆ¤é¢˜")
    
    print("====================\n")
    
    # ä¿å­˜ trace æ–‡ä»¶
    try:
        from gym.core.data_loader import ensure_metadata_summary
        
        # è·å– metadata å’Œ dataset_key
        metadata = case.get("metadata", {})
        if not isinstance(metadata, dict):
            metadata = {}
        
        # ä»æ•°æ®é›†æ–‡ä»¶åæ¨æ–­ dataset_keyï¼ˆå¦‚æœ metadata ä¸­æ²¡æœ‰ï¼‰
        if not metadata.get("dataset_key"):
            dataset_filename = dataset_path.name
            if "single" in dataset_filename.lower():
                metadata["dataset_key"] = "merged_single_questions"
            else:
                metadata["dataset_key"] = "merged_questions"
        
        # ç¡®ä¿ metadata ä¸­æœ‰ _dataset_filenameï¼ˆç”¨äºè·¯å¾„åˆ¤æ–­ï¼‰
        if "_dataset_filename" not in metadata:
            metadata["_dataset_filename"] = dataset_path.name
        
        # ç¡®å®š trace è·¯å¾„ï¼ˆä½¿ç”¨æœ¬æ–‡ä»¶å†…çš„ç²¾ç®€ç‰ˆæ¨å¯¼é€»è¾‘ï¼Œé¿å…å¯¼å…¥ gym.test_executorï¼‰
        mode_name = "with_tools_react" if use_tools else "without_tools"
        trace_path = _derive_trace_path_for_debug(
            model_name=model_name or DEFAULT_MODEL,
            use_tools=use_tools,
            case_id=case_id,
            mode_name=mode_name,
            metadata=metadata,
            dataset_filename=dataset_path.name,
        )
        
        # åºåˆ—åŒ– messagesï¼ˆç®€åŒ–ç‰ˆï¼Œå› ä¸º simple_test_query è¿”å›çš„ messages å·²ç»æ˜¯ dict æ ¼å¼ï¼‰
        serializable_messages = []
        for m in result.get("messages", []):
            if isinstance(m, dict):
                # å·²ç»æ˜¯ dictï¼Œç›´æ¥ä½¿ç”¨ï¼Œä½†éœ€è¦å¤„ç† tool æ¶ˆæ¯çš„ contentï¼ˆå¯èƒ½æ˜¯ JSON å­—ç¬¦ä¸²ï¼‰
                mm = m.copy()
                if m.get("role") == "tool" and isinstance(m.get("content"), str):
                    try:
                        mm["content"] = json.loads(m["content"])
                    except Exception:
                        pass
                serializable_messages.append(mm)
            else:
                # å¦‚æœæ˜¯å¯¹è±¡ï¼Œè½¬æ¢ä¸º dictï¼ˆç®€åŒ–å¤„ç†ï¼‰
                serializable_messages.append({
                    "role": getattr(m, "role", None),
                    "content": getattr(m, "content", None),
                })
        
        # æ„å»º trace æ•°æ®
        metadata_summary = ensure_metadata_summary(case)
        trace_data = {
            "id": case_id,
            "query": case.get("question", ""),
            "query_data": case,  # ä¿å­˜å®Œæ•´çš„ case æ•°æ®
            "model": model_name or DEFAULT_MODEL,
            "use_tools": use_tools,
            "mode": "tool_mode" if use_tools else "text_mode",
            "rounds": result.get("rounds", 1),
            "messages": serializable_messages,
            "model_raw_answer": final_answer,
        }
        
        if metadata:
            trace_data["metadata"] = metadata
        if metadata_summary:
            trace_data["metadata_summary"] = metadata_summary
        
        # æ·»åŠ  evaluation ç»“æœ
        if boxed_answer is not None:
            trace_data["model_boxed_answer"] = boxed_answer
            trace_data["boxed_extraction_success"] = True
        else:
            trace_data["boxed_extraction_success"] = False
        
        if is_correct is not None:
            trace_data["boxed_answer_evaluation"] = {
                "is_correct": is_correct,
                "standard_answer": standard_answer_str,
                "model_answer": boxed_answer,
                "evaluation_method": "gpt4.1_judge",
                "evaluation_success": True,
            }
        
        # ä¿å­˜ trace æ–‡ä»¶
        with open(trace_path, "w", encoding="utf-8") as f:
            json.dump(trace_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ Trace æ–‡ä»¶å·²ä¿å­˜: {trace_path}")
        
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜ trace æ–‡ä»¶å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    return final_answer


__all__ = [
    "simple_test_query",
    "simple_test_refine_query",
    "test_query",
    "debug_simple_test_query_with_first_refined_case",
]

if __name__ == "__main__": 
    debug_simple_test_query_with_first_refined_case('glm-4.6v', use_tools=True)